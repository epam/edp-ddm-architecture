:toc-title: ЗМІСТ
:toc: auto
:toclevels: 5
:experimental:
:important-caption:     ВАЖЛИВО
:note-caption:          ПРИМІТКА
:tip-caption:           ПІДКАЗКА
:warning-caption:       ПОПЕРЕДЖЕННЯ
:caution-caption:       УВАГА
:example-caption:           Приклад
:figure-caption:            Зображення
:table-caption:             Таблиця
:appendix-caption:          Додаток
:sectnums:
:sectnumlevels: 5
:sectanchors:
:sectlinks:
:partnums:

= Виведення Платформи та реєстрів у промислову експлуатацію

CAUTION: Сторінка у процесі формування.

== Підготовчі дії на рівні Платформи

=== Базові передумови

Для виведення платформи реєстрів в промислову експлуатацію обов'язково слід використовувати віртуальні інфраструктури, що отримують офіційну підтримку (наразі це https://aws.amazon.com/[AWS] та https://www.vmware.com/products/vsphere.html[VSphere]).

На таких інфраструктурах має бути встановлений OKD-кластер, версія якого відповідає вимогам Платформи, згідно з рекомендаціями, викладеними в офіційній документації Платформи (xref:admin:installation/okd-requirements.adoc[]).

Встановлення та налаштування Платформи має виконуватися відповідно до вказівок, наданих в офіційній документації, і проводитися на середовищах, які підтримуються офіційно:

* xref:admin:installation/platform-deployment/platform-aws-deployment.adoc[]
* xref:admin:installation/platform-deployment/platform-vsphere-deployment.adoc[]

=== Навчання технічного адміністратора реєстру

Онбординг для адміністраторів реєстру має важливе значення з різних поглядів, зокрема:

* Робота адміністратора реєстру потребує глибоких технічних знань. Це включає знання про структуру та функціонування реєстру, розуміння роботи його компонентів, а також здатність моніторингу та підтримки реєстру. Навчання допомагає адміністраторам отримати ці знання та навички, необхідні для ефективного виконання їх обов'язків.

* Адміністратори реєстру відповідають за розробку та впровадження регламенту реєстру. Це включає розуміння логіки функціонування реєстру, моделі даних, бізнес-процесів та інтерфейсів користувача. Онбординг дозволяє адміністраторам зрозуміти ці особливості, що сприяє ефективній роботі.

* Висвітлення всіх аспектів роботи в ролі адміністратора реєстру під час онбордингу може значно підвищити продуктивність роботи. Якщо адміністратори з самого початку розуміють свої обов'язки, цілі та очікування, вони зможуть швидше адаптуватися та ефективно виконувати свою роботу в межах великої розподіленої системи.

[TIP]
====
Більш детально ви можете ознайомитися із навчальними матеріалами адміністраторів реєстру на сторінках:

* xref:registry-develop:registry-admin-study/registry-admin-profile.adoc[]
* xref:registry-develop:registry-admin-study/registry-admin-study.adoc[]
* xref:registry-develop:study-project/index.adoc[]
====

=== Планування резервного сховища бекапів Платформи та реєстрів

Резервні копії (бекапи) зберігаються в окремому сховищі бекапів, яке є S3-сумісним (https://min.io/[]). Параметри цього сховища з погляду доступного місця у сховищі та обчислювальних ресурсів повинні бути враховані, залежно від розміру бекапів.

[NOTE]
====
Розрахувати точний розмір виділеного місця для зберігання резервних копій дуже важко. Усе залежить від специфіки конкретного реєстру. Нижче наведені приблизні цифри, на які можна орієнтуватися.
====

. Резервне копіювання центральних компонент: треба поділити інсталяцію на 2 типи -- AWS та VSphere.
+
* AWS  --  розмір 1 бекапу ресурсів Openshift для центаральних компонент займає від 1 до 5 Mb. Якщо казати про данні які зберігаються на PersitentVolumeClaim то у випадку з AWS для кожного PersitentVolumeClaim робиться EBS Snapshot (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html) і місце на Minio під зберігання данних для PersitentVolumeClaim не використовується. Але є виключення у вигляді одної PersitentVolumeClaim в namespace user-management і використання Minio у данному випадку буде 5Mb (тут треба розглядати те, що 5Mb це не 1 бекап вольумів, а усі данні вольума у вигляді "резервної" копії).

* VSphere - все так само по ресурсах Openshift, розмір ідентичний тому що і для AWS, але для зберігання резервних копій з *PersitentVolumeClaims* використовується сам Minio і для зберігання резервних копій, використовує приблизно:

** control-plane - 15Гб
** user-management - 12Гб
** control-plane-nexus - 100Гб
** grafana-monitoring - 10Гб
+
Цей розмір - це розмір самих PVC  і можна вважати що на мініо кожен проект буде займати стільки місця. Тобто для центральних копмонент треба брати ~140Гб під резевренне копіювання данних PVC  і ще ~10Гіг закладати під ресурси опеншифта.(10 Гб - брав для майбунього)

. Резеврне копіювання реєстрів для AWS та VSphere не відрізняється, тому для однієї резервної копії для ресурсів Openshift потрібно:

* Розмір одного бекапу Openshift ресурсів -- 10Mb
* Розмір для бекапу PVC (томів) - 200Gb (ця цифра не буде змінюватись впродовж часу, 200Gb - як раз є сумарною цифрою усіх даних у випадку коли диски PVC не збільшувались).
+
Для одного реєстру треба закладати приблизно 210Gb для створення резервних копій.

. Реплікація -- тут для початку можна порекомендувати зарезервувати до 200Gb, у випадку збільшення реєстру розмір може досягати декілька террабайт.

=== Планування місця для сховищ центральних компонентів Платформи

Центральні компоненти Платформи зберігаються у томах cloud-native-сховища. Конфігурації доступного місця на дисках для таких компонентів доступні через OpenShift-консоль, у розділі *Storage* > *PersistentVolumeClaims*, у проєктах відповідних компонентів, зокрема:

* `openshift-logging`
* `grafana-monitoring`
* `control-plane`
* `control-plane-nexus`
* `user-management` (база даних Keycloak)

Розмір дисків можна змінювати відповідно ваших потреб. Для цього перейдіть до *Storage* > *PersistentVolumeClaims* > YAML та встановіть бажане значення для розміру диска параметром `status.capacity.storage`.

image:platform-prod-deployment/platform-prod-deploy-persistent-volume-claims-central.png[]

image:platform-prod-deployment/platform-prod-deploy-persistent-volume-claims-central-1.png[]

NOTE: Кожний сервіс запускається із налаштуваннями розміру дисків за замовчуванням. Ви як адміністратор можете залишити ці налаштування, або якщо ви чітко знаєте, що виділеного розміру дисків недостатньо для ваших потреб, тоді томи можна розширити одразу. Рекомендоване значення для збільшення місця на диску +50%. Наприклад, розмір диска для сервісу Grafana за замовчування -- 10Gi. Тоді для початку ви можете збільшити місце до 15Gi.

=== Планування місця для кореневих томів файлової системи Ceph

Сервіс Ceph -- це також центральний компонент Платформи, але зі своєю специфікою. Кореневі томи (root volumes) для Ceph зберігаються у проєкті `openshift-storage`.

Налаштувати розмір дисків для Ceph можна в OpenShift-консолі > проєкт `openshift-storage` > *Storage* > *PersistentVolumeClaims*.

NOTE: Рекомендоване значення для збільшення місця на диску +50%, як і для інших центральних компонентів. Наприклад, розмір диска для Ceph за замовчування -- 2Ti. Тоді для початку ви можете збільшити місце до 3Ti.

[TIP]
====
Детальніше про налаштування файлової системи ви можете дізнатися на сторінках:

* xref:admin:file-system/ceph-space.adoc[]
* xref:admin:file-system/ceph_scaling.adoc[]
* xref:admin:file-system/s3/lifecycle-policy.adoc[]
====

=== Зберігання цифрових печаток адміністратора для платформи та реєстрів

==== Мережний криптомодуль "Гряда"

У промисловій експлуатації рекомендованим сховищем зберігання ключів є програмно-апаратний комплекс "Гряда". Технічний адміністратор Платформи повинен мати змогу генерувати та сертифікувати ключі для Платформи та реєстрів, що будуть на ній розгорнуті.

"Гряда" встановлюється на окремому екземплярі, в тому ж ЦОД, але окремо від Платформи реєстрів. Налаштування криптомодуля як стороннього продукту виконує адміністратор "Гряди". На стороні Платформи адміністрування виконує технічний адміністратор Платформи. Для Платформи передбачений єдиний екземпляр "Гряди" для всіх реєстрів.

Необхідно забезпечити мережеве з'єднання між Платформою реєстрів та "Грядою". Для цього: ::
+
. На стороні криптомодуля "Гряда" необхідно дозволити трафік з OpenShift. За це відповідає адміністратор "Гряди". Адміністратор Платформи має надати IP-адреси, з яких необхідно дозволити трафік від Платформи реєстрів до "Гряди".

. На поді DSO (_сервіс цифрових підписів_) реєстру необхідно дозволити вихідний трафік на "Гряду":

* Відкрийте налаштування поди DSO-сервісу та перевірте конфігурацію `sidecar.istio`:
+
** Якщо в анотації до сервісу вказано `sidecar.istio.io/inject: 'false'`, то трафік дозволено за замовчуванням, і додаткові налаштування не потрібні.
+
----
metadata:
  annotations:
    sidecar.istio.io/inject: 'false'
----
+
** Якщо вказано `sidecar.istio.io/inject: 'true'`, зокрема:
+
----
metadata:
  annotations:
    sidecar.istio.io/inject: 'true'
    traffic.sidecar.istio.io/excludeOutboundIPRanges: 10.129.71.251/32
----
+
переконайтеся, що YAML-конфігурація на поді DSO має наступні анотації:
+
----
traffic.sidecar.istio.io/excludeOutboundIPRanges: {{ .Values.griada.ip }}
traffic.sidecar.istio.io/excludeOutboundPorts: '{{ .Values.griada.port }}'
----
+
*** де замість `{{ .Values.griada.ip }}` буде IP-адреса "Гряди", наприклад, `0.0.0.0`;

*** замість `'{{ .Values.griada.port }}'` буде порт "Гряди", наприклад, `3080`.

* Анотації з helm chart для DSO-сервісу автоматично сформуються, якщо у _deploy-templates/values.yaml_ реєстру дозволено трафік на "Гряду" й вказані IP та порт.
+
----
griada:
  enabled: true
  ip: 0.0.0.0
  port: 3080
----

. Виконайте налаштування всередині самої Гряди, щоб технічний адміністратор Платформи мав змогу генерувати та сертифікувати ключі для Платформи та реєстрів, що будуть на ній розгорнуті.
+
[TIP]
====
* Як встановити та налаштувати "Гряду, дивіться на офіційному сайті ІІТ: https://iit.com.ua/products[].
* Як налаштувати доступ до "Гряди" на локальній машині, дивіться на сторінці xref:admin:installation/griada/configure-access-griada-locally.adoc[].
* Як використовувати криптомодуль при роботі із ключами цифрового підпису на Платформі, див. на сторінках:

** xref:admin:registry-management/system-keys/control-plane-platform-keys.adoc[]
** xref:admin:registry-management/system-keys/control-plane-registry-keys.adoc[]
====

TIP: Додатково ознайомтеся із розгортанням емулятора "Гряда" в AWS на сторінці xref:admin:installation/griada/griada-301-deployment.adoc[].

==== Файлові ключі

Використання __файлових ключів для підпис__у є методом, який не рекомендується і, відповідно до законодавства, не пройде Комплексної Системи Захисту Інформації (КСЗІ).

Забезпечення взаємодії реєстру з Акредитованим Центром Сертифікації Ключів (АЦСК) вимагає використання українських IP-адрес.

Якщо екземпляр реєстру не має українських IP, бо розміщений на хмарних ресурсах ЦОД AWS або іншого провайдера, тоді власник екземпляра повинен забезпечити додавання цих IP до білого списку відповідного АЦСК. Цей процес називається "whitelisting". Він дозволяє специфічним IP-адресам обходити певні обмеження мережі, таким чином надаючи змогу взаємодіяти з АЦСК.

NOTE: Важливо зауважити, що не всі хмарні сервіси дозволяють пряме управління IP-адресами. Тому власники екземплярів реєстрів мають розглянути можливості використання додаткових послуг або рішень для отримання українських IP-адрес або забезпечення їх "whitelisting".

[TIP]
====
Детальніше про налаштування файлових ключів цифрового підпису ви можете дізнатися на сторінках:

* xref:admin:registry-management/system-keys/control-plane-platform-keys.adoc[]
* xref:admin:registry-management/system-keys/control-plane-registry-keys.adoc[]
====

=== Налаштування взаємодії з "Трембітою" та встановлення Шлюзу Безпечного Обміну (ШБО)

* xref:registry-develop:registry-admin/external-integration/registration-subsystem-trembita/registration-subsystem-trembita.adoc[]
* xref:registry-develop:registry-admin/external-integration/api-publish/trembita-data-invoking.adoc[]
* xref:registry-develop:registry-admin/external-integration/api-publish/trembita-bp-invoking.adoc[]

=== Первісна перевірка після розгортання Платформи

Після розгортання Платформи у цільовому середовищі, необхідно виконати первинне тестування Платформи, зокрема провести такі перевірки компонентів:

Перевірки в OpenShift-консолі: ::

. У компоненті *`control-plane-jenkins`* перевірте, що пайплайн MASTER-Build-cluster-mgmt завершився успішно, й усі кроки виконані.

. Перевірте, що поди `user-management` розгорнулися, зокрема перевірте доступність сервісів Keycloak та DSO Платформи.

. Перевірте стан под *`control-plane-nexus`* -- вони мають бути у "хорошому" стані.

. Перевірте стан под компонента `openshift-logging`.

. Виконайте вхід до сервісу моніторингу *Grafana* для перевірки його доступності.

. Перевірте стан усіх под у проєктах `istio-system` та `istio-operator` -- вони повинні бути у стані `OK`.

. Перевірте компонент *Jager*: відкрийте сторінку Jager, автентифікуйтеся за допомогою системного користувача `KubeAdmin`, виконайте вхід до сервісу Jager та перевірте, чи відкривається сторінка пошуку Jager.

* Перевірте компонент *Kiali*: відкрийте сторінку Kiali, автентифікуйтеся за допомогою системного користувача `KubeAdmin`, виконайте вхід до сервісу Kiali та перевірте, чи відкривається домашня сторінка Kiali.

* Перевірте стан усіх под `openshift-logging` -- вони повинні бути у стані `OK`.

. Перевірте стан усіх под в `openshift-storage`, а також переконайтеся, що `CephObjectStores` у статусі `Connected`.

. Перевірте готовність `clusterSources`.

. Окремо перевірте доступність файлової системи *Ceph* у проєкті `openshift-storage`.

Перевірки control-plane-console: ::

. Виконайте вхід до *Control Plane* та переконайтеся, що можете бачите вміст розділів +++<b style="font-weight: 600">Реєстри<b>+++ та +++<b style="font-weight: 600">Керування Платформою<b>+++.

. Перейдіть до +++<b style="font-weight: 600">Керування Платформою<b>+++ та створіть адміністратора Платформи.

. Зачекайте, доки Jenkins-пайплайн *MASTER-Build-cluster-mgmt* створить адміністратора та встановить права доступу у сервісі Keycloak. Виконайте вхід до Control Plane вже під щойно створеним адміністратором.

. Створіть новий реєстр у Control Plane.


=== Налаштування відправлення повідомлень моніторингу

Налаштування сповіщень моніторингу (alerting notifications) налаштовується у компоненті *Grafana*. Для цього необхідно обрати канал зв'язку, куди надходитимуть сповіщення. Ми рекомендуємо використовувати чат-бот у Telegram.

TIP: Детальніше про це дивіться на сторінці xref:registry-develop:registry-admin/grafana-monitoring/grafana-alerting-notifications.adoc[].

=== Налаштування базових бордів в Kibana

Для логування (журналювання) подій на Платформі використовуються компоненти *EFK*-стека (*Elasticsearch, Fluentd, Kibana*). EFK-стек відповідає за збір, обробку та візуалізацію журналів подій (логів), що сприяє прозорості та відстеженню стану системи.

Підсистема журналювання подій розгортається в окремому проєкті в OpenShift під назвою `*openshift-logging*`. Це дозволяє ізолювати ресурси, пов'язані з логуванням, від інших компонентів системи, що сприяє підвищенню безпеки та стабільності.

Для візуалізації журналів усіх додатків на платформі використовується *Kibana*, яка надає інтерактивний інтерфейс для аналізу логів та відстеження подій в системі.

TIP: Детальніше див. на сторінках розділу xref:registry-develop:registry-admin/openshift-logging/openshift-logging-overview.adoc[].

=== Налаштування бекапів центральних компонент

* xref:admin:backup-restore/control-plane-components-backup-restore.adoc[]
* xref:admin:backup-restore/backup-schedule-cluster-mgmt.adoc[]

=== Налаштування ключів цифрового підпису Платформи

. Створення ключів та сертифікатів цифрового підпису відбувається під час розгортання платформи (_див. детальніше -- xref:admin:installation/platform-deployment/platform-aws-deployment.adoc#preconditions-first-stage[Необхідні елементи для розгортання Платформи]_).
* Загальний опис ключів на Платформі: xref:admin:registry-management/system-keys/system-keys-overview.adoc[]

. Оновлення: xref:admin:registry-management/system-keys/control-plane-platform-keys.adoc[]

=== Отримання дозволу для поштового сервера в AWS-середовищі

* xref:admin:installation/internal-smtp-server-setup.adoc#_отримання_дозволу_на_відправку_email_у_aws[Отримання дозволу для поштового сервера в AWS-середовищі]

=== Налаштування обмежень доступу на мережевому рівні до адміністративних ендпоінтів Платформи

* xref:admin:registry-management/control-plane-cidr-access-endpoints.adoc#_обмеження_доступу_до_платформних_інфраструктурних_та_інших_компонентів[Обмеження доступу до платформних, інфраструктурних та інших компонентів]

=== Захист інфраструктури цільових оточень

За безпеку цільової інфраструктури несе відповідальність адміністратор оточення, згідно з відповідними організаційними політиками.

=== Оновлення інфраструктурних компонентів Платформи (Оновлення cluster-mgmt)

* xref:admin:update/update_cluster-mgmt.adoc[]

=== Міграція реєстрів

* xref:admin:migrate-registry.adoc[]

=== Спеціальні кроки для оновлення кластера Платформи

* xref:admin:update/special-steps-for-update/special-steps-overview.adoc[]

== Підготовчі дії на рівні реєстрів

=== Рекомендації щодо формування команд підтримки реєстрів L1, L2, L3

Обов'язкові пункти: ::

. Обсяг підтримки:

* Визначте обов'язкові рівні підтримки (L1, L1.5, L2, L3 тощо). Зверніть увагу на те, що кожний рівень вимагає окремого набору навичок та ресурсів.
* Окресліть середовища, які підтримуються (Prod, Stage, CR, Monitoring, Automation). Команда повинна мати достатній досвід роботи у цих середовищах.

. Покриття часом (8*5, 16*5, чергування, календар свят і т.д.). Пам'ятайте про необхідність підтримки у неробочі години та святкові дні.

. Підтримка мов. Ваша команда повинна вміти ефективно спілкуватися на потрібних мовах.

. Очікувана кількість запитів. Це допоможе вам визначити потрібну кількість членів команди.

. Основний часовий пояс для бізнесу. Це важливо для планування робочого часу команди.

. Вимоги до SLA/SLO/OLA. Вони визначають очікувані рівні якості сервісу.

. Канали комунікації (запити, телефонна лінія тощо). Команда повинна бути готова працювати з потрібними каналами комунікації.

. Інструменти та технологічний стек. Ваша команда повинна володіти потрібними технологіями.

. Кількість користувачів системи. Це також впливає на кількість потрібних членів команди.

Додатково також зверніть увагу на наступні пункти: ::

. Звіт про запити (tickets dump). Це допоможе вам краще зрозуміти потреби користувачів.

. Розмір наявної команди (якщо є). Це допоможе вам оцінити, чи потрібно вам додаткові ресурси.

. Залежності від команд третіх сторін. Це важливо для планування співробітництва та координації.

=== Створення реєстру та його первинна перевірка

Після xref:admin:registry-management/control-plane-create-registry.adoc[], виконайте первинні перевірки, щоб упевнитися, що усе встановлено добре.

Необхідно виконати наступні перевірки:

. Перевіряємо, що в OpenShift розгорнувся простір імен (namespace) вашого реєстру. Ви маєте бачити лише свій проєкт реєстру.
. Перевіряємо, що у ньому доступні усі поди та роути.
* Перевіряємо реєстрові Gerrit, Jenkins та Nexus.
* Виконуємо вхід до Keycloak, перевіряємо, чи є там реєстрові реалми, зокрема:

* `-officer-portal`
* `-citizen-portal`
* `-admin`
* `-external-system`

. Отримуємо логін та пароль для входу у Control Plane у адміністратора Платформи.
. Перевіряємо доступ до Control Plane та виконуємо вхід.
. Перевіряємо, що бачимо лише свій реєстр у Control Plane.
. Перевіряємо, що можливо внести зміни до конфігурації реєстру. Наприклад, додаємо адміністратора реєстру.
. Перевіряємо, що коректно працює автентифікація.

* Створюємо посадову особу у Keycloak та виконуємо вхід до Кабінету посадової особи із КЕП.
* Виконуємо вхід до Кабінету отримувача послуг з КЕП.

TIP: Також корисно буде ознайомитися зі сторінкою xref:admin:registry-management/control-plane-edit-registry.adoc[].

=== Оновлення компонентів реєстру

* xref:admin:update/update-registry-components.adoc[]

=== Типові проблеми з аутентифікацією у користувачів реєстру та при підписанні БП

* Перевірка ключа на https://id.gov.ua/sign[].
* Перевірка, що на подах DSO реєстру і проєкті `user-management` встановлені останні сертифікати цифрового підпису.

Якщо ключі та сертифікати застаріли або з якоїсь причини не працюють, їх необхідно оновити.

[TIP]
====
Детальніше про оновлення ключів та сертифікатів цифрового підпису читайте на сторінках:

* xref:admin:registry-management/system-keys/control-plane-platform-keys.adoc[]
* xref:admin:registry-management/system-keys/control-plane-registry-keys.adoc[]
====

=== Спеціальні кроки для оновлення реєстру

* xref:admin:update/special-steps-for-update/special-steps-overview.adoc[]

=== Зміна режиму розгортання реєстру з production на development

* xref:registry-develop:registry-admin/change-dev-prod-mode.adoc[]

=== Особливості розгортання регламенту в production-середовищі

Структура регламенту: ::

* xref:registry-develop:registry-admin/regulations-deploy/registry-admin-deploy-regulation.adoc[]

Розгортання регламенту: ::

* xref:registry-develop:registry-admin/regulations-deploy/registry-regulations-structure.adoc[]

Зміна режиму розгортання регламенту: ::

* xref:registry-develop:registry-admin/change-dev-prod-mode.adoc[]

Що необхідно для початку роботи: ::

* xref:registry-develop:study-project/index.adoc#preconditions-setup[Що необхідно для початку роботи?]

Пайплайн публікації регламенту: ::

* xref:platform-develop:registry-regulations-deployment.adoc[]

Автоматична валідація при внесенні змін до регламенту: ::

* xref:registry-develop:registry-admin/regulations-deploy/registry-regulations-auto-validation.adoc[]

Кабінет адміністратора регламентів: ::

* xref:registry-develop:registry-admin/admin-portal/overview.adoc[]

Інші корисні документи: ::
* xref:registry-develop:study-project/index.adoc[]

=== Рекомендований алгоритм внесення змін до регламенту реєстру

Після розгортання реєстру адміністратором Платформи, реєстр матиме порожній репозиторій Gerrit із регламентом.

Внесення змін до регламенту з нуля або згодом, під час оновлення регламенту, процесуально не відрізняється й відбувається за GitOps-підходом.

[TIP]
====
.Що таке GitOps-підхід?
[%collapsible]
=====
GitOps -- це підхід до керування інфраструктурою та розгортання програмного забезпечення, який базується на використанні системи контролю версій Git.

У GitOps-підході всі конфігураційні файли, описи інфраструктури та код програмного забезпечення зберігаються в репозиторії Git. Це означає, що будь-які зміни в інфраструктурі або програмному забезпеченні відбуваються через коміти до Git-репозиторію.
=====
====

Файли оновлюються в локальному середовищі, публікуються до віддаленого Gerrit-репозиторію. Пайплайн публікацій відстежує зміни у файлах директорій регламенту, і при git merge змін до майстер-гілки репозиторію, спрацьовує пайплайн публікацій *Master-Build-registry-regulations*, який збирає увесь код. Після виконання пайплайну, зміни набувають чинності, а регламент оновлюється до версії останнього коміту.

Є можливість оновлювати регламент як за хардкорним шляхом, так і за більш зручним.

_Хардкорний шлях_ -- це робота з каталогами файлів, git та Gerrit, через Git Bash консоль, або в інших інструментах, робота з Jenkins тощо.

Алгоритм внесення змін:

. Клонуйте на локальну машину порожній Gerrit-репозиторій із регламентом реєстру.
. Додайте до каталогу _registry-regulations_ відповідні файли:
* Створіть модель даних реєстру (_data-model_).
* Змоделюйте бізнес-процеси (_bpmn_).
* Змоделюйте UI-форми до бізнес-процесів (_forms_).
* Визначте ролі для вашого реєстру (_roles_).
* Визначте доступи до бізнес-процесів для відповідних ролей (_bp-auth_).
* Визначте інші налаштування, передбачені регламентом вашого реєстру (сповіщення, витяги, глобальні змінні тощо).
. Виконайте наступні команди:
+
--
----
git add --all
----
----
git commit -m "message commit"
----
----
git push refs/for/master
----
--

. Пройдіть рецензування коду -- Code Review. Спочатку має пройти автоматичний Jenkins процес перевірки *MASTER-Code-review-registry-regulations*, далі потрібно, щоб уповноважений адміністратор підтвердив внесення змін до регламенту.

. Виконайте git merge (злиття змін) до master-гілки Gerrit репозиторію із регламентом реєстру.

_Зручний спосіб_ -- це використання нового порталу адміністратора регламенту та його вбудованих можливостей.

Алгоритм внесення змін:

. Увійдіть до Кабінету адміністратора регламентів.
. Створіть нову версію-кандидат на внесення змін.
. Додайте відповідні зміни:
* Створіть модель даних реєстру (Таблиці).
* Змоделюйте бізнес-процеси (Моделі процесів).
* Змоделюйте UI-форми до бізнес-процесів (Форми).
* Визначте інші налаштування, передбачені регламентом вашого реєстру.

. Перейдіть на вкладку Огляд версії та натисніть Застосувати зміни до майсте-гілки.

Внаслідок цього створиться автоматичний запит на внесення змін до регламенту, який автоматично підтвердиться, зміни опублікуються в регламенті.

* xref:registry-develop:registry-admin/regulations-deploy/registry-admin-deploy-regulation.adoc[]
* xref:registry-develop:registry-admin/admin-portal/overview.adoc[]

Інші корисні документи: ::

* xref:registry-develop:study-project/index.adoc[]

=== Налаштування ключів та сертифікатів цифрового підпису реєстру

. Створення ключів та сертифікатів цифрового підпису відбувається під час розгортання реєстру (_див. xref:admin:registry-management/control-plane-create-registry.adoc[]_).
* Загальна інформація про типи ключів на Платформі реєстрів: xref:admin:registry-management/system-keys/system-keys-overview.adoc[]
. Оновлення ключів та сертифікатів цифрового підпису:
* xref:admin:registry-management/system-keys/control-plane-registry-keys.adoc[]

=== Налаштування поштового клієнта (SMTP сервер)

* xref:admin:installation/internal-smtp-server-setup.adoc[]
* xref:registry-develop:registry-admin/user-notifications/email/config-smtp-server.adoc[]

=== Налаштування взаємодії з Дією

* Загальний алгоритм описаний тут: xref:registry-develop:registry-admin/external-integration/ext-integration-overview.adoc#exchange-data-ext-system[Обмін даними з іншими системами за допомогою REST]

* xref:registry-develop:registry-admin/external-integration/cp-integrate-ext-system.adoc[]

* xref:registry-develop:bp-modeling/bp/rest-connector.adoc[]

=== Налаштування взаємодії з іншими реєстрами та зовнішніми системами

* Загальний алгоритм описаний тут: xref:registry-develop:registry-admin/external-integration/ext-integration-overview.adoc#exchange-data-ext-system[Обмін даними з іншими системами за допомогою REST]

* xref:registry-develop:registry-admin/external-integration/cp-integrate-ext-system.adoc[]

* xref:registry-develop:bp-modeling/bp/rest-connector.adoc[]

* xref:registry-develop:registry-admin/external-integration/rest-api-no-trembita.adoc[]

=== Налаштування взаємодії через Трембіту на базі стандартних конекторів

* Загальний алгоритм описаний тут: xref:registry-develop:registry-admin/external-integration/ext-integration-overview.adoc#exchange-data-trembita[Обмін даними за допомогою SOAP через програмний інтерфейс "Трембіта"]
* xref:registry-develop:registry-admin/external-integration/cp-integrate-trembita.adoc[]

* xref:registry-develop:bp-modeling/external-integration/api-call/connectors-external-registry.adoc[]

=== Конфігурація реєстру під навантаження

Кожен реєстр має свої специфічні задачі, логіку роботи й налаштування, а звідси -- й набір певних сервісів, які залучені для виконання цих задач більшою або меншою мірою.

Відповідно до навантаження на певний реєстр, а це напряму залежить від кількості запитів від активних користувачів, сервіси реєстру потребують певної кількості ресурсів та можуть бути розгорнуті в одному та більше екземплярах.

Для прикладу, під навантаженням у 1500 користувачів на 1 годину, умовний реєстр повинен мати приблизно наступну конфігурацію:

.Конфігурація горизонтального масштабування реєстру
[width="100%",cols="72%,28%",options="header",]
|===
|Сервіс |Кількість копій (інстансів)
|Admin portal/Officer portal/Citizen portal |1
|BPMS |4
|BP WS gateway |1
|BP admin portal |1
|DB/DB read replica |1
|Digital document service |1
|Digital signature service |3
|Excerpt services |1
|Form schema provider |3
|Form schema validator |3
|Istio gateway |1
|Infra (jenkins/gerrit/nexus etc.) |1
|Kafka services (exporter, schema registry) |1
|Kafka cluster |3
|Kafka cluster zookeeper |3
|Kong |4
|Language server |1
|Process history rest api |2
|Process history persistence service |1
|Redash services |1
|Registry rest api |4
|Registry kafka api |4
|Redis rfr (1000m) |2
|Redis rfs |3
|User settings rest api |1
|User task management |3
|User process management |2
|Wiremock |1
|===

TIP: Детальніше про тестування навантаження див. на сторінці xref:testing:perf-test/perf-test-overview.adoc[].

Залежно від потреб вашого реєстру, можливо змінювати конфігурації певних сервісів, зокрема:

* Ви можете масштабувати конфігурацію сервісів вертикально. Зробити це можна двома способами:

** В OpenShift-консолі:

*** Оберіть проєкт із вашим реєстром > *Workloads* > *Deployments* > Відкрийте налаштування сервісу > *YAML*.

*** У розділі `spec.containers.resources` ви можете встановити необхідні параметри конфігурації для *CPU* та *memory*.
*** У розділі `spec.containers.resources.env` ви можете визначити змінні оточення для ваших застосунків, як-то `JAVA_OPTS`, змінні для Ceph тощо.

+
image:platform-develop:platform-prod-deployment/platform-prod-deploy-resources.png[]

** В адміністративній панелі Control Plane, у розділі керування ресурсами для сервісів.
+
TIP: Детальніше про це ви можете дізнатися на сторінці xref:admin:registry-management/control-plane-registry-resources.adoc[].

* Ви можете використовувати конфігурацію компонента Horizontal Pod Autoscaler для горизонтального масштабування, внаслідок збільшення кількості реплік.
+
** Оберіть проєкт із вашим реєстром > *Workloads* > *HorizontalPodAutoscalers* > Відкрийте налаштування сервісу > *YAML*.
** Параметри `spec.minReplicas` та `spec.maxReplicas` дозволяють встановити мінімальну та максимальну кількість реплік для под сервісу.

+
image:platform-develop:platform-prod-deployment/platform-prod-deploy-HPA.png[]

=== Планування місця для сховищ реєстру

Компоненти реєстру зберігаються у томах сховища Ceph. Конфігурації доступного місця на дисках для таких компонентів доступні через OpenShift-консоль, у розділі *Storage* > *PersistentVolumeClaims*, у проєкті вашого реєстру.

Розмір дисків можна змінювати відповідно ваших потреб. Для цього перейдіть до *Storage* > *PersistentVolumeClaims* > YAML та встановіть бажане значення для розміру диска параметром `status.capacity.storage`.

image:platform-prod-deployment/platform-prod-deploy-persistent-volume-claims-registry.png[]

image:platform-prod-deployment/platform-prod-deploy-persistent-volume-claims-registry-1.png[]

NOTE: Кожний сервіс запускається із налаштуваннями розміру дисків за замовчуванням. Ви як адміністратор можете залишити ці налаштування, або якщо ви чітко знаєте, що виділеного розміру дисків недостатньо для ваших потреб, тоді томи можна розширити одразу. Рекомендоване значення для збільшення місця на диску +50%. Наприклад, розмір диска для компонента `jenkins-data` за замовчування -- 10Gi. Тоді для початку ви можете збільшити місце до 15Gi.

[TIP]
====
Детальніше про налаштування файлової системи ви можете дізнатися на сторінках:

* xref:admin:file-system/ceph-space.adoc[]
* xref:admin:file-system/ceph_scaling.adoc[]
* xref:admin:file-system/s3/lifecycle-policy.adoc[]
====

=== Отримання доступу для L2 до системи сповіщень, моніторингу та логування у реєстрі

Надайте права доступу до компонента `cluster-mgmt` у розділі Керування Платформою на Control Plane. Все, що потрібно зробити, -- це створити адміністратора Платформи, й відповідні права автоматично додадуться.

TIP: Детальніше про це див. сторінку xref:admin:registry-management/control-plane-assign-platform-admins.adoc[].

=== Налаштування бекапів реєстру

Платформа підтримує два види резервного копіювання компонентів реєстру:

* Ручне резервне копіювання (_див. детальніше -- xref:admin:backup-restore/control-plane-backup-restore.adoc[]_)
* Автоматичне резервне копіювання через встановлений розклад: (_див. детальніше -- xref:admin:backup-restore/backup-schedule-registry-components.adoc[]_)

Після створення резервної копії, реєстр можна відновити безпосередньо з такої копії.

=== Налаштування реплікації файлів реєстру

Платформа надає вбудований механізм реплікації даних між S3-сумісними сховищами.

Реплікація полягає в автоматичному копіюванні даних з одного бакета до іншого, що може бути корисним, наприклад, для створення резервних копій даних в інших географічних регіонах, що забезпечує високу доступність та надійність.

Реєстр містить дані, які є необхідними для бізнес-процесів, зокрема тимчасові дані, історія виконання процесів тощо). Ці дані зберігаються у вигляді `ObjectBucketClaim` (`obc`) в S3-бакетах. Реплікація цих бакетів відбувається автоматично. Ви можете налаштувати резервне копіювання для таких реплікацій через адміністративну панель Control Plane.

TIP: Детальніше про це див. на сторінці xref:admin:backup-restore/backup-schedule-registry-components.adoc#replication-schedule-backup[Резервне копіювання реплікацій об'єктів S3].


=== Інтеграція з http://id.gov.ua[id.gov.ua] та використання стилізованого віджета

* Особливості налаштування автентифікації через ID.GOV.UA див. на сторінці xref:registry-develop:registry-admin/cp-auth-setup/cp-auth-setup-officers.adoc[].

* Особливості автентифікації через ID.GOV.UA користувачами див. на сторінці xref:user:citizen-officer-portal-auth.adoc[].

* Посилання на офіційне джерело ID.DOV.UA: http://id.gov.ua[].

=== Визначення порядку надання доступу надавачам послуг адміністратором доступу

. Створюємо посадову особу.
. Додаємо їй ролі у Keycloak, зокрема системну роль `officer` та інші передбачені логікою реєстру ролі.
. Ці самі ролі визначаємо на рівні Gerrit-репозиторію з регламентом реєстру у файлі _roles/officer.yml_.
. Налаштовуємо доступи до певних бізнес-процесів для відповідних ролей у файлі _bp-auth/officer.yml_.

=== Рекомендації щодо онбордингу надавачів та отримувачів послуг

* Надавачам послуг:

** xref:registry-develop:registry-admin/cp-auth-setup/cp-officer-self-registration.adoc[]
** xref:registry-develop:best-practices/bp-officer-self-register-manual.adoc[]
** xref:registry-develop:best-practices/bp-officer-self-register-auto.adoc[]

* Отримувачам послуг:
+
//TODO:Оновлено процес онбордингу в рамках https://jiraeu.epam.com/browse/MDTUDDM-17161
** xref:arch:architecture/platform/operational/user-management/citizen-onboarding.adoc[]

=== Аутентифікація юридичних осіб до реєстру як отримувачів послуг

* xref:registry-develop:registry-admin/cp-auth-setup/cp-auth-setup-citizens.adoc[]
* xref:user:citizen-officer-portal-auth.adoc[]

=== Рекомендації щодо уникнення "покинутих" бізнес-процесів користувачами

"Покинуті" бізнес-процеси" (abandoned business processes) в контексті Camunda Engine належать до бізнес-процесів, які були розпочаті, але не були завершені або виконані до кінця. Це може статися у випадках, коли бізнес-процес переривається, скасовується або припиняється з певної причини, зокрема помилка або виняток тощо.

Є декілька способів розв'язання цієї проблеми:

. Уникайте "покинутих" процесів. Цього можна досягти через правильне моделювання, зокрема виходом є призначення таймерів завершення процесу (*Timer Boundary Event*) типу *Duration* на кожній користувацькій задачі (User Task).
+
.Бізнес-процес із таймером завершення
image::platform-prod-deployment/platform-prod-deploy-abandoned-bp.png[]
+
NOTE: Загальна рекомендація: моделювати таймер більш як на 14 днів, адже логи бізнес-процесів за замовчуванням зберігаються в Elastic Search протягом 14 днів і згодом видаляються. Після видалення логів буде неможливо ідентифікувати помилку.

+
[TIP]
====
* Детальніше про таймери дивіться на сторінці xref:registry-develop:bp-modeling/bp/bpmn/events/timer-event.adoc[].
* Також про налаштування таймерів можна переглянути на сторінці xref:registry-develop:best-practices/bp-timer-launch.adoc[]
====

. Якщо вже сталося так, що деякі процеси не завершилися, скористайтеся інструментом для моніторингу та адміністрування бізнес-процесів -- Business Process Administration Portal (Camunda Cockpit). Через його інтерфейс можна по-одному видалити усі такі процеси.
+
TIP: Детальніше про це див. на сторінці xref:registry-develop:registry-admin/registry-admin-bp-management-cockpit.adoc[].

. Якщо сталося так, що "покинутих" процесів дуже багато, наприклад, тисячі, то видаляти їх по-одному буде, м'яко сказати, незручно. Для розв'язання цієї задачі скористайтеся видаленням одразу всіх таких процесів за допомогою скриптів напряму.
+
TIP: Детальніше див. на сторінці https://kb.epam.com/pages/viewpage.action?pageId=1796952627[].

//TODO: Move instruction from KB to Antora

=== Налаштування рейт-лімітів

* xref:registry-develop:registry-admin/api-rate-limits.adoc[]

=== Налаштування обмежень доступу на мережевому рівні до адміністративних ендпоінтів реєстру, кабінетів отримувачів та надавачів послуг (CIDR)

* xref:admin:registry-management/control-plane-cidr-access-endpoints.adoc#cidr-registry-components[Обмеження доступу до компонентів реєстру]

=== Обмеження доступу на рівні IP до SOAP-роутів ШБО "Трембіта"

* xref:admin:registry-management/control-plane-soap-api-access-trembita.adoc[]

=== Налаштування власних DNS-імен

* xref:admin:registry-management/custom-dns/custom-dns-overview.adoc[]

=== Додавання нових користувачів (через Keycloak та імпорт)


* xref:registry-develop:registry-admin/create-users/manual-user-creation.adoc[]

* xref:registry-develop:registry-admin/create-users/import-users-officer.adoc[]

=== Призначення адміністраторів Платформи та реєстру

* xref:admin:registry-management/control-plane-assign-platform-admins.adoc[]

* xref:registry-develop:registry-admin/create-users/create-registry-admins.adoc[]

=== Розгортання геосервера та робота з геоданими у реєстрі

* xref:registry-develop:registry-admin/geoserver.adoc[]

=== Первинне завантаження та дозавантаження даних до системи

. Первинне завантаження/дозавантаження даних до таблиць-довідників через процедуру на рівні моделі даних:

* xref:registry-develop:data-modeling/initial-load/index.adoc[]

. Завантаження даних із CSV-файлу масивом до БД (в рамках виконання бізнес-процесу):

* xref:registry-develop:bp-modeling/bp/loading-data-from-csv.adoc[]

[TIP]
====
Додаткові корисні матеріали:

Навчальний курс (Приклад первинного завантаження даних при проходженні тестового завдання):

* xref:registry-develop:study-project/study-tasks/task-1-registry-db-modeling.adoc[]
====

== Рекомендації щодо нефункціонального тестування реєстрів на платформі

=== Тестування продуктивності

Тестування продуктивності Платформи проводиться на базі потужностей «EPAM» під конкретний реліз із використанням попередньо визначеної конфігурації кластера Openshift, окремо для кожного розгорнутого реєстру із певною кількістю активних користувачів при плановому повному навантаженні в робочий час.

Тестування продуктивності виконується інструментом *Carrier* -- комплексним інструментом, що допомагає вимірювати, аналізувати й оптимізувати продуктивність роботи сервісів Платформи та реєстрів, які на ній розгорнуті.

TIP: Детальніше про результати тестів ви можете дізнатися у розділі xref:testing:perf-test/perf-test-overview.adoc[].

=== Тестування безпеки

Платформа реєстрів будується на основі методології безпечної розробки програмного забезпечення *DevSecOps*, відповідно до якої виконуються автоматичні перевірки безпеки на наявність відомих вразливостей. На регулярній основі проводиться тестування на проникнення (penetration testing).
