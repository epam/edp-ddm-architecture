:toc-title: On this page:
:toc: auto
:toclevels: 5
:experimental:
:important-caption:     ВАЖЛИВО
:note-caption:          ПРИМІТКА
:tip-caption:           РЕСУРС
:warning-caption:       ПОПЕРЕДЖЕННЯ
:caution-caption:       УВАГА
:example-caption:           Приклад
:figure-caption:            Зображення
:table-caption:             Таблиця
:appendix-caption:          Додаток
:sectnums:
:sectnumlevels: 5
:sectanchors:
:sectlinks:

= Functional  Testing

CAUTION: Сторінка у процесі розробки.

== Introduction

=== Purpose
Functional testing is a crucial aspect of the software development lifecycle that focuses on verifying whether a software application's individual functions or features work as intended and meet the specified requirements. The primary purpose of functional testing is to ensure that the software behaves correctly according to the defined functional specifications and user expectations. It helps identify defects and discrepancies in the software's behavior, ensuring that it delivers the intended value to users.

=== Objectives

The main objectives of function testing are:


* Validating Requirements: Functional testing helps verify that the software application aligns with the documented requirements and specifications. By executing test scenarios based on these requirements, it ensures that the software meets the intended objectives.
* Defect Detection: Functional testing aims to identify defects, bugs, and errors in the software. By systematically testing various functions and features, QA teams can uncover issues that might negatively impact the user experience or hinder the application's functionality.
* User Experience Assurance: Functional testing ensures that the software provides a seamless and user-friendly experience. It validates that end-users can interact with the application without encountering unexpected errors or incorrect behavior.
* Regression Testing: As software evolves and new features are added, functional testing helps prevent regression issues. It ensures that new code changes do not break existing functionality, maintaining the overall stability of the application.
* Validation of Business Logic: Many software applications rely on complex business rules and logic. Functional testing validates that these rules are correctly implemented and produce the expected results.
* Verification of Integration: Functional testing is essential for ensuring that different components and modules of the software integrate correctly and collaborate as intended. This helps identify integration issues early in the development process.
* Compliance and Standardization: In some industries, software needs to comply with specific regulations or standards. Functional testing verifies that the software adheres to these requirements.
* Enhancing Product Quality: By identifying and rectifying defects early in the development cycle, functional testing contributes to overall product quality improvement. This results in a more reliable and robust software application.
* Risk Mitigation: Effective functional testing minimizes the risk of releasing a faulty or substandard product to end-users. By identifying and resolving issues before deployment, it helps mitigate potential financial, reputational, and legal risks.
* Customer Satisfaction: Ultimately, the goal of functional testing is to ensure that the software meets user expectations and delivers value. By identifying and addressing functional issues, it enhances customer satisfaction and loyalty.

=== Scope of functional testing

Scope of functional testing includes all registry and platform components of the Platform.


== Functional Testing Approach
=== Functional Testing Methodologies

The functional testing follow a combination of manual and automated testing approaches. It includes unit testing, integrational testing, system testing, acceptance testing, regression testing and fuctional testing best practicies.

Here are the list of used functional testing methodologies:

* **Unit testing**: Unit testing involves testing individual components, functions, or modules in isolation. Each unit is tested independently to ensure its correctness and functionality. The primary goal of unit testing is to validate that each unit of code performs as intended. It helps identify defects early in the development cycle and ensures that smaller components work correctly before being integrated into the larger system. Unit tests are typically automated and focus on specific input-output scenarios. Developers often write unit tests as part of the development process to ensure code reliability and maintainability.

* **Integration Testing**: Integration testing verifies the interactions and interfaces between different components or modules. It ensures that integrated components work together as expected. The purpose of integration testing is to detect errors related to data exchange, communication, and collaboration between components. It helps identify issues that might arise due to the integration of individual units. Integration testing can be done using various strategies like top-down, bottom-up, or sandwich approaches. Testers might use stubs or mocks to simulate the behavior of dependent components that are not yet available.

* **System Testing**: System testing evaluates the entire software application as a complete and integrated entity. It involves testing the application's functionalities, interfaces, and interactions in a holistic manner. The goal of system testing is to validate that the software meets the specified requirements and behaves as expected in different scenarios. It covers end-to-end scenarios and assesses the software's overall behavior. System testing often includes different types of tests, such as functional, performance, security, and usability testing. It focuses on user scenarios and real-world usage.

* **Acceptance Testing**: Acceptance testing evaluates whether the software meets the acceptance criteria defined by stakeholders and users. It determines whether the software is ready for deployment. he purpose of acceptance testing is to ensure that the software aligns with user expectations and business requirements. It aims to verify that the software delivers value and meets the needs of its intended users. Acceptance testing includes user acceptance testing (UAT), alpha testing (internal testing), and beta testing (external testing with a limited user group). It provides a final check before releasing the software to users.

* **Regression testing**: Regression testing ensures that recent code changes or enhancements to a software application do not negatively impact its existing functionality. It involves re-running a predefined set of test cases to validate that new code changes have not introduced unintended side effects or defects.

* **Installation testing**: Installation testing aims to confirm that the software can be installed, configured, and removed without any complications, errors, or adverse impacts on the target system. This type of testing is essential because even a well-developed software application can suffer from installation-related issues that might disrupt its functionality or cause conflicts with other software components.



=== Tools and Technologies Used

The following types, tools and technologies are used during functional testing:

[options="header"]
|===
| Functional testing type | Toolset
| Unit testing
| JUnit, Mokito

| Integration Testing
| JUnit, AssertJ, RestAssured

| System testing
| JUnit, Selenide, RestAssured

| Acceptance Testing
| JUnit, Selenide, Moon

| Regression Testing
| JUnit, Selenide, Moon, RestAssured
|===

== Defect managing
=== Registration and Processing of Defects

Newly discovered defects are divided into two types based on their causes:

- Defects found during user story validation, concerning compliance with specified criteria.
- Regression defects found during regression test runs.

The first type is entered into the defect tracking system as sub-tasks of the corresponding story, marked with the [Bug] attribute in the sub-task title. These defects should be resolved during the current iteration within the context of the given story. However, if resolving these defects promptly is not possible due to their complexity or lack of time, they are transformed into "Bug" entities and linked to the current story, appearing in the overall list of defects.

Regression defects are immediately registered as "Bug" entities and may not have references to stories.

All defects registered in the defect tracking system should be labeled with appropriate tags and components that explain the nature of the defect's occurrence:

- 'regression', 'integration', 'PROD', 'UAT', 'automation' - stage during which the defect was reproduced.
- 'low-code', 'data-factory', 'ifra', 'register' - component affected by the occurrence of this error.
- Team responsible for resolving the error.
- Iteration number during which the defect was found.

The defect processing process is as follows:

* All defects are prioritized according to the conditions in section "Defect Priorities" and reviewed following section "Defect Importance Determination Process"
* When a defect is resolved, it is marked as "Ready for QA" and forwarded to the defect registrar. If the defect registrar is a representative of the client, it is forwarded to the testing team leader.
* The defect registrar reviews the defect and, if resolved, marks it as closed. If the defect still reproduces, it is returned to development with a "To Do" status.
* Resolved defects must have a 'fix version' - the build version in which the defect was resolved.

=== Defect Priorities

To determine the severity of defects and their impact on further development, the following criteria (not listed in the table) should be considered:

[options="header"]
|===
| Priority Level | Description | Impact on Testing
| 0 (Blocker)
| Platform stops functioning, and there is no workaround.
| Testing team sends the build back to development.

| 1 (Critical)
| Functionality is not working.
| Testing team does not provide a test report for release into production environment in case a priority 1 defect is detected.

| 2 (Major)
| Critical business requirements are broken.
| Presence of priority 2 defects requires additional agreement with the business team and project management.

| 3 (Minor)
| Functionality is not working according to design, but an acceptable workaround exists.
| Business and development teams agree on the necessity of defect resolution within the current release.

| 4 (Trivial)
| Minor changes needed in functionality - aesthetic or cosmetic changes.
| Business and development teams agree on the necessity of defect resolution within the current release.
|===

=== Defect Importance Determination Process

During the stages of development, regression/stabilization, the development team conducts internal and external sessions to review the list of defects, in order to determine their current priorities and statuses. A defect should be refined by indicating clarifying statuses (provided in the table) and providing a detailed comment.

The responsible individuals for closing defects are the testing team leader and the defect registrar.

[options="header"]
|===
| Status | Explanation | Will Be Resolved?

| Not a bug: Cannot reproduce
| Defect that cannot be reproduced at the moment
| No

| Not a bug: Duplicate
| Defect is already registered
| No

| Done
| Testing completed fully and functionality is working
| Yes

| Won't Do
| Defect has minimal impact on business and won't be resolved
| No

| Fixed
| Testing conducted comprehensively after changes were made
| Yes

| Obsolete
| Defect is outdated
| No

| Cancelled
| Cancelled functionality
| No

| Implemented
| Technical error that doesn't require testing
| Yes

| Deferred
| Awaiting resolution in upcoming releases and planned functionality
| Yes

| Not a Bug
| Is not a defect
| No
|===

== Testing approaches description

=== Acceptance testing
This testing method involves verifying a build that is a potential candidate for further deployment to the production environment. It includes the following procedures:

* Coordination and creation of acceptance scenarios with client representatives
* Establishment of the necessary testing infrastructure
* Search or creation of required test data
* Direct execution of acceptance scenarios and agreement of their results with client representatives.

The tests performed during this phase require confirmation of successful completion - the presence of snapshots, logs, and detailed reproduction steps.

=== Integration Testing

This testing method follows the following approach:

* Building a matrix of external (integrations with KEA, Trembita, data.gov.ua, etc.) and internal (integration between low-code platforms, registries, etc.) integrations.
* Designing testing scenarios for the listed integrations and preparing test data.
* Developing an automated solution for testing integration data and forming test groups if such a solution can be built.
* Manual tests that form the regression suite should be executed regularly and updated, and they should be added to the appropriate test groups in TMS.

Such tests involve testing integrations with real instances of external test systems and require confirmation of successful execution - the presence of snapshots, logs, and detailed reproduction steps.

Artifacts resulting from this type of testing:

* Automated tests added to relevant test groups (nightly runs, integration, etc.).
* Manual tests added to relevant test groups (regression, integration, etc.).
* Updated requirement coverage matrices for tests and automated tests.
* Results of test runs should be well-structured and accessible to all stakeholders:
** Reports of automated test runs on Jenkins.
** Reports of manual test runs in TMS.
* Formulated evidence of test execution - snapshots, attachments, and logs.

=== Regression Testing

This testing method follows the following approach:

* Develope an automated solution for test goal management.
* Automated solutions are designed based on their access levels:
** Backend - This level involves direct access to contracts and their interactions during testing.
**  UI - This level involves building automated solutions for testing platform UI functionality.
* Automated testing encompasses the following methods:
** Functional testing
** Installation testing
** Integration testing
* Developed automated tests are added to corresponding test groups (nightly runs, quality gate, and coverage zone).
* A coverage matrix of requirements with automated tests is developed.
* Developed automated tests reference the requirement they verify.
* The number of tests should be evenly distributed across testing levels, forming a balanced testing pyramid.
* Several levels of quality gates are integrated into the CI/CD process.
* Test data comprises synthetic data resembling industrial data or a sample of real industrial data (if accessible).
* To ensure the stability of the automated solution, virtualization tools are utilized.

Artifacts of this testing include:

* Documented design of the automated solution.
* Developed code conventions and guidelines for automated test developers.
* Established principles and rules for conducting code reviews.
* Description of quality gate levels and test categories.

=== Installation testing

This functionality involves only manual testing, which is added to the regression test suite. As testing requires a separate testing environment and is resource-intensive, it will be executed as needed and agreed upon with the infrastructure team.

Artifacts of this testing include:

* Manual tests added to relevant test groups (Regression, Integration, etc.).
* Updated requirement coverage matrices for tests and automated tests.
* Results of test runs should be well-structured and accessible to all stakeholders.
