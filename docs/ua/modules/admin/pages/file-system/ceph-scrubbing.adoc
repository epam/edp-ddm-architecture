= Первинні налаштування процесів Scrubbing та Deep Scrubbing у Ceph-кластері
include::platform:ROOT:partial$templates/document-attributes/default-set-ua.adoc[]

include::platform:ROOT:partial$admonitions/language-ua.adoc[]

Після встановлення Платформи на *OKD-кластер* рекомендується виконати первинні налаштування процесів *scrubbing* та *deep scrubbing* у Ceph-кластері для забезпечення цілісності даних та стабільної роботи системи.

[IMPORTANT]
====
Ці первинні налаштування призначені для **виробничих (production) оточень** і рекомендовані до застосування одразу після встановлення Платформи.
====

[#scrubbing-overview]
== Загальні відомості

*Scrubbing* (*скрабування*) та *deep scrubbing* -- це фонові процеси перевірки даних, що виконуються у Ceph-кластері для виявлення пошкоджених або невідповідних об'єктів:

- *Scrubbing* перевіряє контрольні суми об'єктів у *Placement Groups (PG)* та дозволяє виявити помилки в метаданих.
- *Deep scrubbing* виконує глибшу перевірку, порівнюючи реальні дані об'єктів з їхніми хешами, щоб виявити пошкоджені або невідповідні дані.
- Scrubbing виконується автоматично, але може бути налаштований вручну для оптимізації продуктивності.

[#why-scrubbing-matters]
== Чому важливо контролювати scrubbing

Навантаження, яке створює процес scrubbing, може впливати на продуктивність кластера, тому рекомендується:

- Запланувати звичайний scrubbing на нічний час або в періоди мінімального навантаження.
- Запускати *deep scrubbing* вручну або за чітким розкладом.
- Обмежувати використання ресурсів для запобігання впливу на продуктивність кластера.

[#recommended-initial-settings]
== Рекомендовані первинні налаштування Scrubbing та Deep Scrubbing

Виконайте такі рекомендовані дії у Ceph-кластері після встановлення Платформи на OKD-кластер:

[%interactive]
* [ ] ① Налаштуйте загальні параметри scrubbing.
* [ ] ② Вимкніть автоматичний запуск deep scrubbing.
* [ ] ③ Налаштуйте розклад звичайного scrubbing.

Детальний опис кожного пункту наведено нижче.

[#scrubbing-general-settings]
=== Налаштування загальних параметрів scrubbing

[#scrub-priority]
==== Налаштування пріоритету процесів scrubbing

Рекомендовано понизити пріоритет процесів скрабування, запущених вручну, для *однозначного надання переваги запитам користувачів*. Наприклад, встановіть значення `10`.

.Параметр Requested Scrub Priority
[source,bash]
----
osd_requested_scrub_priority 10
----

Змінити цей параметр можна через *UI Ceph Dashboard*. Для цього виконайте наступні кроки:

. Відкрийте *UI Ceph Dashboard*.
. Перейдіть до menu:Cluster[OSDs > Cluster-wide configuration].
. Оберіть *PG scrub* та розгорніть вкладку *Advanced*.
. Встановіть відповідне значення для *Requested Scrub Priority*.

+
.UI Ceph Dashboard. Requested Scrub Priority
image::admin:file-system/ceph-scrubbing/ceph-scrubbing-1.png[]

Аналогічні налаштування можна виконати через *Ceph CLI* з поду `rook-operator` в OKD-проєкті `openshift-storage`:

.Приклад налаштування Requested Scrub Priority через CLI
[source,bash]
----
ceph config --conf=/var/lib/rook/openshift-storage/openshift-storage.config set osd osd_requested_scrub_priority 10
----

[#max-concurrent-scrubs]
==== Налаштування паралельних процесів scrubbing

Рекомендовано виконувати *не більше 1 процесу scrubbing одночасно на кожному OSD* (Object Storage Daemon). Для цього використовується параметр `osd_max_scrubs = 1`.

Цей параметр визначає, *скільки одночасних процесів скрабування (scrub)* може бути запущено *на один OSD*. Обмеження до одного процесу дозволяє:

* зменшити навантаження на дискову підсистему та CPU;
* уникнути впливу на продуктивність обробки клієнтських запитів;
* забезпечити стабільність роботи кластера у виробничому середовищі.

TIP: *OSD (Object Storage Daemon)* -- це основний компонент Ceph, який зберігає дані, обробляє запити читання/запису та виконує скрабування. Кожен OSD працює незалежно, і перевірка цілісності даних (scrub) -- це локальна операція для конкретного OSD.

[NOTE]
====
У випадках, коли у кластері накопичилася *велика кількість PG (Placement Groups)*, для яких скрабування давно не проводилось, і які є "простроченими", дозволяється *тимчасово збільшити параметр `osd_max_scrubs` до `4`*.

Це прискорить виконання "простроченого" скрабування, але *може створити підвищене навантаження*.

❗Після завершення обов'язково поверніть параметр до значення `1`.

➡️ Детальніше про статуси PG див. у документації Ceph: https://docs.ceph.com/en/latest/rados/operations/pg-states/[Placement Group States].
====

Змінити цей параметр можна через *UI Ceph Dashboard*. Для цього виконайте наступні кроки:

. Відкрийте *UI Ceph Dashboard*.
. Перейдіть до menu:Cluster[OSDs > Cluster-wide configuration].
. Оберіть *PG scrub* та встановіть відповідне значення для *Max Scrubs*.

+
.UI Ceph Dashboard. Max Scrubs
image::admin:file-system/ceph-scrubbing/ceph-scrubbing-2.png[]

Аналогічні налаштування можна виконати через *Ceph CLI* з поду `rook-operator` в OKD-проєкті `openshift-storage`:

.Приклад налаштування Max Scrubs через CLI
[source,bash]
----
ceph config --conf=/var/lib/rook/openshift-storage/openshift-storage.config set osd osd_max_scrubs 1
----

[#disable-deep-scrub]
=== Вимкнення автоматичного deep scrubbing

У Ceph кластері процес *deep scrubbing* за замовчуванням виконується автоматично з періодичністю, яка визначається у конфігурації. Хоча ця перевірка є критично важливою для забезпечення цілісності даних, вона також є значно більш *ресурсомісткою*, ніж звичайний scrubbing, оскільки виконує повну перевірку *вмісту об'єктів*, а не лише метаданих.

[CAUTION]
====
У виробничих середовищах рекомендується запускати *deep scrubbing* **вручну**, у визначені вікна обслуговування або періоди низького навантаження на кластер, щоб уникнути:

- пікових навантажень на OSD-диски;
- деградації продуктивності інших сервісів Платформи;
- ризику впливу на критичні транзакції.

Ознайомтеся детальніше із процесом: xref:file-system/ceph-cluster-maintenance.adoc#manual-deep-scrub[Ручний запуск та управління процесами Deep Scrubbing].
====

[#disable-auto-deep-scrub]
==== Як вимкнути автоматичний deep scrubbing

Ceph дозволяє *блокувати автоматичний запуск процесу deep scrubbing* на рівні кожного пулу (pool). Для цього потрібно встановити відповідні прапорці *`nodeep-scrub`*.

.*Кроки для вимкнення автоматичного запуску:*

. Визначте шлях до конфігураційного файлу `ceph.conf`, який використовується у вашому кластері (*_у прикладі нижче він вказаний явно_*).
. Виконайте наступну команду, щоб *додати прапорець `nodeep-scrub` для всіх пулів*:
+
[source,bash]
----
for pool in $(ceph --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd pool ls); do
  ceph --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd pool set "$pool" nodeep-scrub true
done
----
+
TIP: Команда переліку пулів `osd pool ls` повертає всі наявні пули у кластері. Цикл застосовує прапорець `nodeep-scrub` до кожного з них.

[#nodeep-scrub-check]
==== Як перевірити, чи встановлено прапорець

Щоб переконатися, що параметр `nodeep-scrub` успішно застосовано, перевірте наявність прапорців (flags) на пулах:

[source,bash]
----
ceph --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd pool ls detail
----

У виводі шукайте ключ `flags`, де має бути вказано `nodeep-scrub`.

[IMPORTANT]
====
❗ *Рекомендовано запускати deep scrubbing вручну регулярно*, щонайменше *раз на кілька тижнів*, залежно від SLA та обсягу критичних даних.

Запускайте deep scrubbing у ті періоди, коли у вашому середовищі *кластер найменше навантажений* -- наприклад, уночі в будні або на вихідних, якщо дозволяє графік.
Формального вікна обслуговування не потрібно, якщо процес не впливає на критичні сервіси.

📌 Важливо -- *не допускати накопичення черги PG*, які тривалий час залишаються без перевірки.
У кластері з великою кількістю об'єктів і PG може знадобитися запуск щодня з обмеженою кількістю паралельних процесів (`osd_max_scrubs`), тоді як у менш навантаженому середовищі достатньо одного запуску на вихідних.
====

[#scrub-schedule]
=== Налаштування розкладу звичайного scrubbing

Рекомендується запускати звичайний scrubbing *щоночі*, з 00:00 до 06:00 за київським часом (`UTC+2` або `UTC+3`), забезпечуючи не більше одного одночасного процесу скрабування на кожен OSD (`osd_max_scrubs = 1`). Такий режим дозволяє регулярно перевіряти цілісність даних без значного впливу на продуктивність кластера (_див. <<max-concurrent-scrubs>>_).

За замовчуванням процес scrubbing уже увімкнено. Для його точного планування можна змінити параметри розкладу.

[#setup-schedule-ceph-dashboard]
==== Як налаштувати розклад через Ceph Dashboard

. Відкрийте *UI Ceph Dashboard*.
. Перейдіть до menu:Cluster[OSDs > Cluster-wide configuration].
. Оберіть розділ *PG scrub* та налаштуйте значення параметрів, визначених у таблиці нижче.
+
TIP: Ці параметри дозволяють задати точний період, у який Ceph має право автоматично запускати звичайний scrubbing. Значення встановлюються в UTC, тому при плануванні за локальним часом (наприклад, за Києвом — UTC+2) слід враховувати зміщення.
+

.UI Ceph Dashboard. Налаштування розкладу PG scrub
[cols="1,1,3", options="header"]
|===
^| Параметр ^| Значення ^| Опис

| `osd_scrub_begin_hour` | `22`
| Визначає годину початку допустимого вікна для scrubbing.

22 година за UTC = 00:00 за Києвом.
Це означає, що scrubbing може починатися не раніше ніж опівночі за локальним часом.

| `osd_scrub_end_hour` | `4`
| Визначає годину завершення допустимого вікна для scrubbing.

4 година за UTC = 06:00 за Києвом.
Усі автоматичні процеси scrubbing будуть зупинені після 06:00.

| `osd_scrub_begin_week_day` | `0`
| Визначає день тижня, з якого дозволено запуск scrubbing.

`0` = неділя (за Ceph-нумерацією днів тижня, де `0` = неділя, `1` = понеділок, ..., `6` = субота).
Якщо `begin` і `end` мають однакове значення (0), scrubbing дозволено щодня.

| `osd_scrub_end_week_day` | `0`
| Визначає день тижня, до якого дозволено запуск scrubbing.

Також 0 = неділя.
Якщо `begin_week_day = end_week_day = 0`, це не обмежує виконання лише одним днем, а навпаки -- означає, що scrubbing дозволено кожного дня тижня.
|===
+
[TIP]
====
📌 *Як працює логіка днів тижня в Ceph?*

Пара `begin_week_day = 0`, `end_week_day = 0` означає "від неділі до неділі включно", тобто усі дні тижня дозволені.

Якщо ж встановити, наприклад, `begin_week_day = 1` (понеділок), а `end_week_day = 5` (п'ятниця), то scrubbing буде запускатися лише з понеділка по п'ятницю.
====

+
. Натисніть *Edit PG scrub options*, щоб зберегти зміни.

+
.UI Ceph Dashboard. Налаштування розкладу PG scrub
image::admin:file-system/ceph-scrubbing/ceph-scrubbing-3.png[]

[#setup-schedule-ceph-cli]
==== Як налаштувати розклад через Ceph CLI

Аналогічні налаштування можна зробити через Ceph CLI. Для цього виконайте наведені нижче команди з поду `rook-operator` в OKD-проєкті `openshift-storage`.

. Вкажіть час початку і завершення scrubbing (UTC).
+
[source,bash]
----
ceph config set osd --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd_scrub_begin_hour 22

ceph config set osd --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd_scrub_end_hour 4
----

. Вкажіть дні тижня (0 = неділя; встановлюємо запуск щодня).
+
----
ceph config set osd --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd_scrub_begin_week_day 0

ceph config set osd --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd_scrub_end_week_day 0
----

. Перевірте застосовані значення за допомогою команди (замініть `<параметр>` відповідним параметром):
+
[source,bash]
----
ceph config get osd --conf=/var/lib/rook/openshift-storage/openshift-storage.config <параметр>
----
+
.Приклад для osd_scrub_begin_hour
[source,bash]
----
ceph config get osd --conf=/var/lib/rook/openshift-storage/openshift-storage.config osd_scrub_begin_hour
----

[#related-pages]
== Пов'язані сторінки

* xref:file-system/ceph-cluster-maintenance.adoc[]
* xref:file-system/ceph-osd-scaling-and-rebalancing.adoc[]
