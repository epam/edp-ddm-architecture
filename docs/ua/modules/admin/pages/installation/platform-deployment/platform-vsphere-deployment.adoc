= Розгортання Платформи з нуля у приватному хмарному середовищі _vSphere_
include::platform:ROOT:partial$templates/document-attributes/default-set-ua.adoc[]

include::platform:ROOT:partial$admonitions/language-ua.adoc[]

IMPORTANT: Будь ласка, зверніться до вашого постачальника, щоб отримати необхідний _vSphere_-інсталятор.

== Підготовка інфраструктури vSphere для встановлення OKD

*OKD* -- це дистрибутив Kubernetes, оптимізований для неперервної розробки додатків та розгортання декількох екземплярів ізольованого контейнерного середовища (у нашому випадку -- екземплярів реєстру). За детальною інформацією зверніться до https://docs.okd.io/[офіційного джерела].

=== Налаштування довіреного інтерфейсу vCenter API

Інсталер вимагає доступу до довіреного інтерфейсу vCenter API, який надає можливість завантажити довірені кореневі сертифікати CA vCenter.

Перед підключенням до API, сертифікати vCenter root CA повинні бути додані до системи, з якої запускатиметься OKD-інсталер.

=== Завантаження CA-сертифікатів

Сертифікати можуть бути завантажені з домашньої сторінки vCenter.

За замовчуванням сертифікати зберігаються за посиланням `<vCenter>/certs/download.zip`. Після завантаження і розархівування буде створено директорію, що містить сертифікати для ОС Linux, macOS та Windows.

==== Приклад перегляду структури

Структуру директорій із розміщеними в ній сертифікатами можна переглянути за допомогою команди:

[source,bash]
----
$ tree certs
----

В результаті буде зображено наступну структуру:

[source,bash]
----
certs

├── lin

│   ├── 108f4d17.0

│   ├── 108f4d17.r1

│   ├── 7e757f6a.0

│   ├── 8e4f8471.0

│   └── 8e4f8471.r0

├── mac

│   ├── 108f4d17.0

│   ├── 108f4d17.r1

│   ├── 7e757f6a.0

│   ├── 8e4f8471.0

│   └── 8e4f8471.r0

└── win

    ├── 108f4d17.0.crt

    ├── 108f4d17.r1.crl

    ├── 7e757f6a.0.crt

    ├── 8e4f8471.0.crt

    └── 8e4f8471.r0.crl


3 directories, 15 files
----

==== Приклад додавання сертифікатів

Необхідно додати відповідні сертифікати для вашої операційної системи.

**Приклад для ОС Fedora**:

[source, bash]
----
$ sudo cp certs/lin/* /etc/pki/ca-trust/source/anchors

$ sudo update-ca-trust extract
----

=== Ресурси стандартної інсталяції

Стандартна інсталяція (Installer-Provisioned Infrastructure) створює наступні ресурси інфраструктури:

* одну папку (1 Folder)
* одну тег-категорію (1 Tag Category)
* 1 тег (1 Tag)
* віртуальні машини (Virtual machines):
- один шаблон (1 template)
- одну тимчасову ноду bootstrap (1 temporary bootstrap node)
- три ноди консолі для управління Платформою (3 control-plane nodes)
- три обчислювальні машини (3 compute machines)

==== Необхідні вимоги до ресурсів

===== Сховище даних

Разом із ресурсами, описаними вище, стандартне розгортання OKD вимагає мінімум 800 Гб простору для сховища даних.

===== DHCP

Розгортання вимагає налаштування DHCP-сервера для конфігурації мережі.

== Розгортання та налаштування DNS і DHCP-компонентів

=== IP-адреси

Розгортання інфраструктури vSphere (Іnstaller-provisioned vSphere) вимагає двох статичних IP-адрес:

* **Адреса програмного інтерфейсу (API)** - використовується для доступу до API-кластера.

* **Вхідна IP-адреса (Ingress)** - використовується для вхідного трафіку кластера.

Віртуальні ІР-адреси для кожного з них повинні бути визначені у файлі xref:create-install-config-yml[`install-config.yaml`].

=== DNS-записи

DNS-записи (DNS records) повинні бути створені для двох ІР-адрес на будь-якому DNS-сервері, призначеному для середовища. Записи повинні містити значення, описані в таблиці:

[options="header"]
|================================================
|Назва| Значення
|`api.${cluster-name}.${base-domain}`|API VIP
|`*.apps.${cluster-name}.${base-domain}``|Ingress VIP
|================================================

NOTE: `${cluster-name}` та `${base-domain}` - це змінні, що взято із відповідних значень, вказаних у файлі xref:create-install-config-yml[`install-config.yaml`].

[#create-install-config-yml]
== Створення конфігураційного файлу install-config.yaml

[WARNING]
====
Передумови ::
. Увійдіть у свій обліковий запис Red Hat. Якщо у вас немає облікового запису, вам потрібно створити його.
. Придбайте платну підписку на DockerHub, якщо у вас її немає.
. Згенеруйте та додайте ssh-ключ до вашого конфігураційного файлу. Це необхідно для доступу до консолей ваших нод.
====

Створення файлу `install-config.yaml`, необхідного для розгортання OKD кластеру, виконується наступною командою:

[source,bash]
$ openshift-installer create install-config

Після створення файлу потрібно заповнити необхідні параметри, які будуть представлені в контекстному меню. Створений конфігураційний файл включає лише необхідні параметри для мінімального розгортання кластера. Для кастомізації налаштувань можна звернутись до офіційної документації.

._Конфігурація install-config.yaml_
[%collapsible]
====
[source,yaml]
----
apiVersion: v1
baseDomain: eua.gov.ua
compute:
- architecture: amd64
  hyperthreading: Enabled
  name: worker
  platform: {}
  replicas: 3
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  platform: {}
  replicas: 3
metadata:
  creationTimestamp: null
  name: mdtuddm
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  vsphere:
    apiVIP: 10.9.1.110
    cluster: HX-02
    datacenter: HXDP-02
    defaultDatastore: NCR_Data2
    ingressVIP: 10.9.1.111
    network: EPAM_OKD_Vlan9_EPG
    password: <password>
    username: epam_dev1@vsphere.local
    vCenter: vcsa.ncr.loc
publish: External
pullSecret: '{"auths":{"fake":{"auth":"aWQ6cGFzcwo="}}}'
sshKey: |
  <ssh_key>
----
====

[NOTE]
====
* Під час створення конфігураційного файлу замініть *`<password>`* на ваш пароль, а *`<ssh_key>`* -- на ваш згенерований ssh-ключ.
* Також скопіюйте параметри автентифікації з облікового запису Red Hat та підставте у поле *`pullSecret`*.
* Зверніть увагу, що деякі параметри, можливо, доведеться змінити, щоб вони відповідали вашій інфраструктурі та потребам.
====

== Запуск OKD4-інсталера та розгортання порожнього кластера OKD4

Після створення файлу `install-config.yaml`, для розгортання OKD-кластера необхідно виконати наступну команду:

[source,bash]
----
$ openshift-installer create cluster
----

NOTE: Процес розгортання кластера зазвичай займає до 1,5 години часу.

При успішному розгортанні, в результаті виконання команди будуть представлені наступні параметри доступу до кластера:

* логін;
* пароль;
* посилання на веб-консоль кластера.

В директорії, де виконувалася команда, буде створено ряд файлів, що зберігають статус кластера, необдхіний для його деінсталяції.

Також в цій директорії з'явиться папка `/auth`, в якій буде збережено два файли для автентифікації для роботи з кластером через **веб-консоль** та **інтерфейс командного рядка** OKD (OKD CLI).

NOTE: Після запуску процесу розгортання кластера, Інсталер видаляє `install-config.yaml`, тому рекомендовано виконати резервування цього файлу, якщо є потреба розгортання кількох кластерів.

== Заміна самопідписаних сертифікатів на довірені сертифікати

Для заміни самопідписаних (self-signed) сертифікатів на довірені (trusted) необхідно спочатку отримати ці сертифікати.

В цьому пункті розглянуто отримання безкоштовних сертифікатів https://letsencrypt.org/[Let's Encrypt] та їх встановлення на сервер.

Отримання сертифікатів Let's Encrypt здійснено за допомогою утиліти https://github.com/acmesh-official/acme.sh[acme.sh].

TIP: Для отримання розширених деталей щодо використання Let's Encrypt на базі ACME-протоколу, зверніться до https://letsencrypt.org/docs/client-options/[офіційного джерела].

=== Підготовка
Необхідно клонувати утиліту acme.sh із репозиторію GitHub:

[source,bash]
----
$ cd $HOME
$ git clone https://github.com/neilpang/acme.sh
$ cd acme.sh
----

=== Запит на отримання сертифікатів

. Щоб полегшити процес отримання сертифікатів, необхідно задати дві змінні середовища. Перша змінна повинна вказувати на API Endpoint. Переконайтесь, що ви увійшли до OKD як `system:admin` і використовуєте CLI-консоль Openshift, щоб знайти API Endpoint URL.
+
[source,bash]
----
$ oc whoami --show-server
----
+
.Приклад отриманої відповіді
----
https://api.e954.ocp4.opentlc.com:6443
----

. Тепер встановіть змінну `LE_API` для повністю визначеного доменного імені API:
+
[source,bash]
----
$ export LE_API=$(oc whoami --show-server | cut -f 2 -d ':' | cut -f 3 -d '/' | sed 's/-api././')
----

. Встановіть другу змінну `LE_WILDCARD` для вашого Wildcard Domain:
+
[source,bash]
----
$ export LE_WILDCARD=$(oc get ingresscontroller default -n openshift-ingress-operator -o jsonpath='{.status.domain}')
----

. Запустіть скрипт `acme.sh`:
+
[source,bash]
----
$ ${HOME}/acme.sh/acme.sh --issue -d ${LE_API} -d *.${LE_WILDCARD} --dns
----
+
.Приклад отриманої відповіді
[source, bash]
----
$  ./acme.sh --issue -d  ${LE_API} -d \*.${LE_WILDCARD} --dns --yes-I-know-dns-manual-mode-enough-go-ahead-please
[Wed Jul 28 18:37:33 EEST 2021] Using CA: https://acme-v02.api.letsencrypt.org/directory
[Wed Jul 28 18:37:33 EEST 2021] Creating domain key
[Wed Jul 28 18:37:33 EEST 2021] The domain key is here: $HOME/.acme.sh/api.e954.ocp4.opentlc.com/api.e954.ocp4.opentlc.com.key
[Wed Jul 28 18:37:33 EEST 2021] Multi domain='DNS:api.e954.ocp4.opentlc.com,DNS:*.apps.e954.ocp4.opentlc.com'
[Wed Jul 28 18:37:33 EEST 2021] Getting domain auth token for each domain
[Wed Jul 28 18:37:37 EEST 2021] Getting webroot for domain='cluster-e954-api.e954.ocp4.opentlc.com'
[Wed Jul 28 18:37:37 EEST 2021] Getting webroot for domain=‘*.apps.cluster-e954-api.e954.ocp4.opentlc.com’
[Wed Jul 28 18:37:38 EEST 2021] Add the following TXT record:
[Wed Jul 28 18:37:38 EEST 2021] Domain: '_acme-challenge.api.e954.ocp4.opentlc.com'
[Wed Jul 28 18:37:38 EEST 2021] TXT value: 'VZ2z3XUe4cdNLwYF7UplBj7ZTD8lO9Een0yTD7m_Bbo'
[Wed Jul 28 18:37:38 EEST 2021] Please be aware that you prepend _acme-challenge. before your domain
[Wed Jul 28 18:37:38 EEST 2021] so the resulting subdomain will be: _acme-challenge.api.e954.ocp4.opentlc.com
[Wed Jul 28 18:37:38 EEST 2021] Add the following TXT record:
[Wed Jul 28 18:37:38 EEST 2021] Domain: '_acme-challenge.apps.e954.ocp4.opentlc.com'
[Wed Jul 28 18:37:38 EEST 2021] TXT value: 'f4KeyXkpSissmiLbIIoDHm5BJ6tOBTA0D8DyK5sl46g'
[Wed Jul 28 18:37:38 EEST 2021] Please be aware that you prepend _acme-challenge. before your domain
[Wed Jul 28 18:37:38 EEST 2021] so the resulting subdomain will be: _acme-challenge.apps.e954.ocp4.opentlc.com
[Wed Jul 28 18:37:38 EEST 2021] Please add the TXT records to the domains, and re-run with --renew.
[Wed Jul 28 18:37:38 EEST 2021] Please add '--debug' or '--log' to check more details.
----
+
CAUTION: DNS-записи з попередньої відповіді необхідно додати на DNS-сервері, що відповідає за зону `e954.ocp4.opentlc.com` (**значення зони тут є прикладом**). Таким чином, TXT-записи повинні мати наступний вигляд:
+
.TXT-запис 1
[source,bash]
----
_acme-challenge.api.e954.ocp4.opentlc.com TXT value: 'VZ2z3XUe4cdNLwYF7UplBj7ZTD8lO9Een0yTD7m_Bbo'
----
+
.TXT-запис 2
[source,bash]
----
_acme-challenge.apps.e954.ocp4.opentlc.com TXT value: 'f4KeyXkpSissmiLbIIoDHm5BJ6tOBTA0D8DyK5sl46g'
----

. Після цього необхідно повторно запустити команду `acme.sh`:
+
[source,bash]
----
$ acme.sh --renew -d e954.ocp4.opentlc.com --yes-I-know-dns-manual-mode-enough-go-ahead-please
----

. Після успішного виконання попередніх пунктів необхідно запустити наступні команди.
+
Зазвичай, хорошим підходом є перенесення сертифікатів зі шляху acme.sh за замовчуванням (default path) до більш зручної директорії. Для цього можна використати `—install-cert`-ключ скрипту `acme.sh` для копіювання сертифікатів до `$HOME/certificates`, для прикладу:
+
[source,bash]
----
$ export CERTDIR=$HOME/certificates

$ mkdir -p ${CERTDIR} ${HOME}/acme.sh/acme.sh --install-cert -d ${LE_API} -d *.${LE_WILDCARD} --cert-file ${CERTDIR}/cert.pem --key-file ${CERTDIR}/key.pem --fullchain-file ${CERTDIR}/fullchain.pem --ca-file ${CERTDIR}/ca.cer
----

==== Встановлення сертифікатів для Router
* Необхідно створити секрет. Для цього виконайте наступну команду:

[source,bash]
----
$ oc create secret tls router-certs --cert=${CERTDIR}/fullchain.pem --key=${CERTDIR}/key.pem -n openshift-ingress
----

* Після виконання попередніх кроків, необхідно оновити Custom Resource:

[source,bash]
----
$ oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec": 	{ "defaultCertificate": { "name": "router-certs" }}}'
----

== Створення MachineSet для інфраструктури Ceph

TIP: _Ресурси **MachineSet**_ -- це групи машин. Набори машин призначені для машин як набори копій (реплік) для Pods, в яких розгорнуто контейнери. Якщо вам потрібно більше машин або, навпаки, необхідно зменшити їх кількість, можна змінити значенням поля реплік на рівні MachineSet, щоб задовольнити ваші обчислювальні потреби. Для детальної інформації щодо створення MachineSet зверніться до https://docs.openshift.com/container-platform/4.6/machine_management/creating_machinesets/creating-machineset-vsphere.html[офіційного джерела.]

Для розгортання Платформи необхідно створити MachineSet для системи зберігання даних https://ceph.io/en/[Ceph]. Для цього необхідно використати конфігураційний файл `machine-set-ceph.yaml`, в якому необхідно змінити назву кластера.

._Приклад конфігураційного файлу machine-set-ceph.yaml_
[%collapsible]
====
[source, yaml]
----
kind: MachineSet
metadata:
  name: mdtuddm-b86zw-ceph
  namespace: openshift-machine-api
  labels:
    machine.openshift.io/cluster-api-cluster: mdtuddm-b86zw
spec:
  replicas: 3
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: mdtuddm-b86zw
      machine.openshift.io/cluster-api-machineset: mdtuddm-b86zw-ceph
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: mdtuddm-b86zw
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: mdtuddm-b86zw-ceph
    spec:
      taints:
        - effect: NoSchedule
          key: node.ocs.openshift.io/storage
          value: 'true'
      metadata:
        labels:
          cluster.ocs.openshift.io/openshift-storage: ''
      providerSpec:
        value:
          numCoresPerSocket: 1
          diskGiB: 120
          snapshot: ''
          userDataSecret:
            name: worker-user-data
          memoryMiB: 73728
          credentialsSecret:
            name: vsphere-cloud-credentials
          network:
            devices:
              - networkName: EPAM_OKD_Vlan9_EPG
          metadata:
            creationTimestamp: null
          numCPUs: 16
          kind: VSphereMachineProviderSpec
          workspace:
            datacenter: HXDP-02
            datastore: NCR_Data2
            folder: /HXDP-02/vm/mdtuddm-b86zw
            resourcePool: /HXDP-02/host/HX-02/Resources
            server: vcsa.ncr.loc
          template: mdtuddm-b86zw-rhcos
          apiVersion: vsphereprovider.openshift.io/v1beta1
----
====

Після редагування файлу відповідно до назви кластера, необхідно виконати команду, що створить необхідний MachineSet та відповідну кількість нод для розгортання сховища даних Ceph.

TIP: У нашому випадку назва кластера визначена в _.yaml_-файлі як `mdtuddm-b86zw`.

== Підготовка та запуск Інсталера для розгортання Платформи на цільовому OKD-кластері

TIP: _Інсталер_ -- набір команд (скрипт) для розгортання Платформи.

Для запуску Інсталера, необхідно виконати ряд умов з підготовки робочої станції, з якої запускатиметься Інсталер. Нижче розглянуто приклад такої підготовки на базі Ubuntu 20.04 LTS.

=== Передумови

Встановіть Docker з офіційного джерела: https://docs.docker.com/engine/install/[].

=== Розгортання та оновлення Платформи

==== Розгортання Платформи з нуля

===== Передумови

NOTE: Переконайтеся, що встановлено необхідні пакети: `docker`, `wget`, `unzip`.

. Завантажте необхідну версію інсталера.
+
[source,shellscript]
----
сd /tmp
wget -O mdtu-ddm-platform-<version>.zip https://nexus-public-mdtu-ddm-edp-cicd.apps.cicd2.mdtu-ddm.projects.epam.com/nexus/repository/edp-maven-releases/ua/gov/mdtu/ddm/infrastructure/mdtu-ddm-platform/<version>/mdtu-ddm-platform-<version>.zip
----

. Розпакуйте архів у домашній директорії.

+
[source,shellscript]
----
unzip /tmp/mdtu-ddm-platform-<version>.zip -d /home/<user>/workdir/installer-<version>
----

. Перенесіть _kubeconfig_ після встановлення кластера:

+
[source,shellscript]
----
cd /home/<user>/workdir/installer-<version>
cp /path/to/kubeconfig ./
----

. Перенесіть папку _certificates_ для DSO:

+
[source,shellscript]
----
cp /path/to/folder/certificates ./
----

===== Додавання окремого конфігураційного файлу для розгортання у середовищі vSphere

. Відредагуйте _exports.list_ для vSphere.
+
Усі значення необхідно взяти після інсталяції кластера. Також необхідно уточнити актуальні значенння для `idgovuaClientId` та `idgovuaClientSecret`.

+
[source,shellscript]
----
vi exports.list

### vSphere Credentials ###
export VSPHERE_SERVER=""
export VSPHERE_USER=""
export VSPHERE_PASSWORD=""
export VSPHERE_CLUSTER=""
export VSPHERE_DATASTORE=""
export VSPHERE_DATACENTER=""
export VSPHERE_NETWORK=""
export VSPHERE_NETWORK_GATEWAY=""
export VSPHERE_RESOURCE_POOL="" #якщо не використовується, ставимо "/"
export VSPHERE_FOLDER=""

### Minio and Vault IPs ###
export VSPHERE_VAULT_INSTANCE_IP=""
export VSPHERE_MINIO_INSTANCE_IP=""

### id.gov.ua ###
export idgovuaClientId=""
export idgovuaClientSecret=""
----

. Відредагуйте _install.sh_, а саме після `source ./functions.sh` додайте `source ./exports.list`.

+
[source,shellscript]
----
vi install.sh
----
+
Це виглядатиме наступним чином:

+
[source,shellscript]
----
#!/usr/bin/env bash
set -e
#Include function file
source ./functions.sh
source ./exports.list
----

===== Розгортання Інсталера

. Виконайте наступні команди:
+
[source,shellscript]
----
IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\\1#" | tr -d '\n');
echo $IMAGE_CHECKSUM
sudo docker tag ${IMAGE_CHECKSUM} control-plane-installer:<version>;
----

. Розгорніть нову версію Платформи з образами з нуля:
+
[source,shellscript]
----
sudo docker run --rm --name control-plane-installer-<version> --user root:$(id -g) --net host -v $(pwd):/tmp/installer --env KUBECONFIG=/tmp/installer/kubeconfig --env idgovuaClientId=mock --env idgovuaClientSecret=mock --env deploymentMode=development --entrypoint "/bin/bash" control-plane-installer:<version> -c "./install.sh -i"
----
+
* де `deploymentMode` може бути `development` чи `production`.

==== Оновлення Платформи

===== Передумови

NOTE: Переконайтеся, що встановлено необхідні пакети: `docker`, `wget`, `unzip`.

. Завантажте необхідну версію інсталера.
+
[source,shellscript]
----
сd /tmp
wget -O mdtu-ddm-platform-<version>.zip https://nexus-public-mdtu-ddm-edp-cicd.apps.cicd2.mdtu-ddm.projects.epam.com/nexus/repository/edp-maven-releases/ua/gov/mdtu/ddm/infrastructure/mdtu-ddm-platform/<version>/mdtu-ddm-platform-<version>.zip
----

. Розпакуйте архів у домашній директорії.

+
[source,shellscript]
----
unzip /tmp/mdtu-ddm-platform-<version>.zip -d /home/<user>/workdir/installer-<version>
----

. Перенесіть _kubeconfig_ після встановлення кластера:

+
[source,shellscript]
----
cd /home/<user>/workdir/installer-<version>
cp /path/to/kubeconfig ./
----

. Перенесіть папку _certificates_ для DSO.
+
NOTE: Якщо сертифікати не змінювалися, даний крок можна пропустити.

+
[source,shellscript]
----
cp /path/to/folder/certificates ./
----

===== Додавання окремого конфігураційного файлу для розгортання у середовищі vSphere

. Перенесіть _exports.list_ з минулого релізу.

+
[source,shellscript]
----
cp /home/<user>/workdir/installer-<previous_version>/exports.list ./
----
+
Також необхідно уточнити актуальні значенння для `idgovuaClientId` та `idgovuaClientSecret`.

. Відредагуйте _install.sh_, а саме після `source ./functions.sh` додайте `source ./exports.list`.

+
[source,shellscript]
----
vi install.sh
----
+
Це виглядатиме наступним чином:

+
[source,shellscript]
----
#!/usr/bin/env bash
set -e
#Include function file
source ./functions.sh
source ./exports.list
----

===== Налаштування компонента MinIO при оновленні кластера у середовищі vSphere

. Перенесіть tfstate MinIO з минулого релізу для vSphere.

+
[source,shellscript]
----
cp /home/<user>/workdir/installer-<version>/terraform/minio/vsphere/terraform.tfstate ./terraform/minio/vsphere/
----

. Перенесіть tfstate MinIO (Packer) з минулого релізу для vSphere.

+
[source,shellscript]
----
сp /home/<user>/workdir/installer-<version>/terraform/minio/vsphere/packer/terraform.tfstate ./terraform/minio/vsphere/packer/
----

===== Налаштування компонента Vault при оновленні кластера у середовищі vSphere

. Перенесіть tfstate Vault з минулого релізу.

+
[source,shellscript]
----
cp /home/<user>/workdir/installer-<version>/terraform/vault/vsphere/terraform.tfstate ./terraform/vault/vsphere/
----

. Перенесіть tfstate Vault (Packer) з минулого релізу.

+
[source,shellscript]
----
сp /home/<user>/workdir/installer-<version>/terraform/vault/vsphere/packer/terraform.tfstate ./terraform/vault/vsphere/packer/
----

===== Розгортання Інсталера

. Виконайте наступні команди:
+
[source,shellscript]
----
IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\\1#" | tr -d '\n');
echo $IMAGE_CHECKSUM
sudo docker tag ${IMAGE_CHECKSUM} control-plane-installer:<version>;
----

. Оновіть версію Платформи з образами оновлення.
+
[source,shellscript]
----
sudo docker run --rm --name control-plane-installer-<version> --user root:$(id -g) --net host -v $(pwd):/tmp/installer --env KUBECONFIG=/tmp/installer/kubeconfig --env idgovuaClientId=mock --env idgovuaClientSecret=mock --env deploymentMode=development --entrypoint "/bin/bash" control-plane-installer:<version> -c "./install.sh -u"
----
+
TIP: де `deploymentMode` може бути `development` чи `production`, залежно від попереднього запуску.

== Управління налаштуваннями Платформи

Управління кластером відбувається за методологією https://about.gitlab.com/topics/gitops/[GitOps]. Це означає, що будь-які зміни в конфігурації кластера, компонентів кластера та компонентів Платформи відбувається через зміну конфігурації кластера в git-гілці відповідного компонента.

Метадані усіх компонентів, для яких реалізовано управління через GitOps-підхід, зберігаються в компоненті `cluster-mgmt`.

Нижче представлено список компонентів, для яких наразі імплементований GitOps-підхід:

- `catalog-source`
- `monitoring`
- `storage`
- `logging`
- `service-mesh`
- `backup-management`
- `user-management`
- `control-plane-nexus`
- `external-integration-mocks`
- `cluster-kafka-operator`
- `smtp-server`
- `redis-operator`
- `postgres-operator`