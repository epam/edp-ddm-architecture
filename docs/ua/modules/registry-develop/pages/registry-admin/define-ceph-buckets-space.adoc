:toc-title: ЗМІСТ
:toc: auto
:toclevels: 5
:experimental:
:important-caption:     ВАЖЛИВО
:note-caption:          ПРИМІТКА
:tip-caption:           ПІДКАЗКА
:warning-caption:       ПОПЕРЕДЖЕННЯ
:caution-caption:       УВАГА
:example-caption:           Приклад
:figure-caption:            Зображення
:table-caption:             Таблиця
:appendix-caption:          Додаток
:sectnums:
:sectnumlevels: 5
:sectanchors:
:sectlinks:
:partnums:

= Визначення розміру бакетів бізнес-процесів

Переглянути розміри усіх бакетів у кластері можна за допомогою CEPH Dashboard.

Для того, щоб увійти до дашборда, виконайте наступні дії:

. Додайте у `StorageCluster` CR разово наступну конфігурацію:
+
[source,yaml]
----
managedResources:
    cephBlockPools: {}
    cephDashboard:
      enable: true
      ssl: false
    cephFilesystems: {}
----

. Виконайте логін на кластер за допомогою *`Copy login command`*, яку можна отримати в інтерфейсі OpenShift-консолі.

. У консолі виконайте наступну команду:
+
[source,kubectl]
----
kubectl -n openshift-storage port-forward svc/rook-ceph-mgr-dashboard 8088:7000
----

. Відкрийте вебінтерфейс за адресою http://localhost:8088/[] (_порт може змінюватися, якщо порт з інструкції зайнятий_).

. Логін для входу -- `admin`. Пароль можна знайти у проєкті `openshift-storage`, в секреті `rook-ceph-dashboard-password`.

. Бакети знаходяться в *Object Gateway* > *Buckets*.
+
Бакети з даними бізнес процесів:

* `lowcode-file-storage`
* `lowcode-form-data-storage`
+
[WARNING]
====
Якщо виникла `500` помилка, необхідно перезапустити поди `rook-ceph-rgw-ocs-storagecluster-cephobjectstore` і `rook-ceph-mgr` у проєкті *`openshift-storage`*.

(_Опційно_) Також перезапустіть поди `rook-operators`.
====

. (_Додатково_) Розв'язання проблеми з `403` помилкою.
+
image:registry-admin/define-ceph-bucket-storage/define-ceph-bucket-storage-1.png[]
+
У поді `rook-ceph-operator` виконайте:
+
----
radosgw-admin --conf=/var/lib/rook/openshift-storage/openshift-storage.config user create --display-name="dashboard" --uid=dashboard --rgw-realm <тут всі наявні рілми>
----
+
`<тут всі наявні рілми>`:
+
----
radosgw-admin --conf=/var/lib/rook/openshift-storage/openshift-storage.config realm list
{
"default_info": "af9cb76b-e9a5-4093-a0cd-c1f15e4ee2f0",
"realms": [
"ocs-storagecluster-cephobjectstore",
"mdtuddm"
...
  ]
}
----
+
+ reboot mgr, operator, rgw pods


////
Version 2 (не перевірена)

1. Make sure it is EXACT the same problem with 403 dashboard accessing RGW.

2. Check users that exist in EACH available objectGateway with the command: radosgw-admin --conf=/var/lib/rook/openshift-storage/openshift-storage.config user list --rgw-realm mdtuddm

3. If there are no "dasboard" and "dashboard-admin" users, then create them in EACH objectGateway with the command:
radosgw-admin --conf=/var/lib/rook/openshift-storage/openshift-storage.config user create --display-name="dashboard-admin" --uid=dashboard-admin
radosgw-admin --conf=/var/lib/rook/openshift-storage/openshift-storage.config user create --display-name="dashboard" --uid=dashboard

4. Reboot RGW, Operator, MGR pods.
////


