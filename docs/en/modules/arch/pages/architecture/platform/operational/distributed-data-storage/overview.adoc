= Distributed data storage subsystem
include::platform:ROOT:partial$templates/document-attributes/arch-set-en.adoc[]

include::platform:ROOT:partial$admonitions/language-en.adoc[]

== Overview

//Підсистема, що забезпечує функціонування об’єктного (S3 сумісного), блочного та файлового сховища в Платформі.
Subsystem that provides the functionality of object (S3-compatible), block, and file storage in the Platform.

== Subsystem functions

//* Підтримка функціонування обʼєктного, файлового та блочного сховища
* Provision of object, block, and file storage
//* Моніторинг сховища
* Storage monitoring
//* Масштабування та аварійне відновлення обʼєктного, файлового та блочного сховища
* Scaling and disaster recovery of the object, block, and file storage

== Subsystem technical design

//На даній діаграмі зображено компоненти, які входять в _Підсистему розподіленого зберігання даних_ та їх взаємодію з іншими підсистемами.
The following diagram displays the components that comprise _Data storage subsystem_, and their interaction with other subsystems.

image::architecture/platform/operational/distributed-data-storage/distributed-data-storage.drawio.svg[width=800,float="center",align="center"]

TIP: You can find more detailed information on _ceph_ and _rook_ components in https://docs.ceph.com/en/quincy/architecture/[ceph architecture] and https://rook.io/docs/rook/v1.10/Getting-Started/storage-architecture/[rook architecture] documentation.


//Основними типами сховищ широко використовуємих в Платформі є
The most widely used storages on the Platform are:

//* Object Storage — масштабована, розподілена система зберігання для неструктурованих даних, що доступна через сумісні з протоколом S3 API.
* Object Storage — a scalable, distributed data storage system for unstructured data, accessible via S3 protocols compatible API;
//* Block Storage — високопродуктивне, розподілене блокове сховище для використання віртуальними серверами або контейнерами.
* Block Storage — high-performance, distrubuted block storage for virtual servers or containers.

//== Глосарій підсистеми
== Subsystem glossary

//RGW:: Акронім від "RADOS Gateway", що є інтерфейсом об'єктного сховища для Ceph. Надає RESTful API для зберігання та отримання даних у кластерах Ceph.
RGW:: RADOS Gateway, which is an object storage interface of Ceph. It provides RESTful API for data storing and receiving in Ceph clusters.
//OSD:: Пристрій об'єктного сховища (Object Storage Device), є фундаментальним компонентом системи зберігання Ceph. OSD відповідає за зберігання та отримання даних на фізичних пристроях зберігання в кластері Ceph. Кожен OSD керує власним локальним сховищем, і дані розподіляються по кількох OSD в кластері, щоб забезпечити надійність та стійкість до відмов.
OSD:: Object Storage Device is a fundamental component of the Ceph storage system. It is responsible for receiving and storing of the data on physical devices in Ceph cluster. Each OSD controls its own local storage, and the data is distributed beyween several OSDs in the cluster, to ensure reliability and fault-tolerance.
//OCS:: Акронім від "OpenShift Container Storage" — рішення для зберігання даних, що базується на технології Ceph та інтегрується з платформою оркестрації контейнерів OpenShift.
OCS:: OpenShift Container Storage - a data storage solution based on Ceph technology that integrates with OpenShift container orchestration platform.
//MDS:: Акронім від "Metadata Server" — компонент файлової системи CephFS. MDS відповідає за керування метаданими для CephFS.
MDS:: Metadata Server - a CephFS file system component. It controls metadata for CephFS.
//cephFS:: Файлова система Ceph (CephFS).
cephFS:: Ceph File System.
CSI:: Акронім від "Container Storage Interface" — це стандартний інтерфейс для забезпечення сумісності різних систем зберігання даних з платформами оркестрації контейнерів Kubernetes або OpenShift.
CSI:: Container Storage Interface - the standard interface used to provide interoperability between data storage systems with Kubernetes or OpenShift container orchestration platforms.
//RBD:: Блочне сховище (RADOS Block Device) — це блокове сховище, яке використовує технологію Ceph для зберігання даних.
RBD:: RADOS Block Device - a block storage that uses Ceph technology to store data.
//OSD Map:: Cтруктура даних у ceph, що містить інформацію про стан та розташування OSD у кластері Ceph. OSD Map відображає стан кожного OSD (активний, відключений або не функціонує), взаємозв'язки між OSD та групування OSD у розподіленій системі зберігання.
OSD Map:: Data structure in Ceph, which contains information on the state and location of OSD in Ceph cluster. An OSD Map displays the state of each OSD (active, inactive, non-functioning), interconnections between OSD, and OSD grouping in the distributed storage system.
//MDS Map:: Cтруктура даних у ceph, що містить інформацію про MDS мапу на конкретний проміжок часу, її час створення та останню зміну, також містить пул для зберігання метаданих, список серверів метаданих та сервери метаданих, які працюють та включені.
DS Map:: Data structure in Ceph, which contains information on the MDS map over a certain time period - it's creation date and last change. Also, it contains a metadata storage pool, metadata server list and their states (active or inactive).

//== Складові підсистеми
== Subsystem components

////
|===
|Назва компоненти|Namespace|Deployment|Походження|Репозиторій|Призначення

|_Ceph дашборд_
|openshift-storage
|rook-ceph-dashboard
|3rd-party
.14+|https://github.com/red-hat-storage/ocs-operator[github:/red-hat-storage/ocs-operator]

https://github.com/rook/rook[github:/rook-operator]

https://gerrit-mdtu-ddm-edp-cicd.apps.cicd2.mdtu-ddm.projects.epam.com/admin/repos/mdtu-ddm/infrastructure/storage[gerrit:/infrastructure/storage]
|Переглядання основних Ceph метрик, стану сховища та логів підсистеми розподіленого зберігання файлів

|_Rook Ceph Operator_
|openshift-storage
|rook-ceph-operator
|3rd-party
|Допоміжне програмне забезпечення, яке виконує функції оркестрування Ceph сховища.

|_OpenShift Container Storage Operator_
|openshift-storage
|ocs-operator
|3rd-party
|Допоміжне програмне забезпечення, яке виконує функції оркестрування ресурсів OpenShift Storage.

|_Ceph Metadata Server_
|openshift-storage
|rook-ceph-mds
|3rd-party
|Компонент, що керує метаданими файлів в Ceph сховищі

|_Ceph Manager_
|openshift-storage
|rook-ceph-mgr
|3rd-party
|Компонент, що працює для забезпечення моніторингу сховища Ceph та взаємодії із зовнішніми системами моніторингу та керування.

|_Ceph Monitor_
|openshift-storage
|rook-ceph-mon
|3rd-party
|Компонент, що підтримує "мапу" стану Ceph сховища та мапу OSD (Object Storage Device)

|_Ceph Object Storage Device_
|openshift-storage
|rook-ceph-osd
|3rd-party
|Програмне забезпечення Ceph сховища, яке взаємодіє з логічними дисками кластера OpenShift.

|_Ceph Object Gateway_
|openshift-storage
|rook-ceph-rgw
|3rd-party
|Компонент Ceph сховища, який забезпечує шлюз до об’єктного Amazon S3 API сховища

|_Ceph RBD CSI Driver_
|openshift-storage
|rook-ceph-rgw
|3rd-party
|Драйвер, що забезпечує інтеграцію Ceph-сумісних об'єктів зберігання, такі як блочні пристрої RBD або CephFS з системою
оркестрації контейнерів OKD.

|_CephFS CSI Driver_
|openshift-storage
|rook-ceph-rgw
|3rd-party
|Драйвер, що забезпечує інтеграцію Ceph-сумісних об'єктів зберігання, такі як блочні пристрої RBD або CephFS з системою
оркестрації контейнерів OKD.

|_OCS Metrics Exporter_
|openshift-storage
|ocs-metrics-exporter
|3rd-party
|Prometheus експортер, що збирає метрики OCS та ceph для моніторингу та подальшого аналізу.

|_Rook Ceph Crash Collector_
|openshift-storage
|ocs-metrics-exporter
|3rd-party
|Компонент Rook Ceph Crash Collector слугує для збирання та агрегування інформації про аварійні завершення в Ceph

|===
////


|===
|Component name|Namespace|Deployment|Source|Repository|Function

|_Ceph dashboard_
|openshift-storage
|rook-ceph-dashboard
|3rd-party
.14+|https://github.com/red-hat-storage/ocs-operator[github:/red-hat-storage/ocs-operator]

https://github.com/rook/rook[github:/rook-operator]

https://gerrit-mdtu-ddm-edp-cicd.apps.cicd2.mdtu-ddm.projects.epam.com/admin/repos/mdtu-ddm/infrastructure/storage[gerrit:/infrastructure/storage]
|Viewing of the main Ceph metrics, storage state, and distributed data storage system logs.

|_Rook Ceph Operator_
|openshift-storage
|rook-ceph-operator
|3rd-party
|Auxiliary software that orchestrates Ceph storage.

|_OpenShift Container Storage Operator_
|openshift-storage
|ocs-operator
|3rd-party
|Auxiliary software that orchestrates OpenShift Storage.

|_Ceph Metadata Server_
|openshift-storage
|rook-ceph-mds
|3rd-party
|Component that controls file metadata in Ceph storage.

|_Ceph Manager_
|openshift-storage
|rook-ceph-mgr
|3rd-party
|Component that provides Ceph storage monitoring and interaction with external monitoring and management systems.

|_Ceph Monitor_
|openshift-storage
|rook-ceph-mon
|3rd-party
|Component that keeps Ceph storage state map, and OSD map.

|_Ceph Object Storage Device_
|openshift-storage
|rook-ceph-osd
|3rd-party
|Ceph software that interacts with OpenShift cluster logical disks.

|_Ceph Object Gateway_
|openshift-storage
|rook-ceph-rgw
|3rd-party
|Ceph storage component that provides a gateway to the Amazon S3 object storage API.

|_Ceph RBD CSI Driver_
|openshift-storage
|rook-ceph-rgw
|3rd-party
|Driver that provides the integration of Ceph-compatible storage objects, like RBD or CephFS block devices, with the OKD container orchestration system.

|_CephFS CSI Driver_
|openshift-storage
|rook-ceph-rgw
|3rd-party
|Driver that provides the integration of Ceph-compatible storage objects, like RBD or CephFS block devices, with the OKD container orchestration system.

|_OCS Metrics Exporter_
|openshift-storage
|ocs-metrics-exporter
|3rd-party
|Prometheus exporter that gathers OCS and ceph metrics for monitoring and further analysis.

|_Rook Ceph Crash Collector_
|openshift-storage
|ocs-metrics-exporter
|3rd-party
|Component that gathers and aggregates information on crashes in Ceph.
|===

== Object storage data classification

|===
|Bucket|Owner subsystem|Description

|xref:arch:architecture/registry/operational/bpms/ceph-storage.adoc#_lowcode_file_storage[lowcode-file-storage]
|xref:arch:architecture/registry/operational/bpms/overview.adoc[Business process execution subsystem]
|Temporary storing of digital documents, uploaded during Business Process execution

|xref:arch:architecture/registry/operational/registry-management/ceph-storage.adoc#_datafactory_ceph_bucket[datafactory-ceph-bucket]
.3+|xref:arch:architecture/registry/operational/registry-management/overview.adoc[Registry data management subsystem]
|Storing of signed data during its writing to Registry

|xref:arch:architecture/registry/operational/registry-management/ceph-storage.adoc#_file_ceph_bucket[file-ceph-bucket]
|Storing of Registry digital documents

|xref:arch:architecture/registry/operational/registry-management/ceph-storage.adoc#_response_ceph_bucket[response-ceph-bucket]
|Temporary storing of data for transfer in inter-service interaction

|xref:arch:architecture/registry/operational/excerpts/ceph-storage.adoc#_file_excerpt_bucket[file-excerpt-bucket]
.3+|xref:arch:architecture/registry/operational/excerpts/overview.adoc[Excerpt forming subsystem]
|Gathering of generated and signed excerpts from the Registry

|xref:arch:architecture/registry/operational/excerpts/ceph-storage.adoc#_excerpt_signature_bucket_deprecated[excerpt-signature-bucket (deprecated)]
|Storing of generated excerpts from the Registry

|xref:arch:architecture/registry/operational/excerpts/ceph-storage.adoc#_excerpt_templates[excerpt-templates]
|Storing of excerpt templates

|xref:arch:architecture/registry/administrative/regulation-management/ceph-storage.adoc#_user_import[user-import]
.2+|xref:arch:architecture/registry/administrative/regulation-management/overview.adoc[Registry regulations modelling subsystem]
|Storing of the files that contain a list of officers, for import to the Registry

|xref:arch:architecture/registry/administrative/regulation-management/ceph-storage.adoc#_user_import_archive[user-import-archive]
|Storing of the files that contain a list of officers imported to the Registry

|===


== Technology stack

* xref:arch:architecture/platform-technologies.adoc#ceph[Ceph]
* xref:arch:architecture/platform-technologies.adoc#rook-operator[Rook]
* xref:arch:architecture/platform-technologies.adoc#okd[okd]

//== Атрибути якості підсистеми
== Subsystem quality attributes

=== _Scalability_
//Підсистема розподіленого зберігання даних розроблена для горизонтального масштабування на сотні або навіть тисячі вузлів зберігання даних, забезпечуючи при цьому величезні обсяги зберігання даних. Підсистема має динамічну здатність масштабування що дозволяє кластерам зростати або зменшуватися за потреби.
Distributed data storage subsystem was designed for horizontal scaling to hundreds or even thousands of data storing nodes, providing data storage on an extensive scale. The subsystem has dynamic scaling capabilities, which allows the clusters to scale up or down on demand.


=== _Reliability_
//Підсистема розподіленого зберігання даних використовує реплікацію даних та _erasure coding (EC)_ методи для захисту від втрати даних та забезпечення відмовостійкості підсистеми. У разі відмови вузла або пристрою зберігання, підсистема автоматично реплікує втрачені дані на інших робочіх вузлах для підтримання надійного збереження даних.
Distributed data storage subsystem uses data replication and _erasure coding (EC)_ to avoid data loss and provide subsystem fault-tolerance. In case of a node or device failure, the subsystem replicates its data automatically on operational nodes to ensure the reliable storing of data.

=== _Resilience_
//Підсистема розподіленого зберігання даних залишається працездатною, навіть коли стикається з проблемами мережі або відмовами вузлів зберігання даних. Завдяки динамічному балансуванню навантаження та методам розподілу даних в поєднанні з відмовостійким проєктуванням забезпечується стійкість в умовах апаратних або програмних проблем.
Distributed data storage subsystem remains operational even when it encounters network problems or data storage nodes failure. Thanks to the dynamic load balance, data distribution methods, and fault-tolerant design, it provides resilience in case of hardware and software problems.

=== _Performance_
//Завдяки паралельному доступу для читання та запису обʼєктів в сховищі (завдяки розподілу даних на маленькі шматки та реплікації їх між декількома OSD та алгоритму CRUSH) та адаптивному балансуванню навантаження підсистема розподіленого зберігання даних забезпечує високу пропускну здатність та продуктивність.
Distributed data storage subsystem provides high performance and throughput thanks to parallel storage object read/write availability (data is broken into small parts and replicated between several OSD, and CRUSH algorithm is used), and adaptive load balancing.
