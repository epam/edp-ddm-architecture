= Platform for state registries: Architectural blueprint, best practices, and guidelines for global deployment and maintenance
include::platform:ROOT:partial$templates/document-attributes/default-set-en.adoc[]

<<<

== Document goals

This document offers an in-depth exploration of the Platform for state registries. Beyond the technical specifications, it encapsulates valuable insights from our journey as system developers. We shed light on pivotal facets like stakeholder management and robust CI/CD practices, intending to furnish actionable guidelines and best practices. Our overarching objective is to streamline the process for other vendors, paving the way for the international dissemination, deployment, maintenance, and evolution of software systems across varied landscapes.

[NOTE]
====
The furnished document and the accompanying system documentation serve as guiding principles for the team responsible for supporting the platform. This encompasses the development of new features, collaboration with vendors for feature implementation, and the management of the open-source community. It is imperative to note that the documents themselves do not guarantee the establishment of a functional and efficient process.

The pivotal element in constructing such a process is the formation of a dedicated team within a government organization. This team is tasked with crafting the process in accordance with the provided guidelines, with a specific focus on addressing the developmental aspects. Furthermore, this team is positioned to offer recommendations and support to registry development teams, augmenting reference examples and conducting audits of developed registries. An additional facet of their responsibilities may involve the installation and adaptation of the platform for new countries.
====

== What is the Platform for state registries

The *_Platform for state registries_* is an information system designed to efficiently deliver government services in a digital format. It enables the rapid creation, modeling, deployment, and maintenance of electronic state registries, striking an optimal balance between data security requirements, deployment speed, registry ownership costs, and data exchange with other registries.

[high-level-structure]
=== High-level structure

The *_Registries Platform_* is a powerful distributed system using the latest and greatest open-source technologies. Built with an innovative microservice architecture, it's a cloud-native platform that guarantees reliability, scalability, and infrastructure independence.

Look at the accompanying diagram, which illustrates how the Platform is structured across multiple levels and zones. You'll notice how it's divided into distinct zones and subsystem levels, each with unique interactions.

.Platform structure diagram
image::ROOT:giz-cicd/ddm-platform-structural-view.drawio.png[width=80%, height=60%]

Within this Platform, you'll find two specific zones that operate at a system level and manage administrative and operational traffic. It's a complex yet efficient system that's sure to impress.

Infrastructure ::
The _Registries Platform_ supports deployment in both *public and private cloud environments*, ensuring a versatile foundation for your needs. At its core, the Platform employs a sophisticated *container orchestration mechanism* to ensure seamless operations.
+
[TIP]
You can learn more about the container orchestration platform here: https://diia-engine.github.io/diia-engine-documentation/en/platform/1.9.6/arch/architecture/container-platform/container-platform.html[Container orchestration platform].

Central components of the Platform::
Every instance of the _Registries Platform_ incorporates a level known as the Central components of the Platform. This level is divided into *two logical zones*:

* [*] *Administrative*: Subsystems that facilitate development functions, deployment operations, and the regulation of digital registry services.
* [*] *Operational*: Subsystems that ensure the registry operates following the deployed digital regulations.

Registries ::
A single Registries Platform instance is equipped to service *a cluster of isolated registries*. Every registry tenant is *represented through two distinct zones*:

* [*] *Administrative*: Subsystems that facilitate development functions, deployment operations, and the regulation of digital registry services.

* [*] *Operational*: Subsystems that ensure the registry operates following the deployed digital regulations.

Component for managing the state of Platform resources ::
This component provides the ability to install and update an instance of the Registries Platform.
+
[TIP]
Dive deeper into the intricacies of the Platform resource state management component: https://diia-engine.github.io/diia-engine-documentation/en/platform/1.9.6/arch/architecture/platform-installer/overview.html[Component for managing the state of platform resources].

== Development and testing environment

The *_Platform for state registries_* offers flexibility in its deployment. While developers can deploy components even on personal machines, setting up a comprehensive environment that mirrors the platform's essence is advised for optimal results.

Component development, testing, and deployment ::
Each component within the platform can be developed, tested, and built locally or in remote environments. A more encompassing setup is beneficial for a holistic approach to testing, especially integration testing.

Integration testing ::
Integration tests validate the platform's interactions with real instances of external systems. Successful execution is confirmed through snapshots, detailed logs, and comprehensive reproduction steps.

System availability and compatibility ::
Establishing a reliable environment is essential to guarantee that the system is available and that components are compatible. This method ensures that the components work independently and cohesively with each other without any issues.

Comprehensive Platform setup ::
For those seeking an in-depth understanding of the Platform's architecture, we suggest creating a comprehensive environment that can accommodate its complete setup. If you want to delve deeper into the details, you can access the source code for all components on GitHub. This will enable you to engage in transparent and collaborative development practices.

<<<
=== Infrastructure

The Platform for state registries is *_cloud-agnostic_*, meaning it can function effectively on different cloud services without relying on a specific environment. This flexibility enables clients and developers to choose the most appropriate cloud service for their needs and preferences without being bound to a single provider.

The Platform must be deployed on virtual infrastructures that are officially supported, which currently include:

* [*] *Public clouds*: such as https://aws.amazon.com/[Amazon Web Services (AWS)], https://azure.microsoft.com/[Microsoft Azure (Azure)], and https://cloud.google.com/[Google Cloud Platform (GCP)]. These services provide access to virtual resources over the Internet and are available to the public.

* [*] *Private clouds*: These are clouds intended solely for the use of one organization. For instance, https://www.vmware.com/products/vsphere.html[VMWare vSphere] is a platform enabling the creation of private, especially on-premises cloud infrastructures.

The infrastructures above should install an OKD cluster, the version of which complies with the Platform's requirements, as laid out in the official documentation of the Platform: https://diia-engine.github.io/diia-engine-documentation/en/platform/1.9.6/admin/installation/okd-requirements.html[Platform for state registries: requirements for OKD clusters].

=== Container orchestration platform

*_OpenShift_* is an open-source container management platform that provides advanced orchestration capabilities and deployment of containerized software. It is developed based on Kubernetes and offers a full stack of solutions and abstractions for developing, deploying, managing, and monitoring containers. This platform provides an opportunity to deploy your software provision in any public cloud environment, private cloud environment, or local infrastructure, delivering resilience, reliability, and security for deployed software.

OpenShift is a flexible platform that can be easily extended, supplemented, and integrated with other tools, platforms, and software. This component allows you to have:

* [*] *monitoring and logging* capabilities that provide information about the health and performance of software and infrastructure;
* [*] *network security policies* and *role-based access control (RBAC)* to enable secure publishing and end-user access;
* [*] *backup* and *scaling* of the platform and deployed software, allowing for rapid *recovery* of the system state and responding to load increases or decreases
* [*] *distributed data stores* for storing state and information of stateful applications.

OpenShift is an ideal solution for organizations looking to modernize their software infrastructure and accelerate digital transformation processes. It is the primary component for deploying and managing containerized applications in the *_Registries Platform_*.

.Container orchestration platform. High-level architecture
image::platform:ROOT:giz-cicd/container-orchestration.drawio.png[width=750,float="center",align="center"]

The OpenShift architecture consists of several virtual machines, including:

* *Master virtual machines*. Responsible for managing the overall health of the cluster, including application planning and deployment.
* *Infrastructure and Platform virtual machines*. They contain system operators and applications that provide work for
_Container orchestration Platform_ and _Registries Platforms_.
* *Registry virtual machines*. Run containers with registry software.

=== Managing the state of Platform resources

[platform-components]
==== Platform components

The Platform components, as seen in the repositories on https://github.com/orgs/epam/repositories?q=edp-ddm&type=all[GitHub], are Helm charts that need to be deployed on an OpenShift cluster in a specific sequence and connection.

.Component deployment and interaction diagram
[plantuml]
----
@startuml

skinparam DefaultFontName Helvetica

skinparam backgroundColor #F2F2F2
skinparam component {
  BackgroundColor #e1e1ea
  BorderColor #34495E
  FontColor black
  BorderThickness 2
}
skinparam package {
  BackgroundColor #E74C3C
  FontColor black
  BorderThickness 2
}

title Component deployment and interaction diagram

package "DDM Platform install" as ddm_platform_install #ffe6e6 {
  [OKD Install] as okd_install
  [DDM Core Install] as ddm_core_install
}
ddm_core_install -[#34495E]-> infra_components: deploys Nexus, Ceph, and Platform Keycloak

ddm_core_install -[#34495E]-> ddm_cp_install: deploys Control Plane components

package "Infra Components" as infra_components #e6faff {
    [catalog-source]
    [monitoring]
    [storage]
    [logging]
    [service-mesh]
    [backup-management]
    package "user-management" as user-management_infra #f2e6ff {
        [groupsync-operator]
        [keycloak-operator]
        [keycloak]
        [hashicorp-vault]
    }
    [control-plane-nexus]
    [external-integration-mocks]
    package "Outside Openshift" #ecd9c6 {
      [platform-vault]
      [platform-minio]
    }
}

package "DDM Control Plane (CP)" as ddm_cp_install #e6ffe6 {
  [keycloak-operator]
  [codebase-operator]
  [control-plane-console]
  [control-plane-gerrit] as ddm_gerrit
  [control-plane-jenkins]
  [infrastructure-jenkins-agent]
}

okd_install -[hidden]-> infra_components
ddm_gerrit -u-> infra_components : contains the composite repository "cluster-mgmt", which defines specific versions of infrastructure components

ddm_gerrit -d-> registry_components : contains registry templates "registry-tenant-template-*", which define specific versions of registry components

package "Registry regulations" as registry_regulations #cce6ff {
  [registry-regulation-template-minimal]
  [registry-regulation-template-recommended]
}

package "Registry components" as registry_components #ffffe6 {
  [bpms]
  [officer-portal]
  [jenkins-operator]
  [dataplatform-jenkins-agent]
  [gerrit-operator]
  [codebase-operator]
  [user-process-management]
  [bp-admin-portal]
  [user-task-management]
  [kong]
  [form-management]
  [admin-portal]
  [redash-chart]
  [pg-exporter-chart]
  [citus]
  [strimzi-kafka-operator]
  [nexus]
  [keycloak-operator]
  [kafka-schema-registry]
  [citizen-portal]
  [user-settings-service-persistence]
  [user-settings-service-api]
  [bp-webservice-gateway]
  [digital-document-service]
  [hashicorp-vault]
  [excerpt-service-api]
  [excerpt-worker]
  [report-exporter]
  [registry-configuration]
  [process-history-service-api]
  [...]
}

ddm_gerrit -r-> registry_regulations : creates repository "registry-regulations-template"

legend
 Each component is a separate git repository
end legend

@enduml
----

It can be time-consuming to deploy these components manually. Hence, utilizing the _Component for managing the state of Platform resources_ is more efficient.

[installer-managing-resources]
==== Installer: Component for managing the state of Platform resources

The *_Installer_* is a set of software tools that offer the capability to *install and update* an instance of the _Registries Platform_.

Key functions: ::

* [*] Installing the Registries Platform
* [*] Updating the Registries Platform
* [*] Deploying the Central service for managing Platform secrets
* [*] Deploying the Platform backup storage

The following diagram illustrates the components encapsulated within the Platform Installer and how they interact with other subsystems:

.Platform Installer. Components deployment and update
image::platform:ROOT:giz-cicd/platform-installer-subsystem.drawio.png[]

____
So, with the _Installer_, you can quickly deploy the Platform and its components in a few straightforward steps:

. Setting up your environment.
. Installing the OKD cluster in this environment.
. Preparing and launching the installer itself.

See https://diia-engine.github.io/diia-engine-documentation/en/platform/1.9.6/admin/installation/platform-deployment/platform-deployment-overview.html[Deploying the Platform on target environments].
____

Installer modules description (functions.sh) ::
+
*INIT-CHECK*: Checks essential parameters.
+
*ENCRYPTION-ETCD*: Sets up ETCD encryption and validates OpenShift certificates.
+
*INSTALL-CLUSTER-MGMT*: Deploys key components: `catalog-source`, `storage`, `keycloak-operator-crd`, `logging`, `service-mesh`.
+
*INSTALL-NEXUS*: Deploys `control-plane-nexus` (_Docker image repository and XSD_).
+
*VAULT-INSTALL*: Initiates the central Vault.
+
*MINIO-INSTALL*: Sets up the central Minio.
+
*INIT-NEXUS*: Loads Docker images to Nexus.
+
*INSTALL-ADDITIONAL-COMPONENTS*: Deploys `user-management`.
+
*INSTALL-CONTROL-PLANE*: Initiates Control Plane components.
+
*NEXUS-RESOURCE-UPLOAD*: Adds XSD resources to Nexus.
+
*BACKUP-CREDENTIALS*: Sets backup access parameters in Minio.
+
*USAGE*: Provides guidance for `install.sh`.

The following diagram presents the structure of the Platform Installer:

.Installer structure. Components and interactions
[plantuml]
----
@startuml

skinparam DefaultFontName Helvetica

skinparam backgroundColor #F2F2F2
skinparam component {
  BackgroundColor #e1e1ea
  BorderColor #34495E
  FontColor black
  BorderThickness 2
}
skinparam package {
  BackgroundColor #E74C3C
  FontColor black
  BorderThickness 2
}

title Installer components and interactions

package "Installer" as installer #e6f3ff {
    package "images" as images #ffffe6 {
        [external docker images]
        [registry docker images]
    }
    package "nexus" as nexus #ffffe6 {
        [liquibase-ext-schema]
    }
    package "repositories" as repositories #ffffe6 {
        package "control-plane" as control-plane #e6ffe6 {
            [codebase-operator]
            [control-plane-console]
            [control-plane-gerrit]
            [control-plane-installer]
            [control-plane-jenkins]
            [ddm-architecture]
            [infrastructure-jenkins-agent]
            [keycloak-operator]
        }
        package "infra" as infra #e6ffe6 {
            [backup-management]
            [catalog-source]
            [control-plane-nexus]
            [external-integration-mock]
            [keycloak]
            [logging]
            [monitoring]
            [service-mesh]
            [storage]
            [user-management]
        }
        package "registry" as registry #e6ffe6 {
            [hashicorp-vault]
            [keycloak-operator]
        }
    }
    package "terraform" as terraform #ffffe6 {
        [minio]
        [vault]
    }
    [control-plane-installer.img] as installer_image
    [docker_load.sh] as docker_load
    [functions.sh] as functions
    [install.sh] as install
}

@enduml
----

=== Development and testing tools

Engage with our high-level diagram to uncover the core technologies and understand how they seamlessly cater to the Registries Platform's diverse needs.

.Key technologies and tools
image::platform:ROOT:giz-cicd/ddm-platform-tech-view.drawio.png[]

TIP: For a deeper dive, visit the https://diia-engine.github.io/diia-engine-documentation/en/platform/1.9.6/arch/architecture/platform-technologies.html[Platform technology stack] page.

=== Development and testing scenarios

Equipped with a target environment and a comprehensive toolkit for development, you're all set to modify any component. Using *_Helm_*, you can easily compile its Docker image locally and deploy it onto a prepared environment within the OKD cluster.

*_Helm_* streamlines and automates creating, packaging, configuring, and deploying Kubernetes applications. It consolidates your configuration files into one versatile package. In a microservices-driven architecture, management can become complex as the application expands and more microservices are added. Helm elegantly tackles this challenge, ensuring efficient deployment and management of these services.

.Development workflow
[plantuml]
----
@startuml

skinparam DefaultFontName Helvetica

skinparam backgroundColor #e6f2ff
skinparam rectangle {
  BackgroundColor #ffffe6
  BorderColor #000000
  FontColor black
  BorderThickness 2
}

title Development workflow

rectangle "Clone source code" as clone
rectangle "Develop" as develop
rectangle "Run unit tests" as unit_tests
rectangle "Build docker image" as build_docker
rectangle "Deploy to env with Helm" as deploy_helm

clone -r-> develop
develop -r-> unit_tests
unit_tests -r-> build_docker
build_docker -r-> deploy_helm

@enduml
----

____
Consider a streamlined deployment strategy? We advocate the *_GitOps_* methodology for its precision and efficiency. By integrating GitOps, you facilitate the automation of infrastructure configuration and component deployment and enhance overall system orchestration.
____

At the heart of GitOps is the Git repository, serving as the definitive source for subsystem configuration files. This method seamlessly governs the Platform's infrastructure and registry deployments, blending automated deployment, efficient version control, simple change reversals, and clear visibility into system changes. This synergy is achieved through Git-based workflows and clear descriptions of the desired state of the Platform and registry.

____
To maximize the benefits of GitOps, automating the integration and deployment of the Platform components is crucial. It's about embracing the *_CI/CD_* approach.
____

The following section provides a closer look at CI/CD.

== Continuous Integration and Continuous Delivery (CI/CD)

=== Overview of CI/CD

*_CI/CD_* represents a harmonized suite of processes empowering developers to craft high-quality software through a fluid, automated process spanning development, testing, delivery, and deployment. This holistic approach amplifies collaboration and drives efficiency throughout the software development lifecycle.

* [*] *Continuous Integration (CI)*: A practice in which developers consistently merge their code changes into a central repository. After each merge, automated builds and tests are triggered, enabling swift detection of integration hiccups.

* [*] *Continuous Delivery (CD)*: Evolution of CI, it ensures the code remains ever-ready for deployment. The focus is on full-throttle automation of the software release process, from changes in code to its testing, culminating in a deployment-ready state. Nevertheless, the deployment decision remains manual.

* [*] *Continuous Deployment (CD)*: Elevating the automation gamut, it instantaneously deploys code changes to production after successfully navigating all CI/CD pipeline tests. This refines the release mechanism by obliterating manual steps in the Continuous Delivery phase.

* [*] *Continuous Testing*: This encompasses the continuous execution of automated tests throughout the software development phase. Incorporating unit, integration, and end-to-end tests ensures swift issue detection and remediation, upholding software reliability.

Collectively, these practices sculpt a robust, agile, and efficient software development and deployment blueprint.

=== Essential CI/CD tools

Building a foundational CI/CD pipeline is achievable with select pivotal tools:

* [*] *Version Control System (VCS)*: A linchpin for monitoring codebase alterations. Git is a popular choice, given its seamless synergy with CI/CD tools. e.g., GitHub, GitLab, Gerrit.

* [*] *Continuous Integration (CI) server*: This server kindles the automation of build and testing processes activated by code amendments. e.g., Jenkins, GitHub Actions, GitLab CI, Travis CI.

* [*] *Build automation tool*: Streamlines the build process. e.g., Maven, Gradle (Java), npm (Node.js).

* [*] *Automated testing framework*: Bolsters code quality via automated test execution in the CI loop. e.g., JUnit (Java), pytest (Python), Jasmine (JavaScript).

* [*] *Artifact repository*: Safeguards build artifacts birthed during the CI phase. e.g., Nexus, JFrog Artifactory.

* [*] *CD orchestration*: Breathes life into the post-CI deployment process. e.g., *Jenkins*, *GitLab CI*.

Special mention to *Tekton*, a tool rooted in cloud-native and container-native ethos, tailored for modern container-centric app development and deployment. It bestows a toolkit for defining and orchestrating CI/CD pipelines via code.

=== Delving into CI/CD Pipelines

A *_CI/CD pipeline_* consists of a series of interconnected steps for *_continuous integration and deployment_*, which is crucial for releasing software versions. This approach enhances software delivery throughout its development lifecycle through process automation. Organizations can accelerate their coding processes without sacrificing quality by incorporating CI/CD automation in the development, testing, release, and monitoring stages. While it's possible to execute each step of the pipeline manually, the actual value of CI/CD emerges with automation. The primary goal of this automation is to reduce human errors and ensure a consistent software release process.

The *_CI pipeline (Continuous Integration pipeline)_* is the bedrock of streamlined coding, seamlessly merging code modifications into a cohesive project. Reviewing code changes through a _Version Control System (VCS)_ crafts an updated codebase for the following stages, including _Code Review_ and _Build Pipelines_.

The *_CD pipeline (Continuous Delivery pipeline)_* is your gateway to seamless software delivery. Its essence is the stage-by-stage promotion of application builds, ensuring each phase is validated before moving to the next. Think of it as a quality gatekeeper, overseeing a collection of pivotal applications and their respective stages.

The *Deploy pipeline* adds versatility to the Continuous Delivery journey, offering:

* [*] Streamlined integration of applications into varied environments, ready for auto-testing and progression.
* [*] An adaptable environment for both automated and manual application checks.
* [*] Efficient deployment options, whether you opt for the latest or a specific build from the Docker registry.
* [*] Fluid movement of image builds across environments.
* [*] Smart auto-deployment of applications using the supplied payload.

=== CI/CD quality gates

Each CI/CD pipeline is intricately laced with steps known as *_Quality gates_*.

A *_Quality gate_* is a set of criteria a project must meet to transition from one phase to the next. These gates can be automated or manual, requiring human or team intervention. Integrating Quality gates into CI/CD pipelines ensures that the software only progresses if it aligns with foundational standards and requirements. Navigating through these gates significantly decreases the likelihood of releasing substandard or vulnerable code to the end-user environment, ensuring swift feedback loops with developers.

.General scheme of passing quality gates
image::platform:ROOT:giz-cicd/cicd-common-QG.drawio.png[]

TIP: For more on recommended software development and quality control practices, visit:
https://diia-engine.github.io/diia-engine-documentation/en/platform/1.9.6/platform-develop/coding-standards.html[Recommended software development and quality control practices].

=== CI/CD pipelines and stages

Look at the full-fledged delivery path through pipelines and their respective stages. It's crucial to note that the stages may vary depending on the type of codebase.

Here's a sample breakdown of potential stages that a pipeline might include. Not all of these stages are mandatory, but they provide a glimpse into a comprehensive development pipeline that ensures the high quality of the final product.

.CI/CD pipelines and stages
image::platform:ROOT:giz-cicd/cicd-pipelines.drawio.png[]

Pipelines' stages and components overview ::

* *Init*: This stage kicks off the information collection process. It checks out all files from the selected branch of the Git repository. For the primary branch, it uses the HEAD, and for code review, it sources from the relevant commit.

* *Commit validation*: At this stage, the Merge Request header is verified for compliance with established naming conventions such as http://semver.org[Semantic Versioning] and https://www.conventionalcommits.org/en/v1.0.0/[Conventional Commits]. This can be used to integrate with an issue-tracking system like Jira.

* *Compile*: The code is compiled using appropriate build tools like npm, maven, gradle, etc., ensuring code consistency.

* *Testing*: This stage initiates the testing procedures:

** *Unit testing*: Low-level testing focused on individual methods, functions, components, or modules used in the software. Passing this quality threshold verifies if recent code changes led to any regressions or introduced errors in previously tested software parts.

** *Integration testing*: This tests the interaction between microservice components, such as interactions with databases or Redis.

* *Code linting*: A quality check for detecting and rectifying inconsistent or undesirable code. This step aims to identify potential issues, apply coding standards, and maintain clean code. Common tools include https://eslint.org/[ESLint], https://checkstyle.sourceforge.io/[Checkstyle], and https://pylint.pycqa.org/en/latest/[Pylint].

* *Docker linting*: This involves linting the Dockerfile using https://github.com/hadolint/hadolint[Hadolint], ensuring Docker images adhere to https://docs.docker.com/develop/develop-images/dockerfile_best-practices/[Best practices for writing Dockerfiles]. Additionally, https://github.com/koalaman/shellcheck[Shellcheck] is utilized to lint Bash code in Docker RUN instructions. An alternative tool that can be used is https://github.com/RedCoolBeans/dockerlint[dockerlint].

* *Code quality*: A code quality control tool checks for common issues, ensuring the code aligns with established quality standards for the programming language. A versatile tool for this is https://www.sonarsource.com/products/sonarqube/[SonarQube].

* *Static application security testing (SAST)*: This involves static testing of the microservice source code to identify vulnerabilities that malicious actors could exploit.

* *Secret detecting*: This step involves identifying and preventing the inclusion of secrets in the code.

* *Software composition analysis (SCA)*: Here, open-source components integrated into the microservice codebase are analyzed for security, license compliance, and code quality. This method can identify all related components, their auxiliary libraries, dependencies, software licenses, outdated dependencies, and potential vulnerabilities. Tools like https://dependencytrack.org/[Dependency Track], https://github.com/jeremylong/DependencyCheck[DependencyCheck: OWASP], and https://snyk.io/series/open-source-security/software-composition-analysis-sca/[Snyk] can be employed.

* *Build*: This stage constructs the application.

* *Docker building*: A Docker image is created from the Dockerfile, which is then stored in the current Artifact repository with the corresponding VCS tag for further deployment and testing.

* *Container scanning* involves scanning the built Docker image and its components for known vulnerabilities. This method can be part of SCA testing, but using both types of analysis is recommended for broader risk coverage.

* *Git tag*: This step involves adding a git tag to the main branch in the VCS repository, corresponding to the artifact's version in the Artifact repository. The tag might include the build number and match the artifact's name.

* *Docker image push*: Pushing the built docker image to storage in the Artifact repository for further deployment on the selected environments.

* *Reports aggregator*: A component of the CI/CD process designed to collect and visualize report checks for further analysis if the code and/or artifact does not meet the set quality thresholds.

** For _Build_ and _Deploy_ pipelines, it is recommended to use the https://www.defectdojo.org[DefectDojo] platform for security and vulnerability management for subsequent analysis and handling by development and security engineers.

** A separate platform with additional aggregation, grouping, and testing results management capabilities is recommended for collecting and analyzing reports from automated testing results. An example is https://reportportal.io/[ReportPortal].

* *Deploy*: Either an automatic or manual step of deploying a specified version of the service on a selected environment (Env) using the helm tool following the GitOps approach.

* *Automation testing*: Automated testing of the application's key functionality to detect fundamental and critical problems before moving to the subsequent testing stages. This approach is initiated after each successful deployment of a new microservice version in the environment.

* *QA Testing*: Manual quality assurance of the code according to predefined functionality use scenarios. Typically, it finishes with a manual approval from a QA engineer.

* *Promote*: Promotes final docker images to the Artifact repository.