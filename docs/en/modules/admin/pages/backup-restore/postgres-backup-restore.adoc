:sectnums:
:sectanchors:
//== Резервне копіювання та відновлення БД реєстру
= Backing up and restoring registry databases
//TODO: Is it the correct translation of the title?
:toclevels: 5
:experimental:
:sectnums:
:sectnumlevels: 5
:sectanchors:
:sectlinks:
:partnums:

//Postgres Operator від Crunchy Data (PGO), який використовується для управління БД реєстру,включає в себе pgBackRest - рішення для резервного копіювання та відновлення з відкритим кодом. PGO робить зручним виконання багатьох поширених дій, необхідних протягом життєвого циклу бази даних, зокрема:
The Postgres Operator by Crunchy Data (PGO), used for managing registry databases, includes pgBackRest, an open-source solution for backup and restoring. PGO facilitates the execution of many common tasks required throughout the lifecycle of a database, including:

//* Налаштування розкладів автоматичного резервного копіювання та політик збереження
* Configuring schedules for automatic backup and retention policies.
//* Резервне копіювання даних у декілька місць
* Backing up data to multiple locations.
//**  Підтримка резервного сховища в Kubernetes, AWS S3 (або S3-сумісних системах, таких як MinIO), Google Cloud Storage (GCS) і Azure Blob Storage
** Supporting backup storage in Kubernetes, AWS S3 (or S3-compatible systems like MinIO), Google Cloud Storage (GCS), and Azure Blob Storage.
//* Одноразове створення резервних копій
* Performing one-time backups
//* Виконання «відновлення на певний момент часу» (PITR)
* Performing point-in-time recovery (PITR).
//* Клонування даних у новий екземпляр БД
* Cloning data to a new database instance.
//* та інше.
* And more

//== Налаштування резервного копіювання
[#_configuring_backup]
== Configuring backup
//За замовчанням операційний та аналітичний кластери Postgres налаштовані таким чином, що вони постійно архівують журнал попереднього запису (WAL) та роблять повну резеврну копію раз на добу. Політикою збереження налаштовано зберігання однієї повної копії, таким чином після створення нової копії pgBackRest очистить попередню копію та пов’язані з нею файли WAL.
By default, the Postgres operational and analytical clusters are configured to continuously archive write-ahead logs (WAL) and create a full backup once a day.The retention policy is set to keep one full backup, so when a new backup is created, pgBackRest cleans up the previous backup and its associated WAL files.

//pgBackRest надає різні типи резервного копіювання, налаштувань політики збереження та розкладу резервного копіювання, які надають можливість налаштувати систему у відповідності з цільовими показниками точки відновлення (RPO), цільовим часом відновлення (RTO) та вимогами до викорастання простору.
pgBackRest provides various types of backups, retention policy configurations, and backup schedule options to customize the system according to desired recovery point objectives (RPO), recovery time objectives (RTO), and space utilization requirements.

//=== Управління розкладом резервного копіювання
=== Managing backup schedules
//Розклад можна налаштувати для всіх трьох типів резервних копій:
Schedules can be configured for all three types of backups:

//* `full`: резервна копія всього кластера Postgres. Це найбільший з усіх типів резервного копіювання.
* `full`: backup of the entire Postgres cluster. This is the largest among all types of backups.
//* `differential`: резервна копія всіх даних з часу останньої повної резервної копії.
* `differential` backup of all data since the last full backup.
//* `incremental`: резервна копія всіх даних з моменту останнього повного, диференціального або інкрементного резервного копіювання.
* `incremental`: backup of all data since the last full, differential, or incremental backup.

//Розклади резервного копіювання зберігаються в розділі `spec.backups.pgbackrest.repos.schedules`. Кожне значення в цьому розділі приймає рядок у форматі cron, який визначає розклад резервного копіювання.
Backup schedules are stored in the `spec.backups.pgbackrest.repos.schedules` section. Each value in this section takes a string in cron format that defines the backup schedule.

//Скажімо, політика резервного копіювання полягає в створенні повної резервної копії щотижня в неділю о 1 годині ночі та створенні інкрементних резервних копій о 1 годині ночі щодня, крім неділі. Конфігурація до нашої специфікації повинна виглядати подібно до:
For example, the backup policy consists of creating a full backup every week on Sunday at 1 AM and creating incremental backups every day at 1 AM, except on Sundays. The configuration should look similar to:
[source,yaml]
----
spec:
  backups:
    pgbackrest:
      repos:
      - name: repo1
        schedules:
          full: "0 1 * * 0"
          incremental: "0 1 * * 1-6"
----
//Щоб керувати запланованим резервним копіюванням, PGO створить кілька Kubernetes CronJobs, які виконуватимуть резервне копіювання в указані періоди. Резервні копії використовуватимуть конфігурацію, яку ви вказали.
To manage scheduled backups, PGO will create several Kubernetes CronJobs that perform backups at the specified intervals. The backups will use the configuration you provided.

//=== Управління політиками збереження резервних копій
=== Managing backup retention policies

//PGO дозволяє встановити політики збереження резервних копій для повних і диференціальних резервних копій. Коли термін дії повної резервної копії закінчується, pgBackRest очистить усі пов’язані з нею резервні копії та файли WAL. Наприклад, якщо у вас є повна резервна копія з чотирма пов’язаними інкрементними резервними копіями, коли закінчується термін дії повної резервної копії, термін дії всіх її інкрементних резервних копій також закінчується.
PGO allows you to set backup retention policies for full and differential backups. When the retention period of a full backup expires, pgBackRest will clean up all associated backups and WAL files. For example, if you have a full backup with four associated incremental backups, when the retention period of the full backup expires, the retention period of all its incremental backups also expires.

//Існує два типа політик `repo1-retention-full-type` які можна встановити:
There are two types of `repo1-retention-full-type` policies that can be set:

//* `count`: на основі кількісті резервних копій, які ви хочете зберегти. Це значення за замовчанням.
* `count`: based on the number of backups you want to retain. This is the default value.
//* `time`: на основі часу в днях. Повні резервні копії, старші за `repo-retention-full`, буде видалено зі сховища, якщо є принаймні одна резервна копія, яка дорівнює або перевищує параметр `repo-retention-full`
* `time`: based on the time in days. Full backups older than `repo-retention-full` will be removed from the repository if there is at least one backup that equals or exceeds the `repo-retention-full` parameter

//Наприклад, ми хочемо зберігати повні резервні копії протягом 14 днів. Найзручніший спосіб зробити це через розділ `spec.backups.pgbackrest.global`:
For example, if we want to retain full backups for 14 days, the most convenient way to do this is through the `spec.backups.pgbackrest.global` section:

[source,yaml]
----
spec:
  backups:
    pgbackrest:
      global:
        repo1-retention-full: "14"
        repo1-retention-full-type: time
----

//В такому випадку якщо ми маємо 2 резервні копії: одну 12-денну і другу 15-денну, жодна резервна копія не буде видалена тому що видалення 15-денної копії залишить лише 12-денну копію, що порушило б політику зберегання згідно з якою ми повинні мати принаймні одну резервну копію 14-денної давності, перш ніж видаляти старішу копію.
In this case, if we have 2 backups: one 12-day-old and another 15-day-old, no backups will be deleted because deleting the 15-day-old backup would leave only the 12-day-old backup, which would violate the retention policy of having at least one backup of 14-day age before removing older backups.

//Може бути так, що добовий об'єм WAL логів дуже значний і збереження простору у сховищі резервних копій важливіше за можливість виконувати відновлення на момент часу (PITR) на значну глибину. Для налаштування зберігання WAL логів для певного числа резервних копій pgBackRest має наступні параметри:
It may be the case that the daily volume of WAL logs is significant, and saving storage space in the backup repository is more important than the ability to perform point-in-time recovery (PITR) to a significant depth. To configure the retention of WAL logs for a certain number of backups, pgBackRest has the following parameters:

//* `repo1-retention-archive` - Кількість резервних копій для яких буде збережено WAL
* `repo1-retention-archive`: The number of backups for which WAL will be retained.

//* `repo1-retention-archive-type` - Тип резервної копії для збереження WAL `(incr,diff,full)`. Якщо встановлено значення `full`, pgBackRest зберігатиме WAL для кількості повних резервних копій, визначених `repo-retention-archive`. Якщо встановлено значення diff , pgBackRest зберігатиме архівні журнали для кількості повних і диференціальних резервних копій, визначених `repo-retention-archive`. Якщо встановлено значення `incr`, pgBackRest зберігатиме журнали архіву для кількості повних, диференціальних і інкрементних резервних копій, визначених `repo-retention-archive`.
* `repo1-retention-archive-type`: The type of backup for retaining WAL `(incr, diff, full)`. If set to `full`, pgBackRest will retain WAL for the number of full backups defined by repo-retention-archive. If set to `diff`, pgBackRest will retain archive logs for the number of full and differential backups defined by repo-retention-archive. If set to `incr`, pgBackRest will retain archive logs for the number of full, differential, and incremental backups defined by repo-retention-archive.

//=== Одноразове створення резервної копії
//TODO: Not sure if this is the right translation. Other options include disposable backup.
=== Creating a one-time backup

//По-перше, вам потрібно налаштувати розділ `spec.backups.pgbackrest.manual`, щоб мати можливість зробити одноразову резервну копію. В ньому міститься інформація про тип резервної копії, яку ви хочете зробити, та будь-які інші параметри конфігурації pgBackRest.
First, you need to configure the `spec.backups.pgbackrest.manual` section to be able to perform a one-time backup. It contains information about the type of backup you want to create and any other pgBackRest configuration parameters.

//Налаштуємо custom resource для одноразового створення повної резервної копії:
Let's configure a custom resource for creating a one-time full backup:
[source,yaml]
----
spec:
  backups:
    pgbackrest:
      manual:
        repoName: repo1
        options:
         - --type=full
----

//Це ще не запускає резервне копіювання – ви повинні додати анотацію `postgres-operator.crunchydata.com/pgbackrest-backup` до свого custom resource. Найкращий спосіб налаштувати цю анотацію — за допомогою позначки часу, щоб ви знали, коли ви ініціалізували резервну копію.
This doesn't initiate the backup yet - you need to add the `postgres-operator.crunchydata.com/pgbackrest-backup` annotation to your custom resource. The best way to configure this annotation is by using a timestamp label, so you know when you initiated the backup.

//Наприклад, для operational кластера  ми можемо виконати таку команду, щоб запустити одноразове резервне копіювання:
For example, for an operational cluster, you can execute the following command to initiate a one-time backup:
[source,bash]
----
kubectl annotate -n postgres-operator postgrescluster operational \
  postgres-operator.crunchydata.com/pgbackrest-backup="$(date)"
----
//PGO виявить цю анотацію та створить нове одноразове завдання резервного копіювання!
PGO will detect this annotation and create a new disposable backup task!

//Якщо ви збираєтеся робити одноразові резервні копії з подібними параметрами в майбутньому, ви можете залишити їх у специфікації; просто оновіть анотацію до іншого значення під час наступного резервного копіювання.
If you plan to make one-time backups with similar parameters in the future, you can leave them in the specification; simply update the annotation to a different value during the next backup.

//Щоб повторно запустити наведену вище команду, вам потрібно буде додати помітку `--overwrite`, щоб можна було оновити значення анотації, тобто.
To re-execute the above command, you will need to add the `--overwrite` flag to update the annotation value, like in the below example:
[source,bash]
----
kubectl annotate -n postgres-operator postgrescluster operational --overwrite \
  postgres-operator.crunchydata.com/pgbackrest-backup="$(date)"
----
== Restoring

//=== Відновлення на момент часу чи на конкретну резервну копію
=== Point-in-Time Recovery (PITR) or restoring a backup copy
//Для того щоб відновити стан БД на потрібну дату і час в першу чергу в секцію `spec.backups.pgbackrest` потрібно додати наступне:
To restore the database to a specific date and time, you need to add the following to the `spec.backups.pgbackrest` section:
[source,yaml]
----
spec:
  backups:
    pgbackrest:
      restore:
        enabled: true
        repoName: repo1
        options:
        - --type=time
        - --target="2022-06-09 14:15:11-04"
----
//де `--target` цільовий час відновлення PITR. Прикладом цілі відновлення є `2022-06-09 14:15:11-04`.
where `--target` is the target time for PITR. An example of a restoring target is `2022-06-09 14:15:11-04`.

//Щоб відновити базу на конкретну резервну копію, в секцію `spec.backups.pgbackrest` треба додати наступне:
To restore the database to a specific backup, you need to add the following to the `spec.backups.pgbackrest section`:
[source,yaml]
----
spec:
  backups:
    pgbackrest:
      restore:
        enabled: true
        repoName: repo1
        options:
        - --type=immediate
        - --set=20220602-073427F_20220602-073507I
----
//де `--set` назва цільової резервної копії. Список доступних резервних копій можно переглянути в бакеті s3 резервного сховища, або виконавши команду pgbackrest info --stanza=db в консолі пода БД.
where `--set` is the name of the target backup. You can view the list of available backups in the S3 backup storage bucket or by executing the command `pgbackrest info --stanza=db` in the database console.

//Тепер, щоб ініціювати відновлення, ви повинні додати анотацію `postgres-operator.crunchydata.com/pgbackrest-restore` наступним чином:
Now, to initiate the restoring process, you need to add the annotation `postgres-operator.crunchydata.com/pgbackrest-restore` as follows:
[source,bash]
----
kubectl annotate -n postgres-operator postgrescluster operational --overwrite \
  postgres-operator.crunchydata.com/pgbackrest-restore=id1
----
//Після завершення відновлення додане налаштування можна вимкнути:
Once the restoring process is complete, you can disable the added configuration:

[source,yaml]
----
spec:
  backups:
    pgbackrest:
      restore:
        enabled: false
----

[IMPORTANT]
====
//Всі ці операції потрібно провести як на операційній так і на аналітичній базі данних.
All these operations need to be performed on both the operational and analytical databases.
//Для відновлення відповідності данних між операційною та аналітичною БД виконайте <<Узгодження данних на аналітичному кластері>>
To synchronize data between the operational and analytical databases, perform  <<Synchronize data on the analytical cluster>>.
====

//=== Клонування з резервної копії
=== Cloning from a backup

//Для клонування БД з резервної копії треба додати в маніфест, який створює новий экземпляр БД, секцію `spec.dataSource`.
To clone a database from a backup, you need to add the `spec.dataSource` section to the manifest that creates a new database instance.
//Для відновлення на момент часу секція буде виглядати подібно до:
To restore to a specific point in time, the section will look similar to the below example:
[source,yaml]
----
spec:
  dataSource:
    pgbackrest:
      stanza: db
      configuration:
      - secret:
          name: s3-conf
      global:
        repo1-path: "/postgres-backup/source_system/operational"
        repo1-s3-uri-style: path
        repo1-storage-verify-tls: n
        repo1-storage-port: "9000"
      options:
      - --type=time
      - --target="2022-06-09 14:15:11-04"
      repo:
        name: repo1
        s3:
          bucket: "bucketName"
          endpoint: "endpoint"
          region: "us-east-1"
----
//Щоб відновити базу на конкретний бекап в секції `spec.dataSource.pgbackrest.options` треба змінити тип відновлення та задати ім’я резервної копії:
To restore the database from a specific backup, in the `spec.dataSource.pgbackrest.options` section you need to change the restoring type and add the name of the backup copy:
[source,yaml]
----
      options:
      - --type=immediate      
      - --set=20220602-073427F_20220602-073507I
----
[IMPORTANT]
====
//Всі ці операції потрібно провести як на операційній так і на аналітичній базі данних.
All of these operations need to be performed on both the operational and analytical databases.

//Для відновлення відповідності данних між операційною та аналітичною БД виконайте <<Узгодження данних на аналітичному кластері>>
To restore data consistency between the operational and analytical databases, perform the following: <<Synchronize data on the analytical cluster>>
====
//=== Узгодження данних на аналітичному кластері
=== Synchronize data on the analytical cluster

//Оскільки операційна та аналітична баз реплікуються у асинхронному режимі, їх резервні копії не синхронізовані. Тому навіть при відновленні на той самий момент часу не може бути гарантована узгодженість данних між цими базами. Щоб привести відновлені бази в узгоджений стан потрібно виконати наступні кроки на базі `registry` аналітичного экземпляра:
Since the operational and analytical databases are asynchronously replicated, their backups are not synchronized. Therefore, even when restoring to the same point in time, data consistency between these databases cannot be guaranteed. To bring the restored databases into a synchronized state, perform the following steps on the `registry` database of the analytical instance:

//    * Зупинити підписку: [source,sql]`ALTER SUBSCRIPTION operational_sub DISABLE;`
    * Disable the subscription: [source,sql]`ALTER SUBSCRIPTION operational_sub DISABLE`;
//    * Очистити всі таблиці, які входять до підписки: [source,sql]`SELECT 'TRUNCATE' ||' '||srrelid::regclass ||' '||'CASCADE;' FROM pg_subscription_rel \gexec`
    * Truncate all tables included in the subscription: [source,sql]`SELECT 'TRUNCATE' ||' '||srrelid::regclass ||' '||'CASCADE;' FROM pg_subscription_rel \gexec`;
//    * Видалити підписку: `DROP SUBSCRIPTION operational_sub;`
    * Drop the subscription: `DROP SUBSCRIPTION operational_sub`;
//    * Створити підписку: `create subscription operational_sub connection 'host=OperationalHost user=postgres dbname=registry password=XXXXXX' PUBLICATION analytical_pub WITH(create_slot=false,slot_name=operational_sub);`
    * Create the subscription: `create subscription operational_sub connection 'host=OperationalHost user=postgres dbname=registry password=XXXXXX' PUBLICATION analytical_pub WITH(create_slot=false,slot_name=operational_sub)`;

//Після того як закінчиться первинна синхронізація таблиць на аналітичному кластері логічная реплікація включиться автоматично і надалі операційний і аналітичний кластер будуть в узгодженому стані.
Once the initial table synchronization on the analytical cluster is completed, logical replication will automatically be enabled, and the operational and analytical clusters will be in a synchronized state.

//== Безперервне відновлення на Standby-кластері
== Continuous restoring on the standby cluster
//Удосконалені стратегії високої доступності та аварійного відновлення передбачають розподіл кластерів баз даних між центрами обробки даних, щоб максимально збільшити час безвідмовної роботи. PGO надає способи розгортання кластерів postgres, які можуть охоплювати кілька кластерів Kubernetes за допомогою зовнішньої системи зберігання даних.
Advanced high availability and disaster recovery strategies involve distributing database clusters across multiple data centers to maximize uptime. PGO provides ways to deploy PostgreSQL clusters that can span multiple Kubernetes clusters using an external storage system.

//=== Створення Standby-кластера
=== Creating a standby cluster

//Репозиторій резервних копій основного кластера має бути доступний для standby кластера.
The backup repository of the primary cluster needs to be accessible to the standby cluster.
//При створенні standby кластера необхідно додати в маніфест `standby.enabled` встановлений у `true` та параметри s3 репозиторія резервних копій основного кластера:
When creating a standby cluster, you need to add the following to the manifest with `standby.enabled` set to true and the backup s3 repository parameters for the primary cluster:
[source,yaml]
----
spec:
  backups:
    pgbackrest:
      image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:ubi8-2.40-1
      repos:
      - name: repo1
        s3:
          bucket: "bucket"
          endpoint: "primary.endpoint"
          region: "ca-central-1"
  standby:
    enabled: true
    repoName: repo1
----
//=== Підвищення (promote) Standby-кластера
=== Promoting the standby cluster
//Перед підвищенням standby кластера нам потрібно переконатися, що ми випадково не створимо сценарій "розщепленого мозку". "Розщеплення мозку" може статися, якщо два основні екземпляри намагаються записати в одне сховище. Якщо основний кластер все ще активний, переконайтеся, що ви вимкнули його, перш ніж намагатися підвищити standby кластер.
Before promoting the standby cluster, we need to ensure that we don't accidentally create a "split-brain" scenario where two primary instances try to write to the same storage. If the primary cluster is still active, make sure you have shut it down before attempting to promote the standby cluster.

//Коли основний кластер стане неактивним, ми можемо підвищити standby кластер, видаливши або вимкнувши його розділ `spec.standby`:
When the primary cluster becomes inactive, we can promote the standby cluster by removing or disabling its `spec.standby` section:
[source,yaml]
----
spec:
  standby:
    enabled: false
----
//Ця зміна запускає підвищення Standby кластера до основного, і кластер починає приймати записи.
This change triggers the promotion of the standby cluster to the primary status, and the cluster starts accepting writes.

