:toc-title: On this page:
:toc: auto
:toclevels: 5
:experimental:
:sectnums:
:sectnumlevels: 5
:sectanchors:
:sectlinks:
:partnums:

= Performance Testing

CAUTION: The page is under development.

== Introduction

=== Purpose
The purpose of providing performance testing is to evaluate and validate a Platform’s performance, reliability, and scalability under various workload conditions. It identifies bottlenecks and problems that needed to be solved in order to improve user experience.  It ensures that the Platform and registries on it can handle the expected number of users or transactions (such as, business process execution, fetching data on dashboards, CRUD operations with different internal or external services or registries, etc).

=== Objectives
The main objectives of performance testing are:

* Ensuring that the Platform and registries on it meet workload requirements: it helps to verify whether the application meets defined performance criteria such as the number of users interacting with the system, response times, throughput, and resource utilization under normal and peak workload conditions.
* Identifying performance bottlenecks: it enables the detection and analysis of potential bottlenecks in the Platform, which can negatively affect overall system performance. These could include inefficient code, improper configurations, or hardware limitations.
* Evaluating scalability: it assesses the Platform’s and registries ability to gracefully handle an increasing number of users or workload without significantly degrading response times and system stability. This helps to determine if the system can support future growth in terms of user base and data volume.
* Ensuring stability and reliability: it validates the Platform’s and registries ability to maintain consistent performance under a continuous, heavy load for an extended duration.
* Providing recommendations for performance improvement: it helps in determining potential improvements in the architecture, code, configurations, and hardware, which can lead to better and more efficient use of the resources.
* Enhancing user experience: Well-performing registries and admin tools on the Platform with quick response times and minimal downtime contribute to a positive user experience.

=== Scope of performance testing
The Scope of performance testing includes all registry components of the Platform.


== Performance Tactics
Performance tactics are techniques and strategies used to optimize the performance of the Platform services. These tactics address the various aspects of a system's performance, including response times, throughput, resource utilization, scalability, and stability. Here is the list of tactics used to optimize the Platform’s performance:

=== Control Resource Demand
**Load balancing**: Distributing service's evenly across multiple nodes to reduce response times, prevent overloading, and ensure optimal resource utilization using.

**Caching**: Storing frequently-used data or precomputed results in memory to reduce the latency of future requests. Caching reduces load and improves response times for repeated requests.

**Code optimization**: Implementing efficient algorithms and data structures, removing redundant or unnecessary code, minimizing the use of heavy resources in the Platform's code, using internal routes for cross-service interaction.

=== Manage Resources
Scaling the infrastructure services vertically by adding more resources to a single service (e.g., CPU, memory) or horizontally by adding more pods to the service. This can improve the system's capacity to handle increased workload and user traffic.

=== Monitoring
Continuously (each release) monitoring Platform's and registries performance to identify and eliminate performance bottlenecks, as well as proactively addressing potential issues before they become critical on design, grooming and implementation phases.


== Performance testing approach
For performance testing of Platform’s services is used an automated testing approach that involves using tools, scripts, and processes to automatically execute, monitor, and analyze performance tests. Implementing an automated approach in performance testing offers several benefits, such as increased efficiency, accuracy, repeatability, and reduced human effort. Key aspects of implementing an automated approach are as follows:

* test scenario design;
* test data definition and generation;
* tool selection;
* test execution (per release) on separate environment;
* monitoring and analytics;
* reporting.


=== Performance scenarios
E2E user flows on API level of one of the developed registry regulation (see detailed description of the regulation in Test data section) and separate GET/POST operations to the database had been taken as a basis for registry performance scenarios.

Conditions and type of launch should be defined for each scenario before the execution.

**Conditions**: execution time (e.g. 1 hour), number of users (e.g. 1500), number of registries (e.g. 5)

[TIP]
The number of users may vary and depends on the baseline metrics stated in the requirements for the type of registry.

**Type of launch**: Load (expected load), Stress (increased load)

Here is the list of scenarios:

==== GET (Read) operations to the database
The current scenario is focused on studying the performance of the isolated Platform database components and provides an opportunity to ensure that the component can withstand the expected loads. The test executes the following steps: logging into the Portal and obtaining a list of regions and districts from database, as this request operates with the largest data array and number of references.

==== POST (Write) operations through the business process level and database
The current scenario is focused on studying the performance of the registry Platform components in integration, ensuring that the main functional components of the Platform can withstand the expected loads. The test performs the following steps: logging into the Portal, retrieving information from the Portal dashboard, and creating a new chemical factor in database.

==== E2E scenario based on integration and user interaction through the Officer and Citizen portals
The current scenario is focused on studying the performance of the Platform as a whole and simulates the main user scenarios and their interactions: creating a laboratory, changing its data, adding staff, etc. These scenarios are adjusted according to the prevalence weight among users and the corresponding delays for typical operations. This scenario runs agains 1 and 5 registries as a separate tests.

E2E scenario steps visualisation are described below:

.Scheme of E2E scenario based on the prevalence weight among users
image::testing:performanceTesting/img-1.png[align="center"]

pass:[<br>]

.API requests scheme of business processes execution for E2E scenarios
image::testing:performanceTesting/img-2.png[align="center"]


=== Types of Performance Testing
There are several types of performance tests, each focusing on different aspects of the Platform’s performance behaviour:

==== Load testing
Checks the application's ability to perform under an expected users load. The goal is to identify and analyze the system's behavior, such as response times, throughput, and resource utilization, when multiple users access the application simultaneously and identify performance bottlenecks. Usually used in all scenarios described in the previous chapter.

==== Stress testing
Evaluates the system's performance and stability under extreme or heavy user interactions. It identifies the breaking point of the Platform and registries and helps uncover unexpected issues. Usually used in login, read/write operations to a database.

==== Endurance testing
Assesses the Platform’s and registries reliability and robustness by subjecting it to a continuous load over an extended period of time. 
This type of testing helps identify issues like memory leaks, resource depletion, and gradual performance degradation. Usually used in all scenarios for the 8-hour period.

==== Scalability testing
Measures the Platform’s and registries ability to scale up or down in response to changes in load, user traffic, or resource requirements. This includes aspects like vertical scaling (adding more resources to a service that produced a bottleneck) and horizontal scaling (running tests against multiple registries).

==== Resilience Testing
Evaluates the Platform’s and registries ability to maintain functionality and performance when faced with adverse conditions, such as system failures, hardware degradation, or network disruptions. The goal is to ensure the system can recover gracefully and continue to provide an acceptable user experience under such circumstances.


=== Test data

==== Registry regulation structure
Data (business processes and forms, data-model with an initial load) of one of the developed registry regulations (certified laboratories registry regulation) is used as a basis for all performance tests.

**Data model**

The data model (xref:arch:attachment$/architecture/performanceTesting/physicalModel.pdf[Download]) is built on the basis of a real excel-like register for the Ministry of Labor. CRUD endpoints for adding, reading, updating and deleting values are developed for each directory and table. Filling out forms with data received from a database is provided in Search conditions.

.Physical data model
image::testing:performanceTesting/physicalDataModel.png[align="center"]

**Business processes**

Business processes and the data model are consistent with each other. Data validation rules on business process forms and in the data model are not contradictory.

List of business processes used in performance tests:

* Create laboratory (xref:arch:attachment$/architecture/performanceTesting/add-lab.bpmn[Download])
* Update laboratory (xref:arch:attachment$/architecture/performanceTesting/update-lab.bpmn[Download])
* Create new personnel (xref:arch:attachment$/architecture/performanceTesting/add-personnel.bpmn[Download])
* Create a chemical dictionary (xref:arch:attachment$/architecture/performanceTesting/update-dict.bpmn[Download])
* Create an application for initial entry (xref:arch:attachment$/architecture/performanceTesting/create-app-primary.bpmn[Download])
* Create an application for the expansion of factors (xref:arch:attachment$/architecture/performanceTesting/create-app-expanse.bpmn[Download])
* Create an application to remove the Laboratory from the list (xref:arch:attachment$/architecture/performanceTesting/create-app-exclude.bpmn[Download])

==== Test Users
Test users are generated in Keycloak service with the appropriate attributes and roles before each test execution.


=== Test tools
Load tests are written using the JMeter tool (industry standard) and the Carrier accelerator (https://public.getcarrier.io/), which directly runs the tests, accumulates the results of their execution in a real-time on the corresponding Dashboard (reports), and provides tools for their analysis.


=== Test environment
An Openshift cluster in AWS has been used for systematic performance testing. A separate registry (perf-test) is created on it, and all necessary stubs (mocks) of integration modules to external systems are configured there. Testing is carried out in isolation from external systems and does not operate with external data sources.


=== Monitoring and analytics
For successful analysis of peaks and bursts of loads, the following monitoring and logging tools are used:

* *Kibana/ElasticSearch -- for searching and analyzing of the Platform and registry logs;
* **Grafana/Prometheus** at the centralized services level -- for monitoring performance metrics of central components;
* **Grafana/Prometheus** at the registry services level -- for monitoring performance metrics of registry components;
* **Jaeger (Kiali)** -- for monitoring "requests/response" tracing.

=== Reporting
The reports are prepared by the dedicated team lead after each iteration of performance scripts execution and published to Platform's documentation.

The performance reports contain:

* metrics and statistics taken from Carrier, Grafana and Jaeger tools: general scenario characteristics, main execution chart, number of requests per time unit chart, table of parameters by each request, resources usage (CPU, RAM, network usage), table of CPU usage per service, table of RAM usage per service, table of network usage per service;
* list of issues (with request name, URL, response code, error message) that occurred during test execution;
* general conclusion about the performance of the registry and it services.

== Performance test schedule
Performance testing conducted in each release. If some issues are detected, appropriate action items are formed by the dedicated team lead and implemented within the release activities. Once all necessary changes are implemented and tested a new round of performance testing is conducted to confirm the absence of performance-related issues.