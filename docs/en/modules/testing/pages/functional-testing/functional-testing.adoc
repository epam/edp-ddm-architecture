:toc-title: On this page:
:toc: auto
:toclevels: 5
:experimental:
:important-caption:     ВАЖЛИВО
:note-caption:          ПРИМІТКА
:tip-caption:           РЕСУРС
:warning-caption:       ПОПЕРЕДЖЕННЯ
:caution-caption:       УВАГА
:example-caption:           Приклад
:figure-caption:            Зображення
:table-caption:             Таблиця
:appendix-caption:          Додаток
:sectnums:
:sectnumlevels: 5
:sectanchors:
:sectlinks:

= Functional  Testing

CAUTION: Сторінка у процесі розробки.

== Introduction

=== Purpose
Functional testing is a crucial aspect of the software development lifecycle that focuses on verifying whether a software application's individual functions or features work as intended and meet the specified requirements. The primary purpose of functional testing is to ensure that the software behaves correctly according to the defined functional specifications and user expectations. It helps identify defects and discrepancies in the software's behavior, ensuring that it delivers the intended value to users.

=== Objectives

The main objectives of function testing are:


* Validating Requirements: Functional testing helps verify that the software application aligns with the documented requirements and specifications. By executing test scenarios based on these requirements, it ensures that the software meets the intended objectives.
* Defect Detection: Functional testing aims to identify defects, bugs, and errors in the software. By systematically testing various functions and features, QA teams can uncover issues that might negatively impact the user experience or hinder the application's functionality.
* User Experience Assurance: Functional testing ensures that the software provides a seamless and user-friendly experience. It validates that end-users can interact with the application without encountering unexpected errors or incorrect behavior.
* Regression Testing: As software evolves and new features are added, functional testing helps prevent regression issues. It ensures that new code changes do not break existing functionality, maintaining the overall stability of the application.
* Validation of Business Logic: Many software applications rely on complex business rules and logic. Functional testing validates that these rules are correctly implemented and produce the expected results.
* Verification of Integration: Functional testing is essential for ensuring that different components and modules of the software integrate correctly and collaborate as intended. This helps identify integration issues early in the development process.
* Compliance and Standardization: In some industries, software needs to comply with specific regulations or standards. Functional testing verifies that the software adheres to these requirements.
* Enhancing Product Quality: By identifying and rectifying defects early in the development cycle, functional testing contributes to overall product quality improvement. This results in a more reliable and robust software application.
* Risk Mitigation: Effective functional testing minimizes the risk of releasing a faulty or substandard product to end-users. By identifying and resolving issues before deployment, it helps mitigate potential financial, reputational, and legal risks.
* Customer Satisfaction: Ultimately, the goal of functional testing is to ensure that the software meets user expectations and delivers value. By identifying and addressing functional issues, it enhances customer satisfaction and loyalty.

=== Scope of functional testing

Scope of functional testing includes all registry and platform components of the Platform.

== Functional Testing Approach
=== Functional Testing Methodologies

The functional testing follow a combination of manual and automated testing approaches. It includes unit testing, integrational testing, system testing, acceptance testing, regression testing and fuctional testing best practicies.

Here are the list of used functional testing methodologies:

* **Unit testing**: Unit testing involves testing individual components, functions, or modules in isolation. Each unit is tested independently to ensure its correctness and functionality. The primary goal of unit testing is to validate that each unit of code performs as intended. It helps identify defects early in the development cycle and ensures that smaller components work correctly before being integrated into the larger system. Unit tests are typically automated and focus on specific input-output scenarios. Developers write unit tests as part of the development process to ensure code reliability and maintainability.

* **Integration Testing**: Integration testing verifies the interactions and interfaces between different components or modules. It ensures that integrated components work together as expected. The purpose of integration testing is to detect errors related to data exchange, communication, and collaboration between components. It helps identify issues that might arise due to the integration of individual units. Integration testing can be done using various strategies like top-down, bottom-up, or sandwich approaches. Testers might use stubs or mocks to simulate the behavior of dependent components that are not yet available.

* **System Testing**: System testing evaluates the entire software application as a complete and integrated entity. It involves testing the application's functionalities, interfaces, and interactions in a holistic manner. The goal of system testing is to validate that the software meets the specified requirements and behaves as expected in different scenarios. It covers end-to-end scenarios and assesses the software's overall behavior. System testing often includes different types of tests, such as functional, performance, security, and usability testing. It focuses on user scenarios and real-world usage.

* **Acceptance Testing**: Acceptance testing evaluates whether the software meets the acceptance criteria defined by stakeholders and users. It determines whether the software is ready for deployment. The purpose of acceptance testing is to ensure that the software aligns with user expectations and business requirements. It aims to verify that the software delivers value and meets the needs of its intended users. Contains from two parts:

** **UAT Testing**: User Acceptance Testing (UAT) is conducted once a year in collaboration with the customer representative. During this testing phase, sets of test cases are prepared, registry regulations are defined, and the registry itself is tested. The customer representative goes through each test case, indicating the success or failure of each scenario.
** **Alpha/Beta Testing**: Alpha/beta testing is performed with each release iteration by the registry development teams using the test cases they have prepared.

* **Regression testing**: Regression testing ensures that recent code changes or enhancements to a software application do not negatively impact its existing functionality. It involves re-running a predefined set of test cases to validate that new code changes have not introduced unintended side effects or defects.

* **Installation/Update testing**: Installation/Update testing aims to confirm that the software can be installed, updated, configured, and removed without any complications, errors, or adverse impacts on the target system. This type of testing is essential because even a well-developed software application can suffer from installation-related or update-related issues that might disrupt its functionality or cause conflicts with other software components.

* **Visual testing**: Visual testing is a critical aspect of quality assurance that focuses on evaluating the visual appearance and layout of a software application or website. It ensures that the user interface elements, such as fonts, colors, images, and graphical components, are displayed correctly and consistently across various devices, browsers, and resolutions. Visual testing utilizes automated tools to capture screenshots of the application's different states and then compares these images to a set of baseline images representing the expected appearance. This process helps identify any discrepancies, visual regressions, or layout issues, ensuring a visually appealing and consistent user experience.

=== Testing approaches description

==== Acceptance Testing

===== UAT Testing
This testing method involves verifying a build that is a potential candidate for further deployment to the production environment.
Act once per year with customer representative. It includes the following procedures:

* Coordination and creation of acceptance scenarios with client representatives
* Establishment of the necessary testing infrastructure
* Search or creation of required test data
* Direct execution of acceptance scenarios and agreement of their results with client representatives.

The tests performed during this phase require confirmation of successful completion - the presence of snapshots, logs, and detailed reproduction steps.

===== Alpha/Beta Testing

Alpha testing involves testing a pre-release version of the software within the development environment. It's usually done by internal testers, developers, or quality assurance teams. The goal is to identify bugs, assess system stability, and ensure basic functionalities work as intended before external testing.

Beta testing occurs after alpha testing and involves releasing the software to a select group of external users or customers. The aim is to gather real-world feedback, uncover usability issues, and identify any remaining bugs or performance problems. This testing phase helps refine the software based on user input before its official release, ensuring a more polished and user-friendly product.

For alpha/beta testing are responsible registry development teams and they act it during release process.

==== Integration Testing

This testing method follows the following approach:

* Designing testing scenarios for the listed integrations and preparing test data.
* Developing an automated solution for testing integration data and forming test groups if such a solution can be built.
* Manual tests that form the regression suite should be executed regularly and updated, and they should be added to the appropriate test groups in git repository that defined by Gerkin language.

Such tests involve testing integrations with real instances of external test systems and require confirmation of successful execution - the presence of snapshots, logs, and detailed reproduction steps.

Artifacts resulting from this type of testing:

* Automated tests added to relevant test groups (nightly runs, integration, etc.)
* Manual tests added to relevant test groups (regression, integration, etc.).
* Updated requirement coverage matrices for tests and automated tests.
* Results of test runs should be well-structured and accessible to all stakeholders:
** Reports of automated test runs on Jenkins.
** Reports of manual test runs in git repository described on Gerkin.
* Formulated evidence of test execution - snapshots, attachments, and logs.

==== Regression Testing

This testing method follows the following approach:

* Develope an automated solution for test goal management.
* Automated solutions are designed based on their access levels:
** Backend - This level involves direct access to contracts and their interactions during testing.
**  UI - This level involves building automated solutions for testing platform UI functionality.
* Automated testing encompasses the following methods:
** Functional testing
** Installation testing
** Integration testing
* Developed automated tests are added to corresponding test groups (nightly runs, quality gate, and coverage zone).
* Developed automated tests reference the requirement they verify.
* The number of tests should be evenly distributed across testing levels, forming a balanced testing pyramid.
* Several levels of quality gates are integrated into the CI/CD process.
* Test data comprises synthetic data resembling industrial data or a sample of real industrial data (if accessible).
* To ensure the stability of the automated solution, virtualization tools are utilized.

Artifacts of this testing include:

* Documented design of the automated solution.
* Developed code conventions and guidelines for automated test developers.
* Established principles and rules for conducting code reviews.
* Description of quality gate levels and test categories.

==== Installation/Update Testing

This functionality involves only manual testing, which is added to the regression test suite. As testing requires a separate testing environment and is resource-intensive, it will be executed as needed and agreed upon with the infrastructure team.

Artifacts of this testing include:

* Manual tests added to relevant test groups (Regression, Integration, etc.).
* Results of test runs should be well-structured and accessible to all stakeholders.

==== Visual Testing
This testing method follows the following approach:

* Develope an automated solution for test goal management
* Automated solution capture screenshots of the application under different scenarios and compare them to baseline images, highlighting any discrepancies
* Visual testing checks how the UI adapts to different screen sizes to ensure a seamless experience on devices
* It verifies that the visual representation matches the expected design and layout, ensuring that no visual regressions or inconsistencies are introduced

Artifacts of this testing include:

* Reports highlight differences between captured screenshots and baseline images, making it easy to identify visual defects or regressions

==== Unit Testing
This testing method follows the following approach:

* Unit tests isolate a specific unit of code, such as a function or method, from the rest of the application. This isolation ensures that the test results are not affected by external dependencies
* Each unit test is independent and should not rely on the execution order of other tests. This allows for parallel execution and accurate identification of failures
* Unit tests detect defects and issues early in the development process, reducing the cost and effort of fixing bugs in later stages

Artifacts of this testing include:

* Passed code review pipeline that validates by sonar 80% coverage
* Generated code coverage reports

=== Quality Gates



=== Tools and Technologies Used

The following types, tools and technologies are used during functional testing:

[options="header"]
|===
| Functional testing type | Toolset | Status
| Unit testing
| JUnit, Mokito
| Automated

| Integration Testing
| JUnit, AssertJ, RestAssured
| Automated

| System testing
| JUnit, Selenide, RestAssured
| Automated

| Visual testing
| Selenide, Moon
| Automated

| Acceptance Testing
| JUnit, Selenide
| Automated/Manual

| Regression Testing
| JUnit, Selenide, Moon, RestAssured
| Automated
|===

== Defect managing
=== Registration and Processing of Defects

Newly discovered defects are divided into three types based on their causes:

- **Production Defects**

- **Regression Defects**

- **Functional Defects**

==== Production Defects
These are defects that are identified in the live or production environment after the software has been deployed. Production defects can impact end-users, disrupt business processes, and require immediate attention to minimize negative effects.

Defects obtained from the production environment should be linked to the defect handling epic related to the production environment. They should have labels like JSM, competencies (DevOps, Backend, Frontend), and include a reference to the user-reported defect description. For defects reported by users, a link to the corresponding Jira defect should be provided.

==== Regression Defects
Regression defects occur when a new code change or feature introduction inadvertently causes a previously working functionality to fail. This can happen due to code changes affecting interconnected parts of the software.

Defects found during regression testing or while testing other tasks should be logged within the scope of a regression epic.

==== Functional Defects
Functional defects arise when a software component does not perform its intended function correctly during it's development process. This can include incorrect calculations, inaccurate data processing, or failure to execute specific actions as expected.

Defects identified during the testing of new functionality should be associated with the user story within which they were discovered.

=== Defects processing process
The defect processing is as follows:

* All defects are prioritized according to the conditions in section "Defect Priorities" and reviewed following section "Defect Importance Determination Process"
* When a defect is resolved, it is marked as "Ready for QA" and forwarded to the defect registrar. If the defect registrar is a representative of the client, it is forwarded to the testing team leader.
* The defect registrar reviews the defect and, if resolved, marks it as closed on valid quality gate. If the defect still reproduces, it is returned to development with a "Rework" status

=== Defect Priorities

To determine the severity of defects and their impact on further development, the following criteria (not listed in the table) should be considered:

[options="header"]
|===
| Priority Level | Description | Impact on Testing
| 0 (Blocker)
| Platform stops functioning, and there is no workaround.
| Testing team sends the build back to development.

| 1 (Critical)
| Functionality is not working.
| Testing team provides a test report for development and management team. Management team decides about flow (rework, hotfix).

| 2 (Major)
| Critical business requirements are broken.
| Presence of priority 2 defects requires additional agreement with the business team and project management.

| 3 (Minor)
| Functionality is not working according to design, but an acceptable workaround exists.
| Business and development teams agree on the necessity of defect resolution within the current release.

| 4 (Trivial)
| Minor changes needed in functionality - aesthetic or cosmetic changes.
| Business and development teams agree on the necessity of defect resolution within the current release.
|===

=== Defect Importance Determination Process

During the stages of development, regression/stabilization, the development team conducts internal and external sessions to review the list of defects, in order to determine their current priorities and statuses. A defect should be refined by indicating clarifying statuses (provided in the table) and providing a detailed comment.

The responsible individuals for closing defects are the testing team leader and the defect registrar.

[options="header"]
|===
| Status | Explanation | Will Be Resolved?

| Not a bug: Cannot reproduce
| Defect that cannot be reproduced at the moment
| No

| Not a bug: Duplicate
| Defect is already registered
| No

| Done
| Testing completed fully and functionality is working
| Yes

| Rework
| Testing completed fully and functionality isn't working after fix
| No

| Won't Do
| Defect has minimal impact on business and won't be resolved
| No

| Fixed
| Testing conducted comprehensively after changes were made
| Yes

| Obsolete
| Defect is outdated
| No

| Cancelled
| Cancelled functionality
| No

| Implemented
| Technical error that doesn't require testing
| Yes

| Deferred
| Awaiting resolution in upcoming releases and planned functionality
| Yes

| Not a Bug
| Is not a defect
| No
|===

== Reporting
Testing reports contains from two types:

* ReportPortal automation report that can be provided to stakeholders
* Manual report after manual suite execution that can be provided to stakeholders