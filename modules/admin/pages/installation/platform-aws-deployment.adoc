:toc-title: ЗМІСТ
:toc: auto
:toclevels: 5
:experimental:
:important-caption:     ВАЖЛИВО
:note-caption:          ПРИМІТКА
:tip-caption:           ПІДКАЗКА
:warning-caption:       ПОПЕРЕДЖЕННЯ
:caution-caption:       УВАГА
:example-caption:           Приклад
:figure-caption:            Зображення
:table-caption:             Таблиця
:appendix-caption:          Додаток
:sectnums:
:sectnumlevels: 5
:sectanchors:
:sectlinks:
:partnums:

= Розгортання платформи з нуля в AWS-середовищі

CAUTION: Сторінка у процесі розробки

Ця інструкція надає детальну інформацію про розгортання платформи з нуля в AWS-середовищі, починаючи зі створення облікового запису AWS і закінчуючи інсталяцією платформи з усіма додатковими налаштуваннями.

== Передумови

Перед розгортанням і налаштуванням платформи потрібно обов'язково виконати наступні дії.

=== Необхідні елементи початкового етапу

Перед початком будь-яких дій потрібно мати в наявності набір ресурсів, які обов'язкові для подальших кроків:

* документ "What`s new";
* документ "Release notes";
* документ із незворотніми змінами;
* документ "Special steps" із додатковими кроками для обраної версії релізу Платформи. Він потрібен тільки для процедури оновлення Платформи;
* DSO (digital-signature-ops) сертифікати:
** Key-6.dat - приватний ключ організації;
** allowed-key.yaml - перелік всіх виданих ключів. Спочатку це тільки первинний Key-6.dat. При зміні ключа, туди додається інформація про новий ключ, не видаляючи старий;
** CAs.json - перелік всіх АЦСК, береться з сайту https://iit.com.ua/downloads[ІІТ];
** CACertificates.p7b - публічний ключ, береться з сайту https://iit.com.ua/downloads[ІІТ]; +
Файли конфігурації для Гряди:
** osplm.ini;
** sign.key.device-type;
** sign.key.file.issuer;
** sign.key.file.password;
** sign.key.hardware.device;
** sign.key.hardware.password;
** sign.key.hardware.type;
* docker образ openshift-install контейнера;
* завантажений Інсталер.

=== Створення облікового запису AWS

Перед встановленням OpenShift Container Platform на Amazon Web Services (AWS) необхідно створити обліковий запис AWS.

Це можна зробити користуючись офіційною документацією на сайті AWS: https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/[How do I create and activate a new AWS account?]

=== Налаштування облікового запису AWS

Перш ніж встановити OpenShift Container Platform, потрібно налаштувати обліковий запис Amazon Web Services (AWS).

==== Налаштування Route 53

Щоб встановити OpenShift Container Platform, потрібно зареєструвати домен. Це можна зробити в сервісі Route 53 або ж використати будь-який інший реєстратор доменних імен.

Також обліковий запис Amazon Web Services (AWS), який використовується, повинен мати виділену публічну зону хостингу в сервісі Route 53.

TIP: Докладніше описано в офіційній документації на сайті OKD: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-route53_installing-aws-account[Configuring Route 53].

==== Налаштування зовнішнього домену

Якщо ж для створення домену було використано не AWS Route 53, а зовнішній реєстратор доменних імен, то потрібно виконати делегування домену. Для цього потрібно виконати наступні дії:

* перейти в створенний обліковий запис AWS та створити публічну зону хостингу в сервісі Route 53 (як було описано в п. 1.2.1). Назвати її необхідно так само, як і зовнішній створенний домен;
* зайти в створену публічну зону хостингу та переглянути запис із типом NS (Name Servers - це сервери імен, які відповідають на DNS-запити для домену). В значенні будуть вказані сервери імен. Необхідно зберегти назви цих серверів для подальшого використання в наступних кроках;
* перейти в зовнішній реєстратор доменних імен, в якому було створено домен;
* відкрити налаштування цього домену та знайти налаштуваня стосовно NS серверів;
* відредагувати NS сервери відповідно до NS серверів, які були взяті із публічної зони хостингу з облікового запису AWS.

==== Ліміти облікового запису AWS

Кластер OpenShift Container Platform використовує ряд компонентів Amazon Web Services (AWS), і стандартні Обмеження послуг впливають на можливість встановлення кластеру.

Перелік компонентів AWS, обмеження яких можуть вплинути на можливість встановлення та запуску кластеру OpenShift Container Platform наведені у наступній документації на сайті OKD: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-limits_installing-aws-account[AWS account limits].

Також обов'язково потрібно збільшити обмеження CPU для on-demand віртуальних машин в обліковому записі Amazon Web Services (AWS). Необхідіні для цього дії описані в офіціальній документації на сайті AWS: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-on-demand-instance-vcpu-increase/[How do I request an EC2 vCPU limit increase for my On-Demand Instance?]

==== Створення користувача IAM

. Перед встановленням OpenShift Container Platform створіть користувача IAM, користуючись офіційною документацією на сайті AWS: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html[Creating an IAM user in your AWS account].

. Окрім цього потрібно виконати наступні важливі вимоги.

* Потрібно видалити будь-які обмеження Service control policies (SCPs) з облікового запису AWS.
+
Під час створення кластеру, також створюється асоційований постачальник ідентичностей AWS OpenID Connect (OIDC). Ця конфігурація постачальника OIDC базується на відкритому ключі, який знаходиться в регіоні AWS us-east-1. Клієнти з AWS SCP повинні дозволити використання регіону AWS us-east-1, навіть якщо кластер буде розгорнуто в іншому регіоні. Без правильного налаштування цих політик, одразу можуть виникнути помилки з дозволами, оскільки інсталятор OKD перевіряє корректність їх налаштування.
+
TIP: Детальну інформацію можна отримати в офіційний документації, у пункті *1.1. DEPLOYMENT PREREQUISITES* документа https://access.redhat.com/documentation/en-us/red_hat_openshift_service_on_aws/4/pdf/prepare_your_environment/red_hat_openshift_service_on_aws-4-prepare_your_environment-en-us.pdf[Red Hat OpenShift Service on AWS 4. Prepare your environment].

* Потрібно правильно налаштувати *_permissions boundary_* у створеного IAM-користувача.
+
Нижче наведено приклад політики permissions boundary. Можна використати її, або зовсім видалити будь-які permissions boundary.
+
[%collapsible]
._Приклад. Налаштування політики *permissions boundary_*
====
[source,json]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "NotAction": [
                "iam:*"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:Get*",
                "iam:List*",
                "iam:Tag*",
                "iam:Untag*",
                "iam:GenerateServiceLastAccessedDetails",
                "iam:GenerateCredentialReport",
                "iam:SimulateCustomPolicy",
                "iam:SimulatePrincipalPolicy",
                "iam:UploadSSHPublicKey",
                "iam:UpdateServerCertificate",
                "iam:CreateInstanceProfile",
                "iam:CreatePolicy",
                "iam:DeletePolicy",
                "iam:CreatePolicyVersion",
                "iam:DeletePolicyVersion",
                "iam:SetDefaultPolicyVersion",
                "iam:CreateServiceLinkedRole",
                "iam:DeleteServiceLinkedRole",
                "iam:CreateInstanceProfile",
                "iam:AddRoleToInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:UpdateRole",
                "iam:UpdateRoleDescription",
                "iam:DeleteRole",
                "iam:PassRole",
                "iam:DetachRolePolicy",
                "iam:DeleteRolePolicy",
                "iam:UpdateAssumeRolePolicy",
                "iam:CreateGroup",
                "iam:UpdateGroup",
                "iam:AddUserToGroup",
                "iam:RemoveUserFromGroup",
                "iam:PutGroupPolicy",
                "iam:DetachGroupPolicy",
                "iam:DetachUserPolicy",
                "iam:DeleteGroupPolicy",
                "iam:DeleteGroup",
                "iam:DeleteUserPolicy",
                "iam:AttachUserPolicy",
                "iam:AttachGroupPolicy",
                "iam:PutUserPolicy",
                "iam:DeleteUser",
                "iam:CreateRole",
                "iam:AttachRolePolicy",
                "iam:PutRolePermissionsBoundary",
                "iam:PutRolePolicy"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateAccessKey",
                "iam:DeleteAccessKey",
                "iam:UpdateAccessKey",
                "iam:CreateLoginProfile",
                "iam:DeleteLoginProfile",
                "iam:UpdateLoginProfile",
                "iam:ChangePassword",
                "iam:CreateVirtualMFADevice",
                "iam:EnableMFADevice",
                "iam:ResyncMFADevice",
                "iam:DeleteVirtualMFADevice",
                "iam:DeactivateMFADevice",
                "iam:CreateServiceSpecificCredential",
                "iam:UpdateServiceSpecificCredential",
                "iam:ResetServiceSpecificCredential",
                "iam:DeleteServiceSpecificCredential"
            ],
            "Resource": "*"
        }
    ]
}
----
====

TIP: Докладніше процес створення IAM-користувача описаний в офіційній документації на сайті OKD: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-iam-user_installing-aws-account[Creating an IAM user].

==== Необхідні дозволи AWS для користувача IAM

Для розгортання всіх компонентів кластера OpenShift Container Platform користувачеві IAM потрібні дозволи, які необхідно прикріпити до цього користувача. +
Приклад таких дозволів наведено у наступній документації на сайті OKD: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-account.html#installation-aws-permissions_installing-aws-account[Required AWS permissions for the IAM user].

=== Створення додаткових облікових записів

Перед встановленням OpenShift Container Platform на Amazon Web Services (AWS), необхідно створити обліковий запис Docker Hub та Red Hat. +
Це необхідно зробити для формування *`docker pull secret`*, який буде використовуватись пізніше.

==== Створення облікового запису Docker Hub

* Деякі сервіси використовують images, які знаходяться у репозиторіях на Docker Hub. Для того, щоб мати можливість їх використовувати, потрібно створити акаунт, користуючись офіційною документацією на сайті Docker: https://docs.docker.com/docker-id/[Docker ID accounts].

* Окрім цього, виникнуть проблеми із лімітом на кількість завантаженнь images на день. Це призведе до того, що сервіси не зможуть запуститися. Щоб цього уникнути, необхідно оновити підписку до рівня Pro. Це допоможе змінити обмеження на кількість пулів із 200/6 годин (200 image pulls per 6 hours) до 5000/день. Це можливо зробити користуючись офіційною документацією на сайті Docker: https://docs.docker.com/subscription/upgrade/[Upgrade your subscription].

==== Створення облікового запису Red Hat

Для того, щоб завантажити необхідні images для встановлення OpenShift Container Platform, необхідно створити Red Hat Account. Докладніше про те, як це зробити, описано в офіційній документації: https://access.redhat.com/articles/5832311[Red Hat Login ID and Account].

Це необхідно для того, щоб завантажити сформований pull secret пізніше (докладніше описано в п. 3). Він дозволить пройти автентифікацію та завантажити образи контейнерів для компонентів OpenShift Container Platform.

== Розгортання додаткових ресурсів для інсталяції OKD-кластера в AWS

Для вдалого встановлення кластера та платформи, потрібно підняти наступні ресурси в AWS. На малюнку нижче зображена схема інфраструктури із ними.

image:installation/aws/installation-aws-1.png[image,width=468,height=375]

Це можна зробити самостійно за рекомендаціями зазначеними нижче або використати підготовлений Terraform-код.

=== Опис Terraform-коду

Як приклад автоматизації процесу було реалізовано Terraform-код, який можна підлаштувати під свої параметри та використати для розгортання інфраструктури.

==== Початковий Terraform-код

Це Terraform-код, який створить ресурси для подальших кроків. До таких ресурсів відносяться:

* S3 Bucket -- сховище для зберігання файлів _*.tfstate_;
* DynamoDB Table -- таблиця, необхідна для блокування стану Terraform.

.Початковий код. Опис шаблонів Terraform
====
.main.tf
[%collapsible]
=====
[source,terraform]
----
data "aws_caller_identity" "current" {}

module "s3_bucket" {
  source  = "terraform-aws-modules/s3-bucket/aws"
  version = "3.6.0"

  bucket = "terraform-states-${data.aws_caller_identity.current.account_id}"
  acl    = "private"
  # S3 bucket-level Public Access Block configuration
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true

  versioning = {
    enabled = true
  }

  tags = merge(var.tags)
}

module "dynamodb_table" {
  source  = "terraform-aws-modules/dynamodb-table/aws"
  version = "3.1.2"

  name           = var.table_name
  billing_mode   = "PROVISIONED"
  read_capacity  = "1"
  write_capacity = "1"
  hash_key       = "LockID"

  attributes = [
    {
      name = "LockID"
      type = "S"
    }
  ]

  tags = merge(var.tags, tomap({ "Name" = var.table_name }))
}
----
=====


.providers.tf
[%collapsible]
=====
[source,terraform]
----
terraform {
  required_version = "= 1.3.7"
}

provider "aws" {
  region = var.region
}
----
=====

.terraform.tfvars
[%collapsible]
=====
[source,terraform]
----
region = "eu-central-1"
tags = {
  "SysName"    = "EPAM"
  "Department" = "MDTU-DDM"
  "user:tag"   = "mdtuddm1"
}
----
=====

.variables.tf
[%collapsible]
=====
[source,terraform]
----
variable "region" {
  description = "The AWS region to deploy the cluster into, e.g. eu-central-1"
  type        = string
}

variable "s3_states_bucket_name" {
  description = "Prefix for S3 bucket name. Since the name should be unique the account number will be added as suffix, e.g. terraform-states-<AWS_ACCOUNT_ID>"
  type        = string
  default     = "terraform-states"
}

variable "table_name" {
  description = "the name of DynamoDb table to store terraform tfstate lock"
  type        = string
  default     = "terraform_locks"
}

variable "tags" {
  description = "A map of tags to apply to all resources"
  type        = map(any)
}
----
=====
====

==== Основний Terraform-код

Основний Terraform-код, розгортає усі необхідні ресурси. Опис шаблонів наведено нижче.

.Основний код. Опис шаблонів Terraform
====

.main.tf
[%collapsible]
=====
[source,terraform]
----
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "3.19.0"

  name = var.platform_name

  cidr            = var.platform_cidr
  azs             = var.subnet_azs
  private_subnets = var.private_cidrs
  public_subnets  = var.public_cidrs

  enable_dns_hostnames   = true
  enable_dns_support     = true
  enable_nat_gateway     = true
  single_nat_gateway     = true
  one_nat_gateway_per_az = false

  tags = var.tags
}

module "ec2_instance" {
  source  = "terraform-aws-modules/ec2-instance/aws"
  version = "4.3.0"

  name = var.node_name

  ami                    = var.node_ami
  instance_type          = var.node_type
  key_name               = module.key_pair.key_pair_name
  vpc_security_group_ids = [aws_security_group.sg_private.id]
  subnet_id              = module.vpc.private_subnets[0]
  user_data              = templatefile("files/user_data.sh.tpl", { cross_account_role = var.cross_account_role_arn })
  iam_instance_profile   = aws_iam_instance_profile.node_profile.name
  enable_volume_tags     = false

  root_block_device = [
    {
      encrypted   = false
      volume_type = var.volume_type
      volume_size = var.volume_size
      tags        = var.tags
    },
  ]

  tags = var.tags
}

module "ec2_bastion" {
  source  = "terraform-aws-modules/ec2-instance/aws"
  version = "4.3.0"

  name = "bastion"

  ami                    = var.node_ami
  instance_type          = "t2.nano"
  key_name               = module.key_pair.key_pair_name
  vpc_security_group_ids = [aws_security_group.sg_public.id]
  subnet_id              = module.vpc.public_subnets[0]
  enable_volume_tags     = false

  tags = var.tags
}

module "key_pair" {
  source  = "terraform-aws-modules/key-pair/aws"
  version = "2.0.1"

  key_name   = var.key_pair
  public_key = trimspace(tls_private_key.main.public_key_openssh)
  tags = merge(var.tags, {
    "Name" = var.key_pair
  })
}
----
=====

.providers.tf
[%collapsible]
=====
[source,terraform]
----
terraform {
  required_version = "= 1.3.7"

  # Fill the gaps instead <...>
  backend "s3" {
    bucket         = "terraform-states-<ACCOUNT_ID>"
    key            = "node/eu-central-1/terraform/terraform.tfstate"
    region         = "eu-central-1"
    acl            = "bucket-owner-full-control"
    dynamodb_table = "terraform_locks"
    encrypt        = true
  }

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 4.51.0"
    }
  }
}

provider "aws" {
  region = var.region
}
----
=====

.iam-node-role.tf
[%collapsible]
=====
[source,terraform]
----
data "aws_iam_policy_document" "assume_role_policy" {
  statement {
    actions = ["sts:AssumeRole"]

    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }

  }
}

resource "aws_iam_role" "node_role" {
  name                  = var.role_name
  description           = "IAM role to assume to initial node"
  assume_role_policy    = data.aws_iam_policy_document.assume_role_policy.json
  force_detach_policies = true

  inline_policy {
    name = "CrossAccountPolicy"

    policy = jsonencode({
      Version = "2012-10-17"
      Statement = [
        {
          Action   = "sts:AssumeRole"
          Effect   = "Allow"
          Resource = var.cross_account_role_arn
        },
      ]
    })
  }
  tags = merge(var.tags, tomap({ "Name" = var.role_name }))
}

resource "aws_iam_instance_profile" "node_profile" {
  name = var.role_name
  role = aws_iam_role.node_role.name

  tags = var.tags
}
----
=====

.elastic-ip.tf
[%collapsible]
=====
[source,terraform]
----
resource "aws_eip" "bastion_ip" {
  instance = module.ec2_bastion.id

  tags = merge(var.tags, {
    "Name" = "bastion-ip"
  })
}
----
=====

.security-groups.tf
[%collapsible]
=====
[source,terraform]
----
resource "aws_security_group" "sg_public" {
  name   = "sg public for bastion"
  vpc_id = module.vpc.vpc_id
  ingress {
    from_port = var.ssh_port
    to_port   = var.ssh_port
    protocol  = "tcp"
    #    cidr_blocks = var.ingress_cidr_blocks
    prefix_list_ids = [var.prefix_list_ids]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  tags = merge(var.tags, {
    "Name" = "sg-public"
  })
}

resource "aws_security_group" "sg_private" {
  name   = "sg private for node"
  vpc_id = module.vpc.vpc_id
  ingress {
    from_port       = var.ssh_port
    to_port         = var.ssh_port
    protocol        = "tcp"
    security_groups = [aws_security_group.sg_public.id]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  tags = merge(var.tags, {
    "Name" = "sg-private"
  })
}
----
=====

.ssh-key.tf
[%collapsible]
=====
[source,terraform]
----
resource "tls_private_key" "main" {
  algorithm = "RSA"
}

resource "null_resource" "main" {
  provisioner "local-exec" {
    command = "echo \"${tls_private_key.main.private_key_pem}\" > private.key"
  }

  provisioner "local-exec" {
    command = "chmod 600 private.key"
  }
}
----
=====

.files/user_data.sh.tpl
[%collapsible]
=====
[source,sh]
----
#!/bin/bash
export VERSION_STRING=5:20.10.23~3-0~ubuntu-bionic

# Install docker
sudo apt-get update -y
sudo apt-get install \
    ca-certificates \
    curl \
    gnupg \
    lsb-release -y
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update -y
sudo apt-get install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-compose-plugin -y
sudo usermod -aG docker ubuntu

# Install unzip
sudo apt install unzip -y

# Install aws-cli-v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# Configure config for cross account integration
mkdir -p /home/ubuntu/.aws
touch /home/ubuntu/.aws/config
cat <<EOT >> /home/ubuntu/.aws/config
[profile cross-account-role]
role_arn = ${cross_account_role}
credential_source = Ec2InstanceMetadata
EOT
----
=====

.terraform.tfvars
[%collapsible]
=====
[source,terraform]
----
# Check out all the inputs based on the comments below and fill the gaps instead <...>
# More details on each variable can be found in the variables.tf file

region        = "eu-central-1"
platform_name = "okd-4-11" # the name of the cluster and AWS resources
platform_cidr = "10.0.0.0/16"
# The following will be created or used existing depending on the create_vpc value
subnet_azs    = ["eu-central-1a", "eu-central-1b", "eu-central-1c"]
private_cidrs = ["10.0.1.0/24"]
public_cidrs  = ["10.0.101.0/24"]

ssh_port = 22

# Uncomment this line to use a custom IP address for the SSH connection
#ingress_cidr_blocks = ["<CUSTOM_IP>"]

# Using prefix-list from epam-east-eu
prefix_list_ids = "pl-0ede2509a36215538"

node_name = "initial-node"
node_ami  = "ami-0e0102e3ff768559b"
node_type = "t2.medium"
key_pair  = "node_key"

volume_type = "gp3"
volume_size = 150

role_name              = "CustomEC2Role"
cross_account_role_arn = "arn:aws:iam::764324427262:role/CustomCrossAccountRole"

tags = {
  "SysName"    = "EPAM"
  "Department" = "MDTU-DDM"
  "user:tag"   = "mdtuddm1"
}
----
=====

.variables.tf
[%collapsible]
=====
[source,terraform]
----
variable "region" {
  description = "The AWS region to deploy the cluster into, e.g. eu-central-1"
  type        = string
}

variable "platform_name" {
  description = "The name of the node that is used for tagging resources. Match the [a-z0-9_-]"
  type        = string
}

variable "platform_cidr" {
  description = "CIDR of your future VPC"
  type        = string
}

variable "subnet_azs" {
  description = "Available zones of your future or existing subnets"
  type        = list(any)
  default     = []
}

variable "private_cidrs" {
  description = "CIDR of your future VPC"
  type        = list(any)
  default     = []
}

variable "public_cidrs" {
  description = "CIDR of your future VPC"
  type        = list(any)
  default     = []
}

variable "node_name" {
  description = "The name of the node that is used for tagging resources. Match the [a-z0-9_-]"
  type        = string
}

variable "node_ami" {
  description = "The ami of the node"
  type        = string
}

variable "node_type" {
  description = "Type of the node"
  type        = string
}

variable "key_pair" {
  description = "The name of DynamoDb table to store terraform tfstate lock"
  type        = string
}

variable "volume_type" {
  description = "Root volume type of the node"
  type        = string
}

variable "volume_size" {
  description = "Root volume size of the node"
  type        = number
}

variable "ssh_port" {
  description = "Open the 22 port"
  type        = number
}

#Use this for a custom IP address for the SSH connection
#variable "ingress_cidr_blocks" {
#  description = "IP CIDR blocks for bastion"
#  type        = list(string)
#}

variable "prefix_list_ids" {
  description = "IP CIDR blocks for bastion"
  type        = string
}

variable "role_name" {
  description = "The AWS IAM role name for initial node"
  type        = string
}

variable "cross_account_role_arn" {
  description = "The AWS IAM role arn to assume from another AWS account"
  type        = string
}


variable "tags" {
  description = "A map of tags to apply to all resources"
  type        = map(any)
}
----
=====

====

[NOTE]
====
IP адреса ::
Для підключення через SSH до додаткової віртуальної машини потрібно додати в файл terraform.tfvars необхідну IP адресу. Якщо потрібно відкрити для підключення декілька адрес, то потрібно створити префікс ліст та використовувати його.
====

//TODO: Add link to chapter 2.6
WARNING: Якщо для підняття додаткових компонентів використано Terraform-код, то перейдіть одразу до пункту 2.6.

=== Рекомендовані налаштування бастіону

У таблиці нижче наведено рекомендовані налаштування для бастіону.

.Налаштування бастіону
[width="100%",cols="6%,33%,61%",options="header",]
|===

|*№* |*Опція налаштування* |*Значення*

|1 |Instance type |t2.nano
|2 |vCPUs |1
|3 |RAM |0.5 GiB
|4 |CPU Credits/hr |3
|5 |Platform |Ubuntu
|6 |AMI name |ubuntu-bionic-18.04-amd64-server-20210224
|7 |Volume |8 Gb

|===

=== Рекомендовані налаштування додаткової віртуальної машини

У таблиці нижче наведено рекомендовані налаштування для додаткової віртуальної машини.

[width="100%",cols="6%,33%,61%",options="header",]
|===

|*№* |*Опція налаштування* |*Значення*
|1 |Instance type |t2.medium
|2 |vCPUs |2
|3 |RAM |4 GiB
|4 |CPU Credits/hr |24
|5 |Platform |Ubuntu
|6 |AMI name |ubuntu-bionic-18.04-amd64-server-20210224
|7 |Volume |150 Gb

|===

=== Налаштування AWS cross account

Щоб встановити кластер та Платформу, необхідно завантажити на додаткову віртуальну машину _Docker-образ для контейнера_ та _Інсталер_. Це можливо лише за умови, що створена спеціальна IAM-роль.

Потрібно перейти до AWS IAM-сервісу та створити роль для EC2-сервісу із наступними дозволами:

.*_Trusted entities_*
[%collapsible]
====
[source,json]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "",
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
----
====

.*_Inline permissions policies_*
[%collapsible]
====
[source,json]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": "sts:AssumeRole",
            "Effect": "Allow",
            "Resource": "arn:aws:iam::764324427262:role/CustomCrossAccountRole"
        }
    ]
}
----
====

Після цього необхідно приєднати створену IAM роль до додаткової віртуальної машини.

TIP: Докладніше про створення IAM-ролі та приєднання її до віртуальної машини описано в офіційній документації на сайті AWS: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html[IAM roles for Amazon EC2].

=== Додаткові налаштування віртуальної машини

==== Підключення до додаткової віртуальної машини

Щоб під'єднатися з локального комп'ютера до додаткової віртуальної машини, потрібно створити SSH-тунель. Це потрібно зробити наступною командою:

.Створення SSH-тунелю
====
----
$ ssh -i <SSH_KEY> -L 1256:<NODE_PRIVATE_IP>:22 -N -f ubuntu@<BASTION_PUBLIC_IP>
----
====

Після створення SSH-тунелю, можна підключатися до додаткової віртуальної машини. Це потрібно зробити наступною командою:

.Підключення через SSH
====
----
$ ssh -i <SSH_KEY> ubuntu@localhost -p 1256
----
====

[IMPORTANT]
====
Мета додаткової віртуальної машини ::

З додаткової віртуальної машини потрібно виконувати усі подальші кроки, а саме інсталяцію кластера та встановлення платформи.
====

==== Встановлення необхідних інструментів

Для подальших дій потрібно встановити необхідні інструменти на додаткову віртуальну машину.

* unzip
* https://docs.docker.com/engine/install/[docker]
* https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[aws cli v2]

Перевірити правильність встановлення інструментів можна за допомогою наступних команд:

.Перевірка встановлення інструментів
====

.Перевірка unzip
----
$ unzip -v
----

.Перевірка docker
----
$ docker --version
----

.Перевірка aws cli
----
$ aws --version
----

====

==== Використання профілю для AWS cross account

Необхідно виконати наступні кроки, щоб авторизуватися під роллю, яка має доступ до Docker образу для контейнера та Інсталера.

. Авторизуватися на машині від IAM-користувача.
+
----
$ export AWS_ACCESS_KEY_ID=<КЛЮЧ_ДОСТУПУ>
$ export AWS_SECRET_ACCESS_KEY=<СЕКРЕТНИЙ_КЛЮЧ_ДОСТУПУ>
----

. Створити директорію *_.aws_* та файл *_config_* усередині:
+
----
$ mkdir -p ~/.aws
$ touch ~/.aws/config
----

. Додати до файлу *_config_* необхідні значення для ролі.
+
----
$ cat <<EOT >> ~/.aws/config
[profile cross-account-role]
role_arn = arn:aws:iam::764324427262:role/CustomCrossAccountRole
credential_source = Ec2InstanceMetadata
EOT
----

=== Запуск контейнера openshift-install

Щоб використовувати docker image контейнера *`openshift-install`* для встановлення кластера, потрібно виконати кроки, подані нижче.

. Авторизуйтеся в AWS ECR.
+
[source,bash]
----
$ sudo aws ecr get-login-password --profile cross-account-role --region eu-central-1 | docker login --username AWS --password-stdin 764324427262.dkr.ecr.eu-central-1.amazonaws.com
----

. Завантажте docker-образ (docker image).
+
[source,bash]
----
$ docker pull 764324427262.dkr.ecr.eu-central-1.amazonaws.com/openshift-install:v3
----

. Додайте тег до завантаженого docker-образу.
+
[source,bash]
----
$ docker tag 764324427262.dkr.ecr.eu-central-1.amazonaws.com/openshift-install:v3 openshift-install:v3
----

. Створіть нову директорію, в якій зберігатимуться усі дані кластера:
+
[source,bash]
----
$ mkdir ~/openshift-cluster
----

. Перейдіть до створеної директорії.
+
[source,bash]
----
$ cd ~/openshift-cluster
----

. Запустіть контейнер *`openshift-install`*.
+
[source,bash]
----
$ sudo docker run --rm -it --name openshift-install-v3 \
    --user root:$(id -g) \
    --net host \
    -v $(pwd):/tmp/openshift-cluster \
    --env AWS_ACCESS_KEY_ID=<КЛЮЧ_ДОСТУПУ> \
    --env AWS_SECRET_ACCESS_KEY=<СЕКРЕТНИЙ_КЛЮЧ_ДОСТУПУ> \
    openshift-install:v3 bash
----

== Підготовка до встановлення OKD-кластера в AWS

У версії `4.11` OpenShift Container Platform можливо встановити кастомізований кластер на інфраструктуру, яка передбачена програмою встановлення на Amazon Web Services (AWS).

[NOTE]
====
Версія OKD ::

Рекомендована версія OKD -- *`4.11.0-0.okd-2022-08-20-022919`*.
====

Для того, щоб встановити кластер потрібно виконати наступні кроки:

. Знаходячись у контейнері, перейдіть до директорії *_/tmp/openshift-cluster_*.
+
[source,bash]
----
$ cd /tmp/openshift-cluster
----

. Виконайте дії, які описані в офіційній документації на сайті OKD, до кроку *Deploying the cluster*: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-customizations.html[Installing a cluster on AWS with customizations].
+
[CAUTION]
Щоб налаштувати встановлення, потрібно створити файл *_install-config.yaml_* і внести до нього необхідні параметри перед тим, як встановити кластер.
+
Після створення файлу потрібно заповнити необхідні параметри, які будуть представлені в контекстному меню. Створений конфігураційний файл включає тільки необхідні параметри для мінімального розгортання кластера. Для кастомізації налаштувань можна звернутись до офіційної документації.
+
Рекомендовані параметри для файлу *_install-config.yaml_*: ::
+
[%collapsible]
.*_install-config.yaml_*
====
[source,yaml]
----
apiVersion: v1
baseDomain: <BASE_DOMAIN>(1)
compute:
  - architecture: amd64
    hyperthreading: Enabled
    name: worker
    platform:
      aws:
        zones:
          - eu-central-1c
        rootVolume:
          size: 80
          type: gp3
        type: r5.2xlarge
    replicas: 3
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  platform:
    aws:
      zones:
        - eu-central-1c
      rootVolume:
        size: 80
        type: gp3
      type: r5.2xlarge
  replicas: 3
metadata:
  name: <CLUSTER_NAME>
networking:
  clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
  machineNetwork:
    - cidr: 10.0.0.0/16
  networkType: OpenShiftSDN
platform:
  aws:
    region: eu-central-1
    userTags:
      'user:tag': <CLUSTER_NAME>(2)
publish: External
pullSecret: <PULL_SECRET>(4)
sshKey: <SSHKEY>(3)
----

* (1) `<BASE_DOMAIN`> -- домен, який було створено та налаштовано в п. 1.2.1 та п. 1.2.2.

* (2) `<CLUSTER_NAME>` -- ім'я майбутнього OKD-кластера.

* (3) `<SSHKEY>` - ключ або ключі SSH для автентифікації доступу до машин кластера. Можна використати той самий ключ, що був створений під час встановлення OKD-кластера, або будь-який інший.
+
TIP: Докладніше описано в офіційній документації на сайті OKD: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-customizations.html#installation-configuration-parameters-optional_installing-aws-customizations[Optional configuration parameters].

* (4) <PULL_SECRET> - секрет, який було створено в п. 1.3. Потрібно отримати цей секрет із Red Hat OpenShift Cluster Manager.
+
TIP: Докладніше про це описано в п. 5 офіційної документації на сайті OKD: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-aws-customizations.html#installation-obtaining-installer_installing-aws-customizations[Obtaining the installation program].
+
До отриманого секрету також потрібно додати секрет для під'єднання до облікового запису Red Hat, а також секрет від акаунта Docker Hub. Об'єднаний секрет буде виглядати наступним чином:
+
._Приклад об'єднаного секрету (*pull secret*)_
[%collapsible]
=====
[source,json]
----
{
   "auths":{
      "cloud.openshift.com":{
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "quay.io":{
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "registry.connect.redhat.com":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "registry.redhat.io":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      },
      "index.docker.io/v2/":{
         "username":"test",
         "password":"test",
         "auth":"b3Blb=",
         "email":"test@example.com"
      }
   }
}
----
=====
+
Для зручності запису цього секрету в файл *_install-config.yaml_* потрібно записати його в один рядок. Фінальний секрет буде виглядати наступним чином:
+
._Приклад *pull secret* в один рядок_
[%collapsible]
=====
[source,json]
----
'{"auths":{"cloud.openshift.com":{"auth":"b3Blb=","email":"test@example.com"},"quay.io":{"auth":"b3Blb=","email":"test@example.com"},"registry.connect.redhat.com":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"},"registry.redhat.io":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"},"index.docker.io/v2/":{"username":"test","password":"test","auth":"b3Blb=","email":"test@example.com"}}}'
----
=====

====
+
WARNING: Після запуску процесу розгортання кластера, Інсталер видаляє *install-config.yam*, тому рекомендовано виконати резервування цього файлу, якщо є потреба розгортання кількох кластерів.

== Запуск OKD4-інсталера та розгортання порожнього кластера OKD4

Після створення файлу *_install-config.yaml_*, для розгортання OKD-кластера виконайте наступну команду:

.*Встановлення OKD-кластера*
[source,bash]
----
$ ./openshift-install create cluster --dir /tmp/openshift-cluster/cluster-state --log-level=info
----

NOTE: Процес розгортання кластера зазвичай займає до 1 години часу.

При успішному розгортанні, в результаті виконання команди будуть представлені наступні параметри доступу до кластера:

* логін;
* пароль;
* посилання до вебконсолі кластера.

image:installation/aws/installation-aws-2.png[image,width=468,height=198]

У директорії, де виконувалася команда, буде створено ряд файлів, що зберігають статус кластера, необхідний для його деінсталяції.

TIP: Докладніше про це описано в офіційній документації на сайті OKD, у секції *Prerequisites*: https://docs.openshift.com/container-platform/4.11/installing/installing_aws/uninstalling-cluster-aws.html#installation-uninstall-clouds_uninstall-cluster-aws[Uninstalling a cluster on AWS].

Також в цій директорії з’явиться папка *_/auth_*, в якій буде збережено два файли для автентифікації: для роботи із кластером через *вебконсоль* та *інтерфейс командного рядка* OKD (OKD CLI).

== Заміна самопідписаних сертифікатів на довірені сертифікати

Для заміни самопідписаних (self-signed) сертифікатів на довірені (trusted), необхідно спочатку отримати ці сертифікати.

//TODO: HERE

У цьому пункті розглянуто отримання безплатних сертифікатів https://letsencrypt.org/[Let’s Encrypt] та їх встановлення на сервер.

Отримання сертифікатів Let’s Encrypt здійснено за допомогою утиліти https://github.com/acmesh-official/acme.sh[acme.sh].

TIP: Для отримання деталей використання Let’s Encrypt на базі ACME-протоколу, зверніться до https://letsencrypt.org/docs/client-options/[офіційного джерела].

Для заміни сертифікатів потрібно виконати наступні дії: ::
+
. Задайте змінну середовища. Змінна повинна вказувати на файл *_kubeconfig_*.
+
[source,bash]
----
$ export KUBECONFIG=cluster-state/auth/kubeconfig
----

. Створіть файл *_letsencrypt.sh_* та вставте у нього скрипт, який наведено нижче:
+
._Скрипт для заміни сертифікатів_
[%collapsible]
====
[source,bash]
----
#!/bin/bash
yum install -y openssl
mkdir -p certificates
export CERT_HOME=./certificates
export CURDIR=$(pwd)
cd $CERT_HOME

# Клонування утиліти acme.sh із репозиторію GitHub
git clone https://github.com/neilpang/acme.sh
sed -i "2i AWS_ACCESS_KEY_ID=\"${AWS_ACCESS_KEY_ID}\"" ./acme.sh/dnsapi/dns_aws.sh
sed -i "3i AWS_SECRET_ACCESS_KEY=\"${AWS_SECRET_ACCESS_KEY}\"" ./acme.sh/dnsapi/dns_aws.sh
cd $CURDIR
# Отримання API Endpoint URL
export LE_API="$(oc whoami --show-server | cut -f 2 -d ':' | cut -f 3 -d '/' | sed 's/-api././')"
#  Отримання Wildcard Domain
export LE_WILDCARD="$(oc get ingresscontroller default -n openshift-ingress-operator -o jsonpath='{.status.domain}')"
${CERT_HOME}/acme.sh/acme.sh --register-account -m user_${RANDOM}@example.com
${CERT_HOME}/acme.sh/acme.sh --issue -d ${LE_API} -d *.${LE_WILDCARD} --dns dns_aws
export CERTDIR=$CERT_HOME/certificates
mkdir -p ${CERTDIR}

# Перенесення сертифікатів із шляху acme.sh за замовчуванням (default path) до більш зручної директорії, за допомогою --install-cert - ключа
${CERT_HOME}/acme.sh/acme.sh --install-cert -d ${LE_API} -d *.${LE_WILDCARD} --cert-file ${CERTDIR}/cert.pem --key-file ${CERTDIR}/key.pem --fullchain-file ${CERTDIR}/fullchain.pem --ca-file ${CERTDIR}/ca.cer
# Cтворення секрету
oc create secret tls router-certs --cert=${CERTDIR}/fullchain.pem --key=${CERTDIR}/key.pem -n openshift-ingress
# Оновлення Custom Resource для Router
oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec": { "defaultCertificate": { "name": "router-certs" }}}'
----
====

. Зробіть цей скрипт таким, що можливо виконати.
+
[source,bash]
----
$ chmod +x ./letsencrypt.sh
----

. Виконайте цей скрипт.
+
[source,bash]
----
$ bash -x ./letsencrypt.sh
----

. Вийдіть із контейнера після виконання скрипту. Це можна зробити за допомогою команди, яка знаходиться нижче. Контейнер видалиться автоматично.
+
.Вихід із контейнера
----
$ exit
----

////

== 6. Підготовка та запуск Інсталера для розгортання Платформи в OKD-кластері

Для запуску Інсталера необхідно виконати ряд умов з підготовки робочої станції, з якої запускатиметься Інсталер.

//TODO: HERE

[width="100%",cols="16%,42%,42%",options="header",]
|===
|*Крок* |*AWS* |
| |*Розгортання з 0* |*Оновлення*
a|
=== 6.1. *Передумови*

a|
Перед запуском скрипта з інсталювання Платформи необхідно виконати наступні кроки:

*1. Завантажити Інсталер відповідної версії*

$ mkdir ~/installer

$ cd ~/installer

$ sudo aws s3 cp --profile cross-account-role s3://mdtu-ddm-platform-installer/<VERSION>/mdtu-ddm-platform-<VERSION>.zip mdtu-ddm-platform-<VERSION>.zip

*2. Розпакувати Інсталер в окрему директорію*

$ unzip mdtu-ddm-platform-(version).zip -d ./installer-<VERSION>

*3. Перенести kubeconfig від встановленного кластеру*

$ cp ~/openshift-cluster/cluster-state/auth/kubeconfig ./installer-<VERSION>

*4. Перенести сертифікати та допоміжні файли сервісу digital-signature-ops в директорію certificates та увійти до директорії з Інсталером*

$ cp -r /path/to/folder/certificates/ ./installer-<VERSION>

$ cd installer-<VERSION>

|
a|
=== 6.2. *Налаштування для Minio*

|Під час запуску Інсталера та розгортання Платформи з нуля ніякі додаткові налаштування для Minio не потрібні. a|
*1. Перенести терраформ стейт minio з минулого релізу* 

$ cp ~/installer/installer-<VERSION>/terraform/minio/aws/terraform.tfstate ./terraform/minio/aws/

*2. Перенести ключ від minio з минулого релізу*

$ cp ~/installer/installer-<VERSION>/terraform/minio/aws/private_minio.key ./terraform/minio/aws/

a|
=== 6.3. *Налаштування для Vault*

|Під час запуску Інсталера та розгортання Платформи з нуля ніякі додаткові налаштування для Vault не потрібні. a|
*1. Перенести терраформ стейт vault з минулого релізу*

$ cp ~/installer/installer-<VERSION>/terraform/vault/aws/terraform.tfstate ./terraform/vault/aws/

*2. Перенести ключ від vault з минулого релізу*

$ ~/installer/installer-<VERSION>/terraform/vault/aws/private.key ./terraform/vault/aws/

a|
=== 6.4. *Розгортання Платформи з Інсталеру*

a|
*1. Виконати команди*

$ IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\\1#" | tr -d '\n')

$ echo $IMAGE_CHECKSUM

$ sudo docker tag $\{IMAGE_CHECKSUM} control-plane-installer:<VERSION>

*2. Запустити процес інсталювання нової Платформи з імеджами*

* *--rm* - цей параметр автоматично видалить контейнер після завершення його роботи. Параметр можна прибрати, якщо потрібно дізнатися статус та лог завершеного контейнеру або при наявності нестабільного інтернету.
* *DEPLOYMENT_MODE* - може бути development чи production

$ sudo docker run --rm \

--name control-plane-installer-<VERSION> \

--user root:$(id -g) \

--net host \

-v $(pwd):/tmp/installer \

--env KUBECONFIG=/tmp/installer/kubeconfig \

--env idgovuaClientId=f90ab33dc272f047dc330c88e5663b75 \

--env idgovuaClientSecret=cba49c104faac8c718e6daf3253bc55f2bf11d9e \

--env CUSTOM_INGRESS_CIDRS='["0.0.0.0/0", "85.223.209.0/24"]' \

--env deploymentMode=<DEPLOYMENT_MODE> \

--entrypoint "/bin/sh" control-plane-installer:<VERSION> \

-c "./install.sh -i"

a|
*1. Виконати команди*

$ IMAGE_CHECKSUM=$(sudo docker load -i control-plane-installer.img | sed -r "s#.*sha256:(.*)#\\1#" | tr -d '\n')

$ echo $IMAGE_CHECKSUM

$ sudo docker tag $\{IMAGE_CHECKSUM} control-plane-installer:<VERSION>

*2. Проапдейтити версію платформи з імеджами*

* *--rm* - цей параметр автоматично видалить контейнер після завершення його роботи. Параметр можна прибрати, якщо потрібно дізнатися статус та лог завершеного контейнеру або при наявності нестабільного інтернету.
* *DEPLOYMENT_MODE* - може бути development чи production (залежить від минулого запуску)

$ sudo docker run --rm \

--name control-plane-installer-<VERSION> \

--user root:$(id -g) \

--net host \

-v $(pwd):/tmp/installer \

--env KUBECONFIG=/tmp/installer/kubeconfig \

--env idgovuaClientId=f90ab33dc272f047dc330c88e5663b75 \

--env idgovuaClientSecret=cba49c104faac8c718e6daf3253bc55f2bf11d9e \

--env CUSTOM_INGRESS_CIDRS='["0.0.0.0/0", "85.223.209.0/24"]' \

--env deploymentMode=<DEPLOYMENT_MODE> \

--entrypoint "/bin/sh" control-plane-installer:<VERSION> \

-c "./install.sh -u"

a|
=== 6.5. *Статус оновлення*

a|
Фінальний лог, зображений нижче, свідчить про вдале завершення процесу оновлення Платформи:

 image:extracted-media/media/image3.tmp[image,width=468,height=178]

Якщо у п. 6.4 було прибрано опцію *--rm*, потрібно:

* виконати наступну команду, щоб впевнитися, що контейнер завершився із статусом 0 (- статус контейнера, що свідчить про те, що він успішно завершив роботу):

____
*container status*

$ docker ps --all --latest

image:extracted-media/media/image4.tmp[image,width=468,height=26]
____

* видалити контейнер наступною командою:

____
*delete container*

$ docker rm $(docker ps --latest -q)
____

|
a|
=== 6.6. *Необхідні кроки після оновлення*

a|
* Після встановлення Платформи потрібно перевірити, що запустився пайплайн cluster-management та впевнитися, що він пройшов успішно (має зелений статус). Після цього Платформа стане придатною для розгортання реєстрів. Без цієї дії реєстри не розгорнуться.

Пайплайн cluster-management знаходиться за наступним шляхом: +
OKD Web UI → control-plane NS → Routes → jenkins url → cluster-mgmt → MASTER-Build-cluster-mgmt.

* Зробити запит на надання доступу до IIT віджету, а саме

____
https://eu.iit.com.ua/sign-widget/v20200922/ 
____

|
|===

Стан додаткових ресурсів

Після виконання усіх дій, бастіон та додаткову віртуальну машину можна вимкнути.

== 7. *Типові помилки під час розгортання платформи*

Ця секція надає інформацію про типові помилки, які можуть виникнути під час розгортання платформи з нуля та методи їх вирішення.

=== 7.1. *Помилка із bootstrap машиною під час розгортання OKD кластера.*

*Опис проблеми.* Під час розгортання кластера з'являється наступна помилка:

*помилка із bootstrap віртуальною машиною*

level=error msg=Attempted to gather ClusterOperator status after installation failure: listing ClusterOperator objects: Get "https://api.<CLUSTER_URL>:6443/apis/config.openshift.io/v1/clusteroperators": dial tcp <CLUSTER_IP>:6443: connect: connection refused

level=error msg=Bootstrap failed to complete: Get "https://api.<CLUSTER_URL>:6443/version": dial tcp <CLUSTER_IP>:6443: connect: connection refused

level=error msg=Failed waiting for Kubernetes API. This error usually happens when there is a problem on the bootstrap host that prevents creating a temporary control plane.

Ця помилка пов'язана із bootstrap віртуальною машиною і зазвичай трапляється, коли на хості bootstrap є проблема, яка перешкоджає створенню тимчасового control plane.

*Вирішення.*

* Запустити команду для видалення кластера, залишивши той самий --dir параметр.

____
*видалення OKD кластера*

$ ./openshift-install destroy cluster --dir /tmp/openshift-cluster/cluster-state --log-level info
____

* Дочекатися видалення кластера та ще раз запустити команду для його встановлення.

____
*повторне встановлення кластера*

$ ./openshift-install create cluster --dir /tmp/openshift-cluster/cluster-state --log-level=info
____

=== 7.2. *Помилка із Vault токеном під час розгортання Платформи.*

*Опис проблеми.* Під час розгортання Платформи, на етапі встановлення Vault, може трапитися помилка із тим, що змінна vault_root_token повертає порожнє значення:

image:extracted-media/media/image5.tmp[image,width=468,height=113]

Ця помилка пов'язана із тим, що Vault не піднявся успішно або були пропущенні деякі кроки Інсталяції платформи.

*Вирішення.*

* Увійти в обліковий запис AWS. Знайти віртуальну машину platform-vault-<CLUSTER_NAME>.
* Зайти на віртуальну машину використовуючи EC2 Instance Connect або SSH.
* Перевірити статус Vault. Initialized повинно бути у значенні true.

____
*vault status*

$ vault status

image:extracted-media/media/image6.tmp[image,width=468,height=182]
____

* Якщо статус інший, то перезавантажити Vault.

____
*restart vault*

$ systemctl restart vault
____

* Якщо ця помилка сталася під час оновлення Платформи, то перевірити чи було перенесено ключ від Vault з минулого релізу, як було описано в п. 6.3.
* Спробувати ще раз запустити процес встановлення Платформи, як описано в п. 6.4.

=== 7.3. *Помилка із Minio SSL сертифікатом під час розгортання Платформи.*

*Опис проблеми.* Під час розгортання Платформи, на етапі встановлення Minio, може трапитися наступна помилка:

image:extracted-media/media/image7.tmp[image,width=468,height=174]

*Вирішення.*

* увійти до до директорії з Інсталером та запустити контейнер для інсталювання Платформи наступною командою:

____
*запуск контейнера*

$ cd ~/installer/installer-<VERSION>

$ sudo docker run -it --rm \

--name control-plane-installer-<VERSION> \

--user root:$(id -g) \

--net host \

-v $(pwd):/tmp/installer \

--env KUBECONFIG=/tmp/installer/kubeconfig \

--env idgovuaClientId=f90ab33dc272f047dc330c88e5663b75 \

--env idgovuaClientSecret=cba49c104faac8c718e6daf3253bc55f2bf11d9e \

--env CUSTOM_INGRESS_CIDRS='["0.0.0.0/0", "85.223.209.0/24"]' \

--env deploymentMode=<DEPLOYMENT_MODE> control-plane-installer:<VERSION> bash
____

* Перейти в необхідну директорію та задати змінні середовища

____
*задавання змінних середовища*

$ cd /tmp/installer/terraform/minio/aws

$ export AWS_ACCESS_KEY_ID=$(oc get secret/aws-creds -n kube-system -o jsonpath='\{.data.aws_access_key_id}' | base64 -d)

$ export AWS_SECRET_ACCESS_KEY=$(oc get secret/aws-creds -n kube-system -o jsonpath='\{.data.aws_secret_access_key}' | base64 -d)

$ export CLUSTER_NAME=$(oc get node -l node-role.kubernetes.io/master -o 'jsonpath=\{.items[0].metadata.annotations.machine\.openshift\.io/machine}' | sed -r 's#.*/(.*)-master.*#\1#')

$ export clusterNameShort="$\{CLUSTER_NAME::-6}"

$ export baseDomain=$(oc get dns cluster --no-headers -o jsonpath='\{.spec.baseDomain}')

$ export route53HostedZone="$\{baseDomain/$\{clusterNameShort}./}"
____

* Видалити Minio за допомогою Terraform.

____
*видалення Minio*

$ terraform init

$ terraform destroy -var cluster_name="$\{clusterNameShort}" -var baseDomain="$\{route53HostedZone}" -auto-approve
____

* Дочекатися видалення Minio. Вийти з контейнеру та спробувати ще раз запустити процес встановлення Платформи, як описано в п. 6.4.

=== 7.4. *Помилка при відправці образів в Nexus під час розгортання Платформи.*

*Опис проблеми.* Під час розгортання Платформи, на етапі відправки образів в Nexus, може трапитися наступна помилка:

image:extracted-media/media/image8.png[image,width=468,height=228]

Ця помилка пов'язана із skopeo. Цей інструмент відправляє образи в Nexus. Якщо образ не зміг завантажитися за 10 хвилин, то skopeo починає повертати помилку через тайм-аут.

*Вирішення.*

Виконувати встановлення Платформи із додаткової віртуальної машини, як описано в п. 2.
